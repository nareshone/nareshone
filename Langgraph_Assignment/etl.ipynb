{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 888,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "\n",
    "#os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "\n",
    "llm=ChatGroq(model=\"qwen-2.5-32b\")\n",
    "#llm = ChatGroq(model=\"gemma2-9b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 889,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.graph import START, StateGraph, END\n",
    "from langgraph.prebuilt import tools_condition, ToolNode\n",
    "\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from typing_extensions import TypedDict, Annotated\n",
    "\n",
    "# System message\n",
    "sys_msg = SystemMessage(content=\"You are the business analyst and your job is to create user stories based on the requirements\")\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: MessagesState\n",
    "    user_stories: str\n",
    "    func_document: Annotated[str, \"multiple\"]  # Allowing multiple values\n",
    "    tech_document: Annotated[str, \"multiple\"]  # Allowing multiple values\n",
    "    code_content: str\n",
    "    security_comments: str\n",
    "    test_cases_content: str\n",
    "    testcases_comments: str\n",
    "    qa_test_comments: str\n",
    "\n",
    "\n",
    "def split_into_chunks(text, chunk_size=2000):  \n",
    "    \"\"\"Splits text into smaller chunks.\"\"\"\n",
    "    return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeUserStories(state: State):\n",
    "    \n",
    "    last_message = state[\"messages\"][\"messages\"][-1].content  # Get last human input\n",
    "\n",
    "    # System message to guide LLM for classification\n",
    "    classify_msg = SystemMessage(\n",
    "        content=f\"\"\"\n",
    "        You are a classifier. Your task is to strictly classify the given message as 'requirement' or 'feedback'. \n",
    "        If the message suggests dissatisfaction, suggests improvements, or requests rewriting of user stories, \n",
    "        classify it as 'feedback'. Otherwise, classify it as 'requirement'.\n",
    "        \n",
    "        Return only the word 'requirement' or 'feedback'. \n",
    "        \n",
    "        Message to classify: {last_message}\n",
    "        \"\"\"\n",
    "    )\n",
    "    print(classify_msg)  # Debugging print\n",
    "\n",
    "    # Ask LLM to classify\n",
    "    response = llm.invoke([classify_msg])\n",
    "    \n",
    "    if hasattr(response, 'content') and response.content.strip().lower() == 'feedback':\n",
    "        # Rewrite if it's feedback\n",
    "        rewrite_msg = SystemMessage(\n",
    "            content=f\"As you are a business analyst, Rewrite the user stories based on the feedback provided: {last_message}\"\n",
    "        )\n",
    "        result = llm.invoke([rewrite_msg] + state[\"messages\"][\"messages\"])\n",
    "        \n",
    "    else:\n",
    "        # Process the requirement\n",
    "        result = llm.invoke([sys_msg] + state[\"messages\"][\"messages\"])\n",
    "    \n",
    "    \n",
    "    # Append the latest user story to the list\n",
    "    state[\"user_stories\"] = result.content  \n",
    "\n",
    "    return {\"messages\": [result], \"user_stories\": result.content}\n",
    "\n",
    "\n",
    "# Human feedback node\n",
    "def po_feedback(state: State):\n",
    "    return {\"messages\": state[\"messages\"]}\n",
    "\n",
    "# Human feedback node\n",
    "def design_feedback(state: State):\n",
    "    return {\"messages\": state[\"messages\"]}\n",
    "\n",
    "# Human feedback node\n",
    "def code_feedback(state: State):\n",
    "    return {\"messages\": state[\"messages\"]}\n",
    "\n",
    "# Human feedback node\n",
    "def security_feedback(state: State):\n",
    "    return {\"messages\": state[\"messages\"]}\n",
    "\n",
    "# Human feedback node\n",
    "def testcases_feedback(state: State):\n",
    "    return {\"messages\": state[\"messages\"]}\n",
    "\n",
    "def old_security_feedback(state: State):\n",
    "    last_message = state[\"messages\"][\"messages\"][-1].content  # Get last human input\n",
    "\n",
    "\n",
    "    security_msg = SystemMessage(\n",
    "            content=f\"\"\"You are a Security Expert. Review the following code for potential security vulnerabilities.\n",
    "        Provide your feedback whether it looks good or not.\n",
    "        Code to Review:\n",
    "        {state[\"code_content\"]}\n",
    "\n",
    "                    \"\"\"\n",
    "        )\n",
    "    \n",
    "    \n",
    "    # Ask LLM to classify\n",
    "    result = llm.invoke([security_msg] + state[\"messages\"][\"messages\"])\n",
    "    \n",
    "    # Append the latest functional document to the list\n",
    "    state[\"security_comments\"] = result.content  \n",
    "\n",
    "    return {\"messages\": [HumanMessage(content=result)], \"security_comments\": result.content}\n",
    "\n",
    "def new_security_feedback(state: State):\n",
    "    last_message = state[\"messages\"][\"messages\"][-1].content  # Get last human input\n",
    "    code_content = state[\"code_content\"]\n",
    "\n",
    "    # Split the code content into chunks\n",
    "    code_chunks = split_into_chunks(code_content, chunk_size=2000)\n",
    "    feedback_list = []\n",
    "\n",
    "    for chunk in code_chunks:\n",
    "        security_msg = SystemMessage(\n",
    "            content=f\"\"\"Verify and check for any security issues in the following code and provide your feedback based on the code security.\n",
    "                        Indicate whether it's 'Good' or 'Bad' and provide reasons if it's 'Bad'.\n",
    "                        Code: {chunk}\"\"\"\n",
    "        )\n",
    "\n",
    "        # Invoke LLM for each chunk\n",
    "        result = llm.invoke([security_msg] + state[\"messages\"][\"messages\"])\n",
    "        \n",
    "        if hasattr(result, 'content'):\n",
    "            feedback_list.append(result.content)\n",
    "    \n",
    "    # Combine all feedback into a single response\n",
    "    combined_feedback = \"\\n\\n\".join(feedback_list)\n",
    "    state[\"security_comments\"] = combined_feedback  \n",
    "\n",
    "    return {\"messages\": [HumanMessage(content=combined_feedback)], \"security_comments\": combined_feedback}\n",
    "\n",
    "\n",
    "def old_testcases_feedback(state: State):\n",
    "    last_message = state[\"messages\"][\"messages\"][-1].content  # Get last human input\n",
    "\n",
    "\n",
    "    testcases_msg = SystemMessage(\n",
    "            content=f\"\"\"You are a QA analyst, Verify the below testcases and add your comments whether it looks good or not by referring the\n",
    "            function design document, technical design document and written code.\n",
    "        \n",
    "        Tescases to Review:\n",
    "        {state[\"test_cases_content\"]}\n",
    "\n",
    "        Refer the following functional Design Document: {state[\"func_document\"]}\n",
    "        Refer the following Technical Design Document: {state[\"tech_document\"]}\n",
    "        Refer the following written code: {state[\"code_content\"]}\n",
    "\n",
    "                    \"\"\"\n",
    "        )\n",
    "    \n",
    "    \n",
    "    # Ask LLM to classify\n",
    "    result = llm.invoke([testcases_msg] + state[\"messages\"][\"messages\"])\n",
    "    \n",
    "    # Append the latest functional document to the list\n",
    "    state[\"testcases_comments\"] = result.content  \n",
    "\n",
    "    return {\"messages\": [HumanMessage(content=result)], \"testcases_comments\": result.content}\n",
    "\n",
    "# Decision function for user stories\n",
    "def evaluate_feedback(state: State) -> list[str]:\n",
    "    \"\"\"Use LLM to classify user feedback as 'positive' or 'negative'.\"\"\"\n",
    "\n",
    "    last_message = state[\"messages\"][\"messages\"][-1].content  # Get last human feedback\n",
    "    print(\"last_message====>\"+last_message)\n",
    "\n",
    "    # System message to guide LLM\n",
    "    classify_msg = SystemMessage(\n",
    "        content=\"Analyze the following feedback and classify it strictly as 'positive' or 'negative'. \"\n",
    "                \"If the feedback suggests dissatisfaction, or rewriting, classify it as 'negative'. \"\n",
    "                \"Otherwise, classify it as 'positive'. Return only 'positive' or 'negative' as output.\\n\"\n",
    "                f\"Feedback: {last_message}\"\n",
    "    )\n",
    "\n",
    "    # Ask LLM to classify\n",
    "    response = llm.invoke([classify_msg])\n",
    "\n",
    "    if hasattr(response, 'content') and response.content:\n",
    "        decision = response.content.strip().lower()\n",
    "        print(f\"Decision made by evaluator: {decision}\")  # Log decision\n",
    "        if decision == \"negative\":\n",
    "            return [\"writeUserStories\"]\n",
    "        elif decision == \"positive\":\n",
    "            return [\"createFuncDesignDocument\", \"createTechDesignDocument\"]\n",
    "    print(\"Defaulting to writeUserStories due to unexpected decision.\")  # Log fallback case\n",
    "    return [\"writeUserStories\"]  # Default if the response is invalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFuncDesignDocument(state: State):\n",
    "    \n",
    "    last_message = state[\"messages\"][\"messages\"][-1].content  # Get last human input\n",
    "\n",
    "    # Check if the last message came from design_feedback by using a flag in state\n",
    "    if \"from_design_feedback\" in state and state[\"from_design_feedback\"] == True:  \n",
    "        func_msg = SystemMessage(\n",
    "            content=f\"\"\" You are an ETL developer, create a functional design document based on these user stories: {state[\"user_stories\"]}.\n",
    "            Also, consider the following feedback: {last_message}\n",
    "            \"\"\"\n",
    "        )\n",
    "    else:\n",
    "        func_msg = SystemMessage(\n",
    "            content=f\"\"\" You are an ETL developer, create a functional design document based on these user stories: {state[\"user_stories\"]}.\"\"\"\n",
    "        )\n",
    "    \n",
    "\n",
    "    # Ask LLM to classify\n",
    "    result = llm.invoke([func_msg] + state[\"messages\"][\"messages\"])\n",
    "    \n",
    "    # Append the latest functional document to the list\n",
    "    state[\"func_document\"] = result.content  \n",
    "\n",
    "    return {\"messages\": [result], \"func_document\": result.content}\n",
    "\n",
    "def createTechDesignDocument(state: State):\n",
    "    \n",
    "    last_message = state[\"messages\"][\"messages\"][-1].content  # Get last human input\n",
    "\n",
    "    # Check if the last message came from design_feedback by using a flag in state\n",
    "    if \"from_design_feedback\" in state and state[\"from_design_feedback\"] == True:  \n",
    "        tech_msg = SystemMessage(\n",
    "            content=f\"\"\" You are an ETL developer, create a technical design document based on these user stories: {state[\"user_stories\"]}.\n",
    "            Also, consider the following feedback: {last_message}\n",
    "            \"\"\"\n",
    "        )\n",
    "    else:\n",
    "        tech_msg = SystemMessage(\n",
    "            content=f\"\"\" You are an ETL developer, create a technical design document based on these user stories: {state[\"user_stories\"]}.\"\"\"\n",
    "        )\n",
    "    \n",
    "\n",
    "    # Ask LLM to classify\n",
    "    result = llm.invoke([tech_msg] + state[\"messages\"][\"messages\"])\n",
    "    \n",
    "    # Append the latest functional document to the list\n",
    "    state[\"tech_document\"] = result.content  \n",
    "\n",
    "    return {\"tech_document\": result.content}\n",
    "\n",
    "# Decision function for user stories\n",
    "def evaluate_feedback_design(state: State) -> list[str]:\n",
    "    \"\"\"Use LLM to classify user feedback as 'positive' or 'negative'.\"\"\"\n",
    "\n",
    "    last_message = state[\"messages\"][\"messages\"][-1].content  # Get last human feedback\n",
    "\n",
    "    # System message to guide LLM\n",
    "    classify_msg = SystemMessage(\n",
    "        content=\"Analyze the following feedback and classify it strictly as 'positive' or 'negative'. \"\n",
    "                \"If the feedback suggests dissatisfaction or rewriting, classify it as 'negative'. \"\n",
    "                \"Otherwise, classify it as 'positive'. Return only 'positive' or 'negative' as output.\\n\"\n",
    "                f\"Feedback: {last_message}\"\n",
    "    )\n",
    "\n",
    "    # Ask LLM to classify\n",
    "    response = llm.invoke([classify_msg])\n",
    "\n",
    "    if hasattr(response, 'content') and response.content:\n",
    "        decision = response.content.strip().lower()\n",
    "        print(f\"Decision made by evaluator: {decision}\")  # Log decision\n",
    "        if decision == \"negative\":\n",
    "            return [\"createFuncDesignDocument\",\"createTechDesignDocument\"]\n",
    "        elif decision == \"positive\":\n",
    "            return [\"GenerateCode\"]\n",
    "    \n",
    "    return [\"writeUserStories\"]  # Default if the response is invalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "metadata": {},
   "outputs": [],
   "source": [
    "def old_GenerateCode(state: State):\n",
    "    \n",
    "    last_message = state[\"messages\"][\"messages\"][-1].content  # Get last human input\n",
    "\n",
    "    # Check if the last message came from design_feedback by using a flag in state\n",
    "    if \"from_code_feedback\" in state and state[\"from_code_feedback\"] == True:  \n",
    "        code_msg = SystemMessage(\n",
    "            content=f\"\"\" You are an ETL developer, Generate code based on following functional and technical design documents: \n",
    "            Refer the following functional Design Document: {state[\"func_document\"]}\n",
    "            Also, refer the following Technical Design Document: {state[\"tech_document\"]}\n",
    "\n",
    "            consider the following feedback as well: {last_message}\n",
    "            \"\"\"\n",
    "        )\n",
    "    elif \"from_security_feedback\" in state and state[\"from_security_feedback\"] == True:  \n",
    "        code_msg = SystemMessage(\n",
    "            content=f\"\"\" You are an ETL developer, Generate code based on following functional and technical design documents: \n",
    "            Refer the following functional Design Document: {state[\"func_document\"]}\n",
    "            Also, refer the following Technical Design Document: {state[\"tech_document\"]}\n",
    "\n",
    "            consider the following feedback as well: {state[\"security_comments\"]}\n",
    "            \"\"\"\n",
    "        )\n",
    "    elif \"from_executingTestCases\" in state and state[\"from_executingTestCases\"] == True:  \n",
    "        code_msg = SystemMessage(\n",
    "            content=f\"\"\" You are an ETL developer, Generate code based on following functional and technical design documents: \n",
    "            Refer the following functional Design Document: {state[\"func_document\"]}\n",
    "            Also, refer the following Technical Design Document: {state[\"tech_document\"]}\n",
    "\n",
    "            consider the following feedback as well: {state[\"qa_test_comments\"]}\n",
    "            \"\"\"\n",
    "        )\n",
    "    else:\n",
    "        code_msg = SystemMessage(\n",
    "            content=f\"\"\" You are an ETL developer, Generate code based on following functional and technical design documents: \n",
    "            Refer the following functional Design Document: {state[\"func_document\"]}\n",
    "            Also, refer the following Technical Design Document: {state[\"tech_document\"]}\"\"\"\n",
    "        )\n",
    "    \n",
    "\n",
    "    # Ask LLM to classify\n",
    "    result = llm.invoke([code_msg] + state[\"messages\"][\"messages\"])\n",
    "    \n",
    "    # Append the latest functional document to the list\n",
    "    state[\"code_content\"] = result.content  \n",
    "\n",
    "    return {\"messages\": [result], \"code_content\": result.content}\n",
    "\n",
    "def GenerateCode(state: State):\n",
    "    \n",
    "    last_message = state[\"messages\"][\"messages\"][-1].content  # Get last human input\n",
    "\n",
    "    # Function to summarize content\n",
    "    def summarize_content(content, role):\n",
    "        summary_msg = SystemMessage(\n",
    "            content=f'Summarize the following {role} document to a concise description, retaining key implementation points.\\nContent: {content}'\n",
    "        )\n",
    "        summary_response = llm.invoke([summary_msg])\n",
    "        return summary_response.content.strip()\n",
    "\n",
    "    # Summarize Functional Design Document\n",
    "    func_summary = summarize_content(state[\"func_document\"], \"functional design\")\n",
    "\n",
    "    # Summarize Technical Design Document\n",
    "    tech_summary = summarize_content(state[\"tech_document\"], \"technical design\")\n",
    "    \n",
    "    # Prepare the prompt based on the state flags\n",
    "    if \"from_code_feedback\" in state and state[\"from_code_feedback\"] == True:  \n",
    "        feedback_content = last_message\n",
    "    elif \"from_security_feedback\" in state and state[\"from_security_feedback\"] == True:  \n",
    "        feedback_content = state[\"security_comments\"]\n",
    "    elif \"from_executingTestCases\" in state and state[\"from_executingTestCases\"] == True:  \n",
    "        feedback_content = state[\"qa_test_comments\"]\n",
    "    else:\n",
    "        feedback_content = None  # No additional feedback\n",
    "\n",
    "    # Prepare the generation message\n",
    "    if feedback_content:\n",
    "        code_msg = SystemMessage(\n",
    "            content=f\"\"\"\n",
    "            You are an ETL developer. Generate code based on the following summarized documents:\n",
    "\n",
    "            Functional Design Document Summary: {func_summary}\n",
    "            Technical Design Document Summary: {tech_summary}\n",
    "\n",
    "            Also, consider the following feedback: {feedback_content}\n",
    "            \"\"\"\n",
    "        )\n",
    "    else:\n",
    "        code_msg = SystemMessage(\n",
    "            content=f\"\"\"\n",
    "            You are an ETL developer. Generate code based on the following summarized documents:\n",
    "\n",
    "            Functional Design Document Summary: {func_summary}\n",
    "            Technical Design Document Summary: {tech_summary}\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "    # Generate code\n",
    "    result = llm.invoke([code_msg])\n",
    "    \n",
    "    # Store the generated code\n",
    "    state[\"code_content\"] = result.content  \n",
    "\n",
    "    return {\"messages\": [result], \"code_content\": result.content}\n",
    "\n",
    "\n",
    "# Decision function for user stories\n",
    "def evaluate_code_feedback(state: State) -> list[str]:\n",
    "    \"\"\"Use LLM to classify user feedback as 'positive' or 'negative'.\"\"\"\n",
    "\n",
    "    last_message = state[\"messages\"][\"messages\"][-1].content  # Get last human feedback\n",
    "\n",
    "    # System message to guide LLM\n",
    "    classify_msg = SystemMessage(\n",
    "        content=\"Analyze the following feedback and classify it strictly as 'positive' or 'negative'. \"\n",
    "                \"If the feedback suggests dissatisfaction or rewriting, classify it as 'negative'. \"\n",
    "                \"Otherwise, classify it as 'positive'. Return only 'positive' or 'negative' as output.\\n\"\n",
    "                f\"Feedback: {last_message}\"\n",
    "    )\n",
    "\n",
    "    # Ask LLM to classify\n",
    "    response = llm.invoke([classify_msg])\n",
    "\n",
    "    if hasattr(response, 'content') and response.content:\n",
    "        decision = response.content.strip().lower()\n",
    "        print(f\"Decision made by evaluator: {decision}\")  # Log decision\n",
    "        if decision == \"negative\":\n",
    "            return [\"GenerateCode\"]\n",
    "        elif decision == \"positive\":\n",
    "            return [\"security_feedback\"]\n",
    "    \n",
    "    return [\"GenerateCode\"]  # Default if the response is invalid\n",
    "\n",
    "# Decision function for user stories\n",
    "def evaluate_security_feedback(state: State) -> list[str]:\n",
    "    \"\"\"Use LLM to classify user feedback as 'positive' or 'negative'.\"\"\"\n",
    "\n",
    "    last_message = state[\"messages\"][\"messages\"][-1].content  # Get last human feedback\n",
    "\n",
    "    # System message to guide LLM\n",
    "    classify_msg = SystemMessage(\n",
    "        content=\"Analyze the following feedback and classify it strictly as 'positive' or 'negative'. \"\n",
    "                \"If the feedback suggests dissatisfaction or rewriting, classify it as 'negative'. \"\n",
    "                \"Otherwise, classify it as 'positive'. Return only 'positive' or 'negative' as output.\\n\"\n",
    "                f\"Feedback: {last_message}\"\n",
    "    )\n",
    "\n",
    "    # Ask LLM to classify\n",
    "    response = llm.invoke([classify_msg])\n",
    "\n",
    "    if hasattr(response, 'content') and response.content:\n",
    "        decision = response.content.strip().lower()\n",
    "        print(f\"Decision made by evaluator: {decision}\")  # Log decision\n",
    "        if decision == \"negative\":\n",
    "            return [\"GenerateCode\"]\n",
    "        elif decision == \"positive\":\n",
    "            return [\"writeTestCases\"]\n",
    "    \n",
    "    return [\"GenerateCode\"]  # Default if the response is invalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 893,
   "metadata": {},
   "outputs": [],
   "source": [
    "def old_writeTestCases(state: State):\n",
    "    \n",
    "    last_message = state[\"messages\"][\"messages\"][-1].content  \n",
    "\n",
    "    # Check if the last message came from design_feedback by using a flag in state\n",
    "    if \"from_test_feedback\" in state and state[\"from_test_feedback\"] == True:  \n",
    "        test_msg = SystemMessage(\n",
    "            content=f\"\"\" You are a QA Analyst, write Test cases by referring the following functional design document, technical design document and code content : \n",
    "            Refer the following functional Design Document: {state[\"func_document\"]}\n",
    "            Refer the following Technical Design Document: {state[\"tech_document\"]}\n",
    "            Refer the following Written code: {state[\"code_content\"]}\n",
    "\n",
    "            consider the following feedback as well: {last_message}\n",
    "            \"\"\"\n",
    "        )\n",
    "    else:\n",
    "        test_msg = SystemMessage(\n",
    "            content=f\"\"\" You are a QA Analyst, write Test cases by referring the following functional design document, technical design document and code content : \n",
    "            Refer the following functional Design Document: {state[\"func_document\"]}\n",
    "            Refer the following Technical Design Document: {state[\"tech_document\"]}\n",
    "            Refer the following Written code: {state[\"code_content\"]}\n",
    "\n",
    "            \"\"\"\n",
    "        )\n",
    "    \n",
    "\n",
    "    # Ask LLM to classify\n",
    "    result = llm.invoke([test_msg] + state[\"messages\"][\"messages\"])\n",
    "    \n",
    "    # Append the latest functional document to the list\n",
    "    state[\"test_cases_content\"] = result.content  \n",
    "\n",
    "    return {\"messages\": [result], \"test_cases_content\": result.content}\n",
    "\n",
    "# Optimized writeTestCases function\n",
    "\n",
    "def writeTestCases(state: State):\n",
    "\n",
    "    last_message = state['messages']['messages'][-1].content  # Get last human feedback\n",
    "\n",
    "    # Function to summarize content\n",
    "    def summarize_content(content, role):\n",
    "        summary_msg = SystemMessage(\n",
    "            content=f'Summarize the following {role} document to a concise description, retaining key points for writing test cases.\\nContent: {content}'\n",
    "        )\n",
    "        summary_response = llm.invoke([summary_msg])\n",
    "        return summary_response.content.strip()\n",
    "\n",
    "    # Summarize Functional Design Document\n",
    "    func_summary = summarize_content(state['func_document'], 'functional design')\n",
    "\n",
    "    # Summarize Technical Design Document\n",
    "    tech_summary = summarize_content(state['tech_document'], 'technical design')\n",
    "\n",
    "    # Summarize Code Content\n",
    "    code_summary = summarize_content(state['code_content'], 'code')\n",
    "\n",
    "    # Prepare the test case generation message\n",
    "    if 'from_test_feedback' in state and state['from_test_feedback'] == True:  \n",
    "        test_msg = SystemMessage(\n",
    "            content=f\"\"\"\n",
    "            You are a QA Analyst. Write test cases based on the following summaries of documents and code:\n",
    "\n",
    "            Functional Design Document Summary: {func_summary}\n",
    "            Technical Design Document Summary: {tech_summary}\n",
    "            Code Summary: {code_summary}\n",
    "\n",
    "            Consider the following feedback: {last_message}\n",
    "            \"\"\"\n",
    "        )\n",
    "    else:\n",
    "        test_msg = SystemMessage(\n",
    "            content=f\"\"\"\n",
    "            You are a QA Analyst. Write test cases based on the following summaries of documents and code:\n",
    "\n",
    "            Functional Design Document Summary: {func_summary}\n",
    "            Technical Design Document Summary: {tech_summary}\n",
    "            Code Summary: {code_summary}\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "    # Generate test cases\n",
    "    result = llm.invoke([test_msg])\n",
    "\n",
    "    # Store the generated test cases\n",
    "    state['test_cases_content'] = result.content  \n",
    "\n",
    "    return {'messages': [result], 'test_cases_content': result.content}\n",
    "\n",
    "# Decision function for user stories\n",
    "def evaluate_test_cases_feedback(state: State) -> list[str]:\n",
    "    \"\"\"Use LLM to classify user feedback as 'positive' or 'negative'.\"\"\"\n",
    "\n",
    "    last_message = state[\"messages\"][\"messages\"][-1].content  # Get last human feedback\n",
    "\n",
    "    # System message to guide LLM\n",
    "    classify_msg = SystemMessage(\n",
    "        content=\"Analyze the following feedback and classify it strictly as 'positive' or 'negative'. \"\n",
    "                \"If the feedback suggests dissatisfaction or rewriting, classify it as 'negative'. \"\n",
    "                \"Otherwise, classify it as 'positive'. Return only 'positive' or 'negative' as output.\\n\"\n",
    "                f\"Feedback: {last_message}\"\n",
    "    )\n",
    "\n",
    "    # Ask LLM to classify\n",
    "    response = llm.invoke([classify_msg])\n",
    "\n",
    "    if hasattr(response, 'content') and response.content:\n",
    "        decision = response.content.strip().lower()\n",
    "        print(f\"Decision made by evaluator: {decision}\")  # Log decision\n",
    "        if decision == \"negative\":\n",
    "            return [\"writeTestCases\"]\n",
    "        elif decision == \"positive\":\n",
    "            return \"executingTestCases\"\n",
    "    \n",
    "    return [\"writeTestCases\"]  # Default if the response is invalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "metadata": {},
   "outputs": [],
   "source": [
    "def old_executingTestCases(state: State):\n",
    "    \n",
    "    last_message = state[\"messages\"][\"messages\"][-1].content  \n",
    "\n",
    "    # Check if the last message came from design_feedback by using a flag in state\n",
    "\n",
    "    exec_msg = SystemMessage(\n",
    "            content=f\"\"\" You are a QA Tester, execute the below test cases and provide your feedback whether its good or not \n",
    "\n",
    "            Refer Test cases: {state[\"test_cases_content\"]}\n",
    "            Refer Code: {state[\"code_content\"]}\n",
    "\n",
    "            \"\"\"\n",
    "        )\n",
    "    \n",
    "    # Ask LLM to classify\n",
    "    result = llm.invoke([exec_msg] + state[\"messages\"][\"messages\"])\n",
    "    \n",
    "    # Append the latest functional document to the list\n",
    "    state[\"qa_test_comments\"] = result.content  \n",
    "\n",
    "    return {\"messages\": [result], \"qa_test_comments\": result.content}\n",
    "\n",
    "def executingTestCases(state: State):\n",
    "    \n",
    "    # Checking if 'messages' key exists in 'state'\n",
    "    if \"messages\" not in state or not isinstance(state[\"messages\"], dict):\n",
    "        raise TypeError(\"State is missing 'messages' or 'messages' is not a valid dictionary.\")\n",
    "\n",
    "    # Check if messages are present, otherwise use default message\n",
    "    if \"messages\" in state[\"messages\"] and state[\"messages\"][\"messages\"]:\n",
    "        last_message = state[\"messages\"][\"messages\"][-1].content\n",
    "    else:\n",
    "        last_message = \"good\"  # Default message if no messages are found\n",
    "\n",
    "    # Checking if 'test_cases_content' and 'code_content' exist in state\n",
    "    if \"test_cases_content\" not in state or \"code_content\" not in state:\n",
    "        raise ValueError(\"State is missing 'test_cases_content' or 'code_content'.\")\n",
    "    \n",
    "    # Preparing the message to execute test cases\n",
    "    exec_msg = SystemMessage(\n",
    "        content=f\"\"\" You are a QA Tester, execute the below test cases and provide your feedback whether it's good or not. \n",
    "\n",
    "        Test Cases: {state[\"test_cases_content\"]}\n",
    "        Code: {state[\"code_content\"]}\n",
    "        Feedback: {last_message}\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    # Ask LLM to execute test cases and provide feedback\n",
    "    result = llm.invoke([exec_msg])\n",
    "    \n",
    "    # Storing the feedback in state\n",
    "    #state[\"qa_test_comments\"] = result.content\n",
    "    state[\"qa_test_comments\"] = \"its good\"  \n",
    "\n",
    "    return {\"messages\": [result], \"qa_test_comments\": result.content}\n",
    "\n",
    "\n",
    "\n",
    "# Decision function for user stories\n",
    "def evaluate_qa_test_feedback(state: State) -> list[str]:\n",
    "    \"\"\"Use LLM to classify user feedback as 'positive' or 'negative'.\"\"\"\n",
    "\n",
    "    last_message = state[\"messages\"][\"messages\"][-1].content  \n",
    "\n",
    "    # System message to guide LLM\n",
    "    classify_msg = SystemMessage(\n",
    "        content=\"Analyze the following feedback and classify it strictly as 'positive' or 'negative'. \"\n",
    "                \"If the feedback suggests dissatisfaction or rewriting the code, classify it as 'negative'. \"\n",
    "                \"Otherwise, classify it as 'positive'. Return only 'positive' or 'negative' as output.\\n\"\n",
    "                f\"Feedback: {last_message}\"\n",
    "    )\n",
    "\n",
    "    # Ask LLM to classify\n",
    "    response = llm.invoke([classify_msg])\n",
    "\n",
    "    if hasattr(response, 'content') and response.content:\n",
    "        decision = response.content.strip().lower()\n",
    "        print(f\"Decision made by evaluator: {decision}\")  # Log decision\n",
    "        if decision == \"negative\":\n",
    "            return [\"GenerateCode\"]\n",
    "        elif decision == \"positive\":\n",
    "            return END\n",
    "    \n",
    "    return [\"GenerateCode\"]  # Default if the response is invalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTechDesignDocument(state: State):\n",
    "    \n",
    "    last_message = state[\"messages\"][\"messages\"][-1].content  # Get last human input\n",
    "\n",
    "    # Check if the last message came from design_feedback by using a flag in state\n",
    "    if \"from_design_feedback\" in state and state[\"from_design_feedback\"] == True:  \n",
    "        tech_msg = SystemMessage(\n",
    "            content=f\"\"\" You are an ETL developer, create a technical design document based on these user stories: {state[\"user_stories\"]}.\n",
    "            Also, consider the following feedback: {last_message}\n",
    "            \"\"\"\n",
    "        )\n",
    "    else:\n",
    "        tech_msg = SystemMessage(\n",
    "            content=f\"\"\" You are an ETL developer, create a technical design document based on these user stories: {state[\"user_stories\"]}.\"\"\"\n",
    "        )\n",
    "    \n",
    "\n",
    "    # Ask LLM to classify\n",
    "    result = llm.invoke([tech_msg] + state[\"messages\"][\"messages\"])\n",
    "    \n",
    "    # Append the latest functional document to the list\n",
    "    state[\"tech_document\"] = result.content  \n",
    "\n",
    "    return {\"tech_document\": result.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Graph\n",
    "builder = StateGraph(State)\n",
    "\n",
    "# Define nodes: these do the work\n",
    "builder.add_node(\"writeUserStories\", writeUserStories)\n",
    "builder.add_node(\"po_feedback\", po_feedback)\n",
    "builder.add_node(\"design_feedback\", design_feedback)\n",
    "builder.add_node(\"code_feedback\", code_feedback)\n",
    "builder.add_node(\"createFuncDesignDocument\", createFuncDesignDocument)\n",
    "builder.add_node(\"createTechDesignDocument\", createTechDesignDocument)\n",
    "builder.add_node(\"GenerateCode\", GenerateCode)\n",
    "builder.add_node(\"security_feedback\", security_feedback)\n",
    "builder.add_node(\"testcases_feedback\", testcases_feedback)\n",
    "builder.add_node(\"writeTestCases\", writeTestCases)\n",
    "builder.add_node(\"executingTestCases\", executingTestCases)\n",
    "\n",
    "builder.add_edge(START,\"writeUserStories\")\n",
    "builder.add_edge(\"writeUserStories\",\"po_feedback\")\n",
    "\n",
    "builder.add_edge(\"createFuncDesignDocument\",\"design_feedback\")\n",
    "builder.add_edge(\"createTechDesignDocument\",\"design_feedback\")\n",
    "builder.add_edge(\"GenerateCode\",\"code_feedback\")\n",
    "builder.add_edge(\"writeTestCases\",\"testcases_feedback\")\n",
    "\n",
    "#builder.add_conditional_edges(\"po_feedback\",evaluate_feedback,[\"writeUserStories\", \"fork_node\"])\n",
    "# Add conditional edges from 'po_feedback' based on user feedback\n",
    "builder.add_conditional_edges(\"po_feedback\", evaluate_feedback, [\"writeUserStories\", \"createFuncDesignDocument\", \"createTechDesignDocument\"])\n",
    "builder.add_conditional_edges(\"design_feedback\", evaluate_feedback_design, [\"GenerateCode\", \"createFuncDesignDocument\", \"createTechDesignDocument\"])\n",
    "builder.add_conditional_edges(\"code_feedback\", evaluate_code_feedback, [\"security_feedback\", \"GenerateCode\"])\n",
    "builder.add_conditional_edges(\"security_feedback\", evaluate_security_feedback, [\"writeTestCases\", \"GenerateCode\"])\n",
    "builder.add_conditional_edges(\"testcases_feedback\", evaluate_test_cases_feedback, [\"writeTestCases\", \"executingTestCases\"])\n",
    "builder.add_conditional_edges(\"executingTestCases\", evaluate_qa_test_feedback, [END, \"GenerateCode\"])\n",
    "\n",
    "memory = MemorySaver()\n",
    "## human in the loop\n",
    "#graph = builder.compile(interrupt_before=[\"po_feedback\",\"design_feedback\",\"code_feedback\"],checkpointer=memory)\n",
    "graph = builder.compile(interrupt_before=[\"po_feedback\",\"design_feedback\",\"code_feedback\"],checkpointer=memory)\n",
    "\n",
    "# Show\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'InMemorySaver' object has no attribute 'clear'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[676], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclear\u001b[49m()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'InMemorySaver' object has no attribute 'clear'"
     ]
    }
   ],
   "source": [
    "\n",
    "memory.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 899,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': {'messages': [HumanMessage(content='The requirement is to transfer employee data file from one directory to another using pyspark', additional_kwargs={}, response_metadata={})]}}\n",
      "content=\"\\n        You are a classifier. Your task is to strictly classify the given message as 'requirement' or 'feedback'. \\n        If the message suggests dissatisfaction, suggests improvements, or requests rewriting of user stories, \\n        classify it as 'feedback'. Otherwise, classify it as 'requirement'.\\n        \\n        Return only the word 'requirement' or 'feedback'. \\n        \\n        Message to classify: The requirement is to transfer employee data file from one directory to another using pyspark\\n        \" additional_kwargs={} response_metadata={}\n",
      "{'messages': [AIMessage(content='Certainly! Below is a user story that captures the requirement of transferring employee data files from one directory to another using PySpark.\\n\\n---\\n\\n**User Story: Transfer Employee Data File Using PySpark**\\n\\n**As a** Data Engineer  \\n**I want to** transfer an employee data file from one directory to another using PySpark  \\n**So that** the data can be processed and analyzed in the destination directory without manual intervention.\\n\\n**Acceptance Criteria:**\\n\\n1. **File Identification:**\\n   - The system should be able to identify the specific employee data file (e.g., `employee_data.csv`) in the source directory.\\n\\n2. **PySpark Implementation:**\\n   - The transfer process must be implemented using PySpark.\\n   - The PySpark job should read the employee data file from the source directory.\\n\\n3. **File Transfer:**\\n   - The system should transfer the identified file to the specified destination directory.\\n   - The file should be transferred in its original format (e.g., CSV).\\n\\n4. **Error Handling:**\\n   - If the file does not exist in the source directory, the system should log an error message.\\n   - If there is an error during the transfer process, it should be logged, and the job should fail gracefully.\\n\\n5. **Logging:**\\n   - The system should log the start and completion of the file transfer process.\\n   - The log should include the date, time, and status of the transfer.\\n\\n6. **Confirmation:**\\n   - After successful transfer, the system should confirm the file transfer by logging the success message.\\n\\n---\\n\\nThis user story covers the key aspects of the requirement while ensuring that the process is automated, reliable, and traceable through logging.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 344, 'prompt_tokens': 46, 'total_tokens': 390, 'completion_time': 1.72, 'prompt_time': 0.005023208, 'queue_time': 0.051268471999999995, 'total_time': 1.725023208}, 'model_name': 'qwen-2.5-32b', 'system_fingerprint': 'fp_c527211fd1', 'finish_reason': 'stop', 'logprobs': None}, id='run-829e71db-d8f7-4da4-90f0-1c2b3ac5dd75-0', usage_metadata={'input_tokens': 46, 'output_tokens': 344, 'total_tokens': 390})], 'user_stories': 'Certainly! Below is a user story that captures the requirement of transferring employee data files from one directory to another using PySpark.\\n\\n---\\n\\n**User Story: Transfer Employee Data File Using PySpark**\\n\\n**As a** Data Engineer  \\n**I want to** transfer an employee data file from one directory to another using PySpark  \\n**So that** the data can be processed and analyzed in the destination directory without manual intervention.\\n\\n**Acceptance Criteria:**\\n\\n1. **File Identification:**\\n   - The system should be able to identify the specific employee data file (e.g., `employee_data.csv`) in the source directory.\\n\\n2. **PySpark Implementation:**\\n   - The transfer process must be implemented using PySpark.\\n   - The PySpark job should read the employee data file from the source directory.\\n\\n3. **File Transfer:**\\n   - The system should transfer the identified file to the specified destination directory.\\n   - The file should be transferred in its original format (e.g., CSV).\\n\\n4. **Error Handling:**\\n   - If the file does not exist in the source directory, the system should log an error message.\\n   - If there is an error during the transfer process, it should be logged, and the job should fail gracefully.\\n\\n5. **Logging:**\\n   - The system should log the start and completion of the file transfer process.\\n   - The log should include the date, time, and status of the transfer.\\n\\n6. **Confirmation:**\\n   - After successful transfer, the system should confirm the file transfer by logging the success message.\\n\\n---\\n\\nThis user story covers the key aspects of the requirement while ensuring that the process is automated, reliable, and traceable through logging.'}\n"
     ]
    }
   ],
   "source": [
    "initial_input = {\"messages\": MessagesState(messages=[HumanMessage(content=\"The requirement is to transfer employee data file from one directory to another using pyspark\")])}\n",
    "thread={\"configurable\":{\"thread_id\":\"39\"}}\n",
    "\n",
    "for event in graph.stream(initial_input,thread,stream_mode=\"values\"):\n",
    "    print(event)\n",
    "    #event['messages']['messages']['user_stories'].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('testcases_feedback',)"
      ]
     },
     "execution_count": 867,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state=graph.get_state(thread)\n",
    "state.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StateSnapshot(values={'messages': {'messages': [AIMessage(content='Based on the provided summaries, the following test cases are designed to ensure that all aspects of the ETL process, including data integrity, error handling, logging, performance, and security, are thoroughly tested.\\n\\n### Test Cases\\n\\n#### 1. Test Reading Data\\n**Test Case ID:** TC01\\n**Title:** Verify data reading from the source directory.\\n**Preconditions:** Source directory contains valid Parquet files.\\n**Steps:**\\n1. Call the `read_data` function with the path to the source directory.\\n2. Verify that the function reads the data successfully.\\n3. Verify that the function handles exceptions (e.g., file not found, permission issues) appropriately.\\n\\n**Expected Results:**\\n- Data is read successfully.\\n- Exceptions are handled gracefully with appropriate error messages logged.\\n\\n#### 2. Test Writing Data\\n**Test Case ID:** TC02\\n**Title:** Verify data writing to the destination directory.\\n**Preconditions:** Source directory contains valid Parquet files.\\n**Steps:**\\n1. Call the `write_data` function with the path to the destination directory and a sample dataframe.\\n2. Verify that the function writes the data successfully.\\n3. Verify that the function handles exceptions (e.g., file write failures, permission issues) appropriately.\\n\\n**Expected Results:**\\n- Data is written successfully to the destination directory.\\n- Exceptions are handled gracefully with appropriate error messages logged.\\n\\n#### 3. Test Data Integrity\\n**Test Case ID:** TC03\\n**Title:** Verify data integrity by comparing row counts.\\n**Preconditions:** Source directory contains valid Parquet files.\\n**Steps:**\\n1. Call the `read_data` function to read data from the source directory.\\n2. Call the `write_data` function to write data to the destination directory.\\n3. Call the `verify_data_integrity` function to compare row counts between source and destination dataframes.\\n4. Verify that the row counts match.\\n\\n**Expected Results:**\\n- Row counts match between source and destination dataframes.\\n- Any discrepancies are logged and handled appropriately.\\n\\n#### 4. Test Main Function\\n**Test Case ID:** TC04\\n**Title:** Verify the main function orchestrates the ETL process correctly.\\n**Preconditions:** Source directory contains valid Parquet files.\\n**Steps:**\\n1. Call the `transfer_data` function with paths to the source and destination directories.\\n2. Verify that the function reads data from the source directory.\\n3. Verify that the function writes data to the destination directory.\\n4. Verify that the function verifies data integrity.\\n5. Verify that the function logs start, end, and any errors encountered.\\n\\n**Expected Results:**\\n- Data is read, written, and verified successfully.\\n- Logging captures all necessary information and errors.\\n- Errors are handled gracefully with retries where applicable.\\n\\n#### 5. Test Error Handling\\n**Test Case ID:** TC05\\n**Title:** Verify error handling and logging of errors.\\n**Preconditions:** Source directory contains valid Parquet files.\\n**Steps:**\\n1. Simulate file not found error by providing a non-existent source directory.\\n2. Simulate permission issues by giving read-only access to the destination directory.\\n3. Simulate a transient error and verify retry logic.\\n4. Log all errors and verify the logs for error messages and retry attempts.\\n\\n**Expected Results:**\\n- Errors are caught and logged appropriately.\\n- Retry logic is applied for transient errors.\\n- Error messages are descriptive and meaningful.\\n\\n#### 6. Test Logging\\n**Test Case ID:** TC06\\n**Title:** Verify logging of process information and errors.\\n**Preconditions:** Source directory contains valid Parquet files.\\n**Steps:**\\n1. Call the `transfer_data` function with paths to the source and destination directories.\\n2. Verify that logs are created and contain:\\n   - Start and end of the transfer process.\\n   - Number of files successfully transferred.\\n   - Any errors encountered during the transfer process.\\n\\n**Expected Results:**\\n- Logs contain all necessary information about the process.\\n- Errors are logged with detailed messages.\\n\\n#### 7. Test Performance\\n**Test Case ID:** TC07\\n**Title:** Verify performance of the ETL process.\\n**Preconditions:** Source directory contains a large dataset.\\n**Steps:**\\n1. Call the `transfer_data` function with a large dataset in the source directory.\\n2. Measure the time taken for the entire ETL process.\\n3. Verify that the process completes within an acceptable time frame.\\n\\n**Expected Results:**\\n- The ETL process completes within a reasonable time frame.\\n- Performance optimizations (partitioning, caching) are effective.\\n\\n#### 8. Test Security\\n**Test Case ID:** TC08\\n**Title:** Verify secure data transfer and access control.\\n**Preconditions:** Source and destination directories have appropriate access control.\\n**Steps:**\\n1. Verify that data is encrypted during transfer if required.\\n2. Verify that only authorized users can access the source and destination directories.\\n\\n**Expected Results:**\\n- Data is transferred securely (encrypted if required).\\n- Access control is enforced, and unauthorized access is prevented.\\n\\n### Summary of Test Cases\\n\\n- **TC01:** Test reading data from the source directory.\\n- **TC02:** Test writing data to the destination directory.\\n- **TC03:** Test data integrity verification by comparing row counts.\\n- **TC04:** Test the main function for the entire ETL process.\\n- **TC05:** Test error handling and logging of errors.\\n- **TC06:** Test logging of process information.\\n- **TC07:** Test performance of the ETL process.\\n- **TC08:** Test security measures for data transfer and access control.\\n\\nThese test cases ensure that the ETL process is robust, secure, and performs well under various conditions.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 1183, 'prompt_tokens': 1105, 'total_tokens': 2288, 'completion_time': 5.915, 'prompt_time': 0.057772873, 'queue_time': 0.051656016, 'total_time': 5.972772873}, 'model_name': 'qwen-2.5-32b', 'system_fingerprint': 'fp_c527211fd1', 'finish_reason': 'stop', 'logprobs': None}, id='run-4a0c8682-ad1e-4b25-8964-9cd9dd05ea9a-0', usage_metadata={'input_tokens': 1105, 'output_tokens': 1183, 'total_tokens': 2288}), HumanMessage(content='It looks good now', additional_kwargs={}, response_metadata={})]}, 'user_stories': \"Certainly! Here's a user story based on the requirement to transfer employee data files from one directory to another using PySpark:\\n\\n---\\n\\n**As a** data management administrator,  \\n**I want** to transfer employee data files from an old directory to a new directory using PySpark,  \\n**So that** I can ensure that all employee data is correctly moved and processed in the new directory for reporting and analysis purposes.\\n\\n**Acceptance Criteria:**\\n\\n1. **File Transfer Process**:\\n   - The system should be able to read employee data files from the source directory.\\n   - The system should be able to write the employee data files to the destination directory using PySpark.\\n\\n2. **Data Integrity**:\\n   - Ensure all employee data is accurately transferred without loss or corruption.\\n   - Verify that the data structure and content are the same in the new directory as in the old directory.\\n\\n3. **Error Handling**:\\n   - The system should handle any errors during the transfer process and log them appropriately (e.g., file not found, permission errors, etc.).\\n   - The system should retry file transfer in case of transient errors.\\n\\n4. **Logging**:\\n   - Log the start and end of the transfer process.\\n   - Log any errors encountered during the file transfer process.\\n   - Log the number of files transferred successfully.\\n\\n5. **Performance**:\\n   - The transfer process should be optimized to handle files of varying sizes efficiently.\\n   - The system should be capable of processing a large number of files within a reasonable time frame.\\n\\n6. **Security**:\\n   - Ensure that the data is transferred securely, adhering to any necessary encryption standards.\\n   - Maintain access control to ensure that only authorized users can initiate the transfer process.\\n\\n7. **Testing**:\\n   - Include unit tests to verify that each part of the transfer process works as expected.\\n   - Include integration tests to ensure the entire transfer process from reading to writing is functioning correctly.\\n\\n---\\n\\nThis user story captures the essence of the requirement and includes key acceptance criteria to ensure the successful implementation of the functionality.\", 'func_document': 'Great! Let\\'s proceed with creating a functional design document based on the user story and acceptance criteria provided. This document will outline the detailed design and implementation steps required to achieve the desired functionality.\\n\\n---\\n\\n# Functional Design Document\\n\\n## 1. Overview\\n\\n### 1.1 Purpose\\nThis document outlines the functional design for the process of transferring employee data files from an old directory to a new directory using PySpark. The goal is to ensure that all employee data is accurately transferred and processed in the new directory for reporting and analysis purposes.\\n\\n### 1.2 Scope\\nThe scope of this document includes the design and implementation details for the file transfer process, including data integrity checks, error handling, logging, performance optimization, and security measures.\\n\\n## 2. System Requirements\\n\\n### 2.1 Software Requirements\\n- PySpark\\n- Python (version 3.8 or higher)\\n- Spark (version 3.0 or higher)\\n- Logging library (e.g., Python\\'s `logging` module)\\n\\n### 2.2 Hardware Requirements\\n- Sufficient storage capacity to handle the file transfer process\\n- Adequate computational resources to process large files efficiently\\n\\n## 3. Functional Requirements\\n\\n### 3.1 File Transfer Process\\n#### 3.1.1 Reading Files from Source Directory\\n- Use PySpark to read files from the source directory.\\n- Ensure the files are read in a scalable manner to handle large datasets.\\n\\n#### 3.1.2 Writing Files to Destination Directory\\n- Use PySpark to write the files to the destination directory.\\n- Ensure the files are written in a structured format (e.g., CSV, Parquet) to maintain data integrity.\\n\\n### 3.2 Data Integrity\\n#### 3.2.1 Verify Data Structure and Content\\n- Compare the file metadata (e.g., file size, checksum) between the source and destination directories to ensure data integrity.\\n- Perform a sample validation to check the data content and structure.\\n\\n### 3.3 Error Handling\\n#### 3.3.1 Handle Errors During Transfer\\n- Implement error handling for file not found, permission issues, and other common errors.\\n- Implement retry logic for transient errors.\\n\\n### 3.4 Logging\\n#### 3.4.1 Log Transfer Process\\n- Log the start and end of the transfer process.\\n- Log any errors encountered during the file transfer process.\\n- Log the number of files transferred successfully.\\n\\n### 3.5 Performance Optimization\\n#### 3.5.1 Efficient File Handling\\n- Optimize the file reading and writing processes to handle large files efficiently.\\n- Use PySpark\\'s built-in optimizations for large-scale data processing.\\n\\n### 3.6 Security\\n#### 3.6.1 Secure Data Transfer\\n- Ensure secure file transfer by adhering to encryption standards if necessary.\\n- Implement access controls to ensure only authorized users can initiate the transfer process.\\n\\n## 4. Implementation Details\\n\\n### 4.1 File Transfer Process\\n#### 4.1.1 Reading Files\\n```python\\nfrom pyspark.sql import SparkSession\\n\\nspark = SparkSession.builder.appName(\"EmployeeDataTransfer\").getOrCreate()\\n\\n# Read files from the source directory\\ndf = spark.read.csv(\"path/to/source_directory\", header=True, inferSchema=True)\\n```\\n\\n#### 4.1.2 Writing Files\\n```python\\n# Write files to the destination directory\\ndf.write.csv(\"path/to/destination_directory\", header=True)\\n```\\n\\n### 4.2 Data Integrity\\n#### 4.2.1 Verify Data Structure and Content\\n```python\\ndef verify_data_integrity(source_path, destination_path):\\n    # Read files from both directories\\n    df_source = spark.read.csv(source_path, header=True, inferSchema=True)\\n    df_destination = spark.read.csv(destination_path, header=True, inferSchema=True)\\n\\n    # Compare the data structure and content\\n    return df_source.schema == df_destination.schema and df_source.collect() == df_destination.collect()\\n```\\n\\n### 4.3 Error Handling\\n#### 4.3.1 Handle Errors During Transfer\\n```python\\nimport logging\\n\\ndef transfer_files_with_error_handling(source_path, destination_path):\\n    try:\\n        df = spark.read.csv(source_path, header=True, inferSchema=True)\\n        df.write.csv(destination_path, header=True)\\n        logging.info(\"Files transferred successfully.\")\\n    except Exception as e:\\n        logging.error(f\"Error occurred during file transfer: {str(e)}\")\\n        # Implement retry logic if necessary\\n```\\n\\n### 4.4 Logging\\n#### 4.4.1 Log Transfer Process\\n```python\\nimport logging\\n\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s - %(levelname)s - %(message)s\\')\\n\\ndef log_transfer_process(source_path, destination_path):\\n    logging.info(f\"Starting file transfer from {source_path} to {destination_path}\")\\n    # Call the transfer function\\n    transfer_files_with_error_handling(source_path, destination_path)\\n    logging.info(f\"File transfer completed from {source_path} to {destination_path}\")\\n```\\n\\n### 4.5 Performance Optimization\\n#### 4.5.1 Efficient File Handling\\n- Use PySpark\\'s built-in optimizations such as partitioning and caching to handle large files efficiently.\\n- Ensure that the Spark configuration is tuned for optimal performance (e.g., `spark.executor.memory`, `spark.driver.memory`).\\n\\n### 4.6 Security\\n#### 4.6.1 Secure Data Transfer\\n- Ensure that the file paths and data are handled securely.\\n- Implement encryption and access controls as necessary.\\n\\n## 5. Testing\\n\\n### 5.1 Unit Tests\\n- Test individual components of the file transfer process (e.g., reading files, writing files, error handling).\\n\\n### 5.2 Integration Tests\\n- Test the entire process from reading to writing to ensure the system works as expected.\\n- Validate the data integrity and compare the source and destination files.\\n\\n## 6. Conclusion\\n\\nThis functional design document provides a comprehensive plan for transferring employee data files from an old directory to a new directory using PySpark. The design addresses critical aspects such as data integrity, error handling, logging, performance optimization, and security. The implementation details and testing strategies ensure that the system is robust, reliable, and secure.\\n\\n---\\n\\nThis document should provide a clear roadmap for the development team to implement the required functionality while ensuring all the acceptance criteria are met.', 'tech_document': 'Glad to hear that the user story looks good! Let\\'s proceed with creating a technical design document based on the user story and acceptance criteria. This document will outline the technical approach, components, and implementation details.\\n\\n---\\n\\n# Technical Design Document\\n\\n## 1. Introduction\\n\\nThis document outlines the technical design for the process of transferring employee data files from an old directory to a new directory using PySpark. The goal is to ensure that the data is accurately and securely transferred while maintaining data integrity and logging for error handling and performance.\\n\\n## 2. System Overview\\n\\n### 2.1 Components\\n\\n1. **Source Directory**: Contains the original employee data files.\\n2. **Destination Directory**: Where the employee data files will be moved.\\n3. **PySpark ETL Pipeline**: The core component responsible for reading, processing, and writing the data.\\n4. **Logging System**: Captures detailed logs for each step of the process.\\n5. **Error Handling Mechanism**: Detects and handles errors during the transfer process.\\n\\n### 2.2 Data Flow\\n\\n1. **Read Files**: PySpark reads files from the source directory.\\n2. **Process Files**: PySpark processes the files if necessary (e.g., data transformations).\\n3. **Write Files**: PySpark writes the files to the destination directory.\\n4. **Logging**: Logs the start, end, and any errors during the transfer process.\\n5. **Error Handling**: Manages and retries the transfer in case of transient errors.\\n\\n## 3. Technical Details\\n\\n### 3.1 Source Directory\\n\\n- **Path**: `/path/to/old/directory/`\\n- **File Format**: CSV, Parquet, etc.\\n- **Permissions**: Ensure the ETL process has read permissions.\\n\\n### 3.2 Destination Directory\\n\\n- **Path**: `/path/to/new/directory/`\\n- **File Format**: Same as the source or transformed as needed.\\n- **Permissions**: Ensure the ETL process has write permissions.\\n\\n### 3.3 PySpark ETL Pipeline\\n\\n#### 3.3.1 Reading Files\\n\\n```python\\nfrom pyspark.sql import SparkSession\\n\\n# Initialize SparkSession\\nspark = SparkSession.builder.appName(\"EmployeeDataTransfer\").getOrCreate()\\n\\n# Read files from source directory\\ndf = spark.read.csv(\"/path/to/old/directory/*.csv\", header=True, inferSchema=True)\\n```\\n\\n#### 3.3.2 Processing Files (Optional)\\n\\nIf any data transformations are required, they can be performed here.\\n\\n```python\\n# Example: Transformations\\ndf = df.withColumn(\"new_column\", df[\"existing_column\"] * 2)\\n```\\n\\n#### 3.3.3 Writing Files\\n\\n```python\\n# Write files to destination directory\\ndf.write.csv(\"/path/to/new/directory/employee_data\", header=True)\\n```\\n\\n### 3.4 Logging System\\n\\n#### 3.4.1 Logging Configuration\\n\\n```python\\nimport logging\\n\\n# Configure logging\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s - %(levelname)s - %(message)s\\')\\nlogger = logging.getLogger(__name__)\\n```\\n\\n#### 3.4.2 Log Statements\\n\\n```python\\n# Log start of the process\\nlogger.info(\"Starting data transfer process\")\\n\\n# Log file transfer success\\nlogger.info(f\"File {file_name} transferred successfully to {destination_path}\")\\n\\n# Log any errors\\nlogger.error(f\"Error transferring file {file_name}: {str(e)}\")\\n```\\n\\n### 3.5 Error Handling Mechanism\\n\\n#### 3.5.1 Error Handling Logic\\n\\n```python\\ndef transfer_files(source_dir, destination_dir):\\n    try:\\n        # Read files\\n        df = spark.read.csv(source_dir + \"/*.csv\", header=True, inferSchema=True)\\n\\n        # Process files if necessary\\n        # df = df.withColumn(\"new_column\", df[\"existing_column\"] * 2)\\n\\n        # Write files\\n        df.write.csv(destination_dir + \"/employee_data\", header=True)\\n\\n        # Log success\\n        logger.info(f\"Files successfully transferred from {source_dir} to {destination_dir}\")\\n\\n    except Exception as e:\\n        # Log error\\n        logger.error(f\"Error during file transfer: {str(e)}\")\\n        # Retry mechanism (optional)\\n        logger.info(\"Retrying file transfer...\")\\n        transfer_files(source_dir, destination_dir)  # Recursive retry\\n\\n# Example usage\\ntransfer_files(\"/path/to/old/directory/\", \"/path/to/new/directory/\")\\n```\\n\\n### 3.6 Security\\n\\n- **Data Encryption**: Ensure data is encrypted during transfer if necessary.\\n- **Access Control**: Ensure only authorized users have access to the source and destination directories.\\n\\n### 3.7 Testing\\n\\n#### 3.7.1 Unit Tests\\n\\n- **Test Data Reading**: Verify that files are read correctly from the source directory.\\n- **Test Data Writing**: Verify that files are written correctly to the destination directory.\\n- **Test Error Handling**: Verify that errors are logged and handled appropriately.\\n\\n```python\\nimport unittest\\n\\nclass TestETLPipeline(unittest.TestCase):\\n    def test_read_files(self):\\n        df = spark.read.csv(\"/path/to/old/directory/*.csv\", header=True, inferSchema=True)\\n        self.assertIsNotNone(df)\\n        self.assertGreater(df.count(), 0)\\n\\n    def test_write_files(self):\\n        df = spark.createDataFrame([(1, \"John\"), (2, \"Jane\")], [\"id\", \"name\"])\\n        df.write.csv(\"/path/to/new/directory/test_data\", header=True)\\n        self.assertTrue(os.path.exists(\"/path/to/new/directory/test_data\"))\\n\\n    def test_error_handling(self):\\n        # Simulate error handling\\n        try:\\n            raise Exception(\"Simulated error\")\\n        except Exception as e:\\n            logger.error(f\"Error during file transfer: {str(e)}\")\\n\\nif __name__ == \\'__main__\\':\\n    unittest.main()\\n```\\n\\n#### 3.7.2 Integration Tests\\n\\n- **End-to-End Testing**: Verify that the entire process works as expected from reading to writing.\\n- **Performance Testing**: Ensure the system can handle a large number of files efficiently.\\n\\n## 4. Deployment and Maintenance\\n\\n### 4.1 Deployment\\n\\n- **Environment Setup**: Ensure the PySpark environment is set up with all necessary dependencies.\\n- **Scheduling**: Schedule the ETL job to run automatically at regular intervals using tools like Apache Airflow or cron jobs.\\n\\n### 4.2 Maintenance\\n\\n- **Monitoring**: Set up monitoring to track the success rate, transfer times, and any errors.\\n- **Documentation**: Maintain up-to-date documentation for the ETL process and any changes made.\\n- **Backup and Recovery**: Ensure there is a backup and recovery plan in case of data loss or corruption.\\n\\n## 5. Conclusion\\n\\nThis technical design document outlines the technical approach for transferring employee data files from an old directory to a new directory using PySpark. The design includes detailed steps for reading, processing, writing, logging, error handling, and security. The document also includes testing and deployment strategies to ensure the reliability and maintainability of the system.\\n\\n---\\n\\nFeel free to provide any additional details or specific requirements if needed!', 'code_content': 'Based on the provided summaries, the following Python script implements the ETL process for transferring employee data files from an old to a new directory using PySpark. It includes reading and writing files, data integrity checks, error handling, logging, and basic security measures.\\n\\n### Required Libraries\\nFirst, ensure you have the necessary libraries installed:\\n```bash\\npip install pyspark\\n```\\n\\n### Python Script\\n\\n```python\\nimport logging\\nfrom pyspark.sql import SparkSession\\n\\n# Initialize Logging\\nlogging.basicConfig(level=logging.INFO, format=\\'%(asctime)s - %(levelname)s - %(message)s\\')\\nlogger = logging.getLogger(__name__)\\n\\n# Define Source and Destination Paths\\nSOURCE_DIR = \"path/to/source/employee_data\"\\nDESTINATION_DIR = \"path/to/destination/employee_data\"\\n\\n# Initialize PySpark\\nspark = SparkSession.builder.appName(\"EmployeeDataTransfer\").getOrCreate()\\n\\ndef read_data(path):\\n    \"\"\"Reads data from a specified path.\"\"\"\\n    try:\\n        df = spark.read.parquet(path)\\n        logger.info(f\"Data read successfully from {path}\")\\n        return df\\n    except Exception as e:\\n        logger.error(f\"Error reading data from {path}: {str(e)}\")\\n        return None\\n\\ndef write_data(df, path):\\n    \"\"\"Writes data to a specified path.\"\"\"\\n    try:\\n        df.write.mode(\\'overwrite\\').parquet(path)\\n        logger.info(f\"Data written successfully to {path}\")\\n    except Exception as e:\\n        logger.error(f\"Error writing data to {path}: {str(e)}\")\\n\\ndef verify_data_integrity(source_df, destination_df):\\n    \"\"\"Verifies data integrity by comparing row counts.\"\"\"\\n    if source_df.count() == destination_df.count():\\n        logger.info(\"Data integrity verified.\")\\n        return True\\n    else:\\n        logger.error(\"Data integrity check failed.\")\\n        return False\\n\\ndef transfer_data():\\n    \"\"\"Main function to transfer data from source to destination.\"\"\"\\n    source_df = read_data(SOURCE_DIR)\\n    if source_df is None:\\n        logger.error(\"Failed to read source data. Exiting.\")\\n        return\\n\\n    write_data(source_df, DESTINATION_DIR)\\n    \\n    destination_df = read_data(DESTINATION_DIR)\\n    if destination_df is None:\\n        logger.error(\"Failed to read destination data. Exiting.\")\\n        return\\n\\n    if verify_data_integrity(source_df, destination_df):\\n        logger.info(\"Data transfer completed successfully.\")\\n    else:\\n        logger.error(\"Data transfer failed due to integrity issues.\")\\n\\nif __name__ == \"__main__\":\\n    try:\\n        transfer_data()\\n    except Exception as e:\\n        logger.error(f\"An error occurred during data transfer: {str(e)}\")\\n    finally:\\n        # Stop the Spark session\\n        spark.stop()\\n```\\n\\n### Explanation of the Code:\\n\\n1. **Logging**: The `logging` module is configured to log information and errors.\\n2. **Source and Destination Paths**: Define the paths for the source and destination directories.\\n3. **PySpark Initialization**: A Spark session is created.\\n4. **Read Data**: The `read_data` function reads data from the specified path and handles exceptions.\\n5. **Write Data**: The `write_data` function writes data to the specified path and handles exceptions.\\n6. **Verify Data Integrity**: The `verify_data_integrity` function checks if row counts match between source and destination dataframes.\\n7. **Transfer Data**: The `transfer_data` function orchestrates the reading, writing, and integrity verification.\\n8. **Main Execution Block**: The `transfer_data` function is called within a try-except block to handle any unexpected errors, and the Spark session is stopped in a `finally` block.\\n\\n### Additional Considerations:\\n- **Security**: For security, consider implementing encryption for data transfer, securing access to directories, and ensuring that only authorized users can initiate the transfer.\\n- **Testing**: Unit tests can be written to test individual functions and integration tests to ensure the entire process works end-to-end.\\n- **Automation**: Use tools like Apache Airflow or cron jobs to automate the ETL job at regular intervals.\\n- **Maintenance**: Regularly monitor the jobs for performance and errors, and maintain up-to-date documentation for the process.\\n\\nThis script provides a basic framework for the ETL process and can be extended to meet additional requirements like more granular data transformations, additional error handling, or integration with other systems.', 'test_cases_content': 'Based on the provided summaries, the following test cases are designed to ensure that all aspects of the ETL process, including data integrity, error handling, logging, performance, and security, are thoroughly tested.\\n\\n### Test Cases\\n\\n#### 1. Test Reading Data\\n**Test Case ID:** TC01\\n**Title:** Verify data reading from the source directory.\\n**Preconditions:** Source directory contains valid Parquet files.\\n**Steps:**\\n1. Call the `read_data` function with the path to the source directory.\\n2. Verify that the function reads the data successfully.\\n3. Verify that the function handles exceptions (e.g., file not found, permission issues) appropriately.\\n\\n**Expected Results:**\\n- Data is read successfully.\\n- Exceptions are handled gracefully with appropriate error messages logged.\\n\\n#### 2. Test Writing Data\\n**Test Case ID:** TC02\\n**Title:** Verify data writing to the destination directory.\\n**Preconditions:** Source directory contains valid Parquet files.\\n**Steps:**\\n1. Call the `write_data` function with the path to the destination directory and a sample dataframe.\\n2. Verify that the function writes the data successfully.\\n3. Verify that the function handles exceptions (e.g., file write failures, permission issues) appropriately.\\n\\n**Expected Results:**\\n- Data is written successfully to the destination directory.\\n- Exceptions are handled gracefully with appropriate error messages logged.\\n\\n#### 3. Test Data Integrity\\n**Test Case ID:** TC03\\n**Title:** Verify data integrity by comparing row counts.\\n**Preconditions:** Source directory contains valid Parquet files.\\n**Steps:**\\n1. Call the `read_data` function to read data from the source directory.\\n2. Call the `write_data` function to write data to the destination directory.\\n3. Call the `verify_data_integrity` function to compare row counts between source and destination dataframes.\\n4. Verify that the row counts match.\\n\\n**Expected Results:**\\n- Row counts match between source and destination dataframes.\\n- Any discrepancies are logged and handled appropriately.\\n\\n#### 4. Test Main Function\\n**Test Case ID:** TC04\\n**Title:** Verify the main function orchestrates the ETL process correctly.\\n**Preconditions:** Source directory contains valid Parquet files.\\n**Steps:**\\n1. Call the `transfer_data` function with paths to the source and destination directories.\\n2. Verify that the function reads data from the source directory.\\n3. Verify that the function writes data to the destination directory.\\n4. Verify that the function verifies data integrity.\\n5. Verify that the function logs start, end, and any errors encountered.\\n\\n**Expected Results:**\\n- Data is read, written, and verified successfully.\\n- Logging captures all necessary information and errors.\\n- Errors are handled gracefully with retries where applicable.\\n\\n#### 5. Test Error Handling\\n**Test Case ID:** TC05\\n**Title:** Verify error handling and logging of errors.\\n**Preconditions:** Source directory contains valid Parquet files.\\n**Steps:**\\n1. Simulate file not found error by providing a non-existent source directory.\\n2. Simulate permission issues by giving read-only access to the destination directory.\\n3. Simulate a transient error and verify retry logic.\\n4. Log all errors and verify the logs for error messages and retry attempts.\\n\\n**Expected Results:**\\n- Errors are caught and logged appropriately.\\n- Retry logic is applied for transient errors.\\n- Error messages are descriptive and meaningful.\\n\\n#### 6. Test Logging\\n**Test Case ID:** TC06\\n**Title:** Verify logging of process information and errors.\\n**Preconditions:** Source directory contains valid Parquet files.\\n**Steps:**\\n1. Call the `transfer_data` function with paths to the source and destination directories.\\n2. Verify that logs are created and contain:\\n   - Start and end of the transfer process.\\n   - Number of files successfully transferred.\\n   - Any errors encountered during the transfer process.\\n\\n**Expected Results:**\\n- Logs contain all necessary information about the process.\\n- Errors are logged with detailed messages.\\n\\n#### 7. Test Performance\\n**Test Case ID:** TC07\\n**Title:** Verify performance of the ETL process.\\n**Preconditions:** Source directory contains a large dataset.\\n**Steps:**\\n1. Call the `transfer_data` function with a large dataset in the source directory.\\n2. Measure the time taken for the entire ETL process.\\n3. Verify that the process completes within an acceptable time frame.\\n\\n**Expected Results:**\\n- The ETL process completes within a reasonable time frame.\\n- Performance optimizations (partitioning, caching) are effective.\\n\\n#### 8. Test Security\\n**Test Case ID:** TC08\\n**Title:** Verify secure data transfer and access control.\\n**Preconditions:** Source and destination directories have appropriate access control.\\n**Steps:**\\n1. Verify that data is encrypted during transfer if required.\\n2. Verify that only authorized users can access the source and destination directories.\\n\\n**Expected Results:**\\n- Data is transferred securely (encrypted if required).\\n- Access control is enforced, and unauthorized access is prevented.\\n\\n### Summary of Test Cases\\n\\n- **TC01:** Test reading data from the source directory.\\n- **TC02:** Test writing data to the destination directory.\\n- **TC03:** Test data integrity verification by comparing row counts.\\n- **TC04:** Test the main function for the entire ETL process.\\n- **TC05:** Test error handling and logging of errors.\\n- **TC06:** Test logging of process information.\\n- **TC07:** Test performance of the ETL process.\\n- **TC08:** Test security measures for data transfer and access control.\\n\\nThese test cases ensure that the ETL process is robust, secure, and performs well under various conditions.'}, next=('executingTestCases',), config={'configurable': {'thread_id': '38', 'checkpoint_ns': '', 'checkpoint_id': '1f007e20-a4a5-6d4f-800d-27f83fa49ad5'}}, metadata={'source': 'loop', 'writes': {'testcases_feedback': {'messages': {'messages': [AIMessage(content='Based on the provided summaries, the following test cases are designed to ensure that all aspects of the ETL process, including data integrity, error handling, logging, performance, and security, are thoroughly tested.\\n\\n### Test Cases\\n\\n#### 1. Test Reading Data\\n**Test Case ID:** TC01\\n**Title:** Verify data reading from the source directory.\\n**Preconditions:** Source directory contains valid Parquet files.\\n**Steps:**\\n1. Call the `read_data` function with the path to the source directory.\\n2. Verify that the function reads the data successfully.\\n3. Verify that the function handles exceptions (e.g., file not found, permission issues) appropriately.\\n\\n**Expected Results:**\\n- Data is read successfully.\\n- Exceptions are handled gracefully with appropriate error messages logged.\\n\\n#### 2. Test Writing Data\\n**Test Case ID:** TC02\\n**Title:** Verify data writing to the destination directory.\\n**Preconditions:** Source directory contains valid Parquet files.\\n**Steps:**\\n1. Call the `write_data` function with the path to the destination directory and a sample dataframe.\\n2. Verify that the function writes the data successfully.\\n3. Verify that the function handles exceptions (e.g., file write failures, permission issues) appropriately.\\n\\n**Expected Results:**\\n- Data is written successfully to the destination directory.\\n- Exceptions are handled gracefully with appropriate error messages logged.\\n\\n#### 3. Test Data Integrity\\n**Test Case ID:** TC03\\n**Title:** Verify data integrity by comparing row counts.\\n**Preconditions:** Source directory contains valid Parquet files.\\n**Steps:**\\n1. Call the `read_data` function to read data from the source directory.\\n2. Call the `write_data` function to write data to the destination directory.\\n3. Call the `verify_data_integrity` function to compare row counts between source and destination dataframes.\\n4. Verify that the row counts match.\\n\\n**Expected Results:**\\n- Row counts match between source and destination dataframes.\\n- Any discrepancies are logged and handled appropriately.\\n\\n#### 4. Test Main Function\\n**Test Case ID:** TC04\\n**Title:** Verify the main function orchestrates the ETL process correctly.\\n**Preconditions:** Source directory contains valid Parquet files.\\n**Steps:**\\n1. Call the `transfer_data` function with paths to the source and destination directories.\\n2. Verify that the function reads data from the source directory.\\n3. Verify that the function writes data to the destination directory.\\n4. Verify that the function verifies data integrity.\\n5. Verify that the function logs start, end, and any errors encountered.\\n\\n**Expected Results:**\\n- Data is read, written, and verified successfully.\\n- Logging captures all necessary information and errors.\\n- Errors are handled gracefully with retries where applicable.\\n\\n#### 5. Test Error Handling\\n**Test Case ID:** TC05\\n**Title:** Verify error handling and logging of errors.\\n**Preconditions:** Source directory contains valid Parquet files.\\n**Steps:**\\n1. Simulate file not found error by providing a non-existent source directory.\\n2. Simulate permission issues by giving read-only access to the destination directory.\\n3. Simulate a transient error and verify retry logic.\\n4. Log all errors and verify the logs for error messages and retry attempts.\\n\\n**Expected Results:**\\n- Errors are caught and logged appropriately.\\n- Retry logic is applied for transient errors.\\n- Error messages are descriptive and meaningful.\\n\\n#### 6. Test Logging\\n**Test Case ID:** TC06\\n**Title:** Verify logging of process information and errors.\\n**Preconditions:** Source directory contains valid Parquet files.\\n**Steps:**\\n1. Call the `transfer_data` function with paths to the source and destination directories.\\n2. Verify that logs are created and contain:\\n   - Start and end of the transfer process.\\n   - Number of files successfully transferred.\\n   - Any errors encountered during the transfer process.\\n\\n**Expected Results:**\\n- Logs contain all necessary information about the process.\\n- Errors are logged with detailed messages.\\n\\n#### 7. Test Performance\\n**Test Case ID:** TC07\\n**Title:** Verify performance of the ETL process.\\n**Preconditions:** Source directory contains a large dataset.\\n**Steps:**\\n1. Call the `transfer_data` function with a large dataset in the source directory.\\n2. Measure the time taken for the entire ETL process.\\n3. Verify that the process completes within an acceptable time frame.\\n\\n**Expected Results:**\\n- The ETL process completes within a reasonable time frame.\\n- Performance optimizations (partitioning, caching) are effective.\\n\\n#### 8. Test Security\\n**Test Case ID:** TC08\\n**Title:** Verify secure data transfer and access control.\\n**Preconditions:** Source and destination directories have appropriate access control.\\n**Steps:**\\n1. Verify that data is encrypted during transfer if required.\\n2. Verify that only authorized users can access the source and destination directories.\\n\\n**Expected Results:**\\n- Data is transferred securely (encrypted if required).\\n- Access control is enforced, and unauthorized access is prevented.\\n\\n### Summary of Test Cases\\n\\n- **TC01:** Test reading data from the source directory.\\n- **TC02:** Test writing data to the destination directory.\\n- **TC03:** Test data integrity verification by comparing row counts.\\n- **TC04:** Test the main function for the entire ETL process.\\n- **TC05:** Test error handling and logging of errors.\\n- **TC06:** Test logging of process information.\\n- **TC07:** Test performance of the ETL process.\\n- **TC08:** Test security measures for data transfer and access control.\\n\\nThese test cases ensure that the ETL process is robust, secure, and performs well under various conditions.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 1183, 'prompt_tokens': 1105, 'total_tokens': 2288, 'completion_time': 5.915, 'prompt_time': 0.057772873, 'queue_time': 0.051656016, 'total_time': 5.972772873}, 'model_name': 'qwen-2.5-32b', 'system_fingerprint': 'fp_c527211fd1', 'finish_reason': 'stop', 'logprobs': None}, id='run-4a0c8682-ad1e-4b25-8964-9cd9dd05ea9a-0', usage_metadata={'input_tokens': 1105, 'output_tokens': 1183, 'total_tokens': 2288}), HumanMessage(content='It looks good now', additional_kwargs={}, response_metadata={})]}}}, 'thread_id': '38', 'step': 13, 'parents': {}}, created_at='2025-03-23T12:26:26.513441+00:00', parent_config={'configurable': {'thread_id': '38', 'checkpoint_ns': '', 'checkpoint_id': '1f007e20-8b89-6d5f-800c-1a909f2c76a1'}}, tasks=(PregelTask(id='bfadbf83-f535-e967-b256-cfe7ec3b01c8', name='executingTestCases', path=('__pregel_pull', 'executingTestCases'), error=\"TypeError('list indices must be integers or slices, not str')\", interrupts=(), state=None, result=None),))"
      ]
     },
     "execution_count": 846,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state\n",
    "#state[\"user_stories\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AIMessage(content='Based on the provided summaries, here\\'s a PySpark script that implements the file transfer process as described:\\n\\n```python\\nfrom pyspark.sql import SparkSession\\nimport logging\\nfrom datetime import datetime\\n\\n# Initialize Logger\\nlogging.basicConfig(level=logging.INFO)\\nlogger = logging.getLogger(__name__)\\n\\n# Define source and destination paths\\nsource_path = \"/source/employee_data.csv\"\\ndestination_path = \"/destination/employee_data.csv\"\\n\\ndef main():\\n    try:\\n        # Initialize Spark session\\n        spark = SparkSession.builder.appName(\"EmployeeDataTransfer\").getOrCreate()\\n        \\n        # Log start time\\n        start_time = datetime.now()\\n        logger.info(f\"Transfer process started at {start_time}\")\\n        \\n        # Read data from source\\n        logger.info(f\"Reading data from {source_path}\")\\n        df = spark.read.csv(source_path, header=True, inferSchema=True)\\n        \\n        # Write data to destination\\n        logger.info(f\"Writing data to {destination_path}\")\\n        df.write.csv(destination_path, header=True)\\n        \\n        # Log completion time and success message\\n        end_time = datetime.now()\\n        logger.info(f\"Transfer process completed at {end_time}\")\\n        logger.info(f\"Data transfer was successful. Time taken: {end_time - start_time}\")\\n    \\n    except FileNotFoundError as fnfe:\\n        logger.error(f\"File not found: {fnfe}\")\\n    \\n    except Exception as e:\\n        logger.error(f\"An error occurred: {e}\")\\n    \\n    finally:\\n        # Stop Spark session\\n        spark.stop()\\n\\nif __name__ == \"__main__\":\\n    main()\\n```\\n\\n### Explanation of the Script:\\n\\n1. **Initialization and Setup:**\\n   - The script initializes a PySpark session and sets up a logger for logging the process.\\n\\n2. **Main Function:**\\n   - The `main()` function orchestrates the file transfer process.\\n   - It starts by logging the start time.\\n   - It reads the CSV file from the source path using the `spark.read.csv` function.\\n   - It writes the DataFrame to the destination path using the `df.write.csv` function.\\n   - It logs the completion time and the status of the transfer process.\\n\\n3. **Error Handling:**\\n   - The script catches `FileNotFoundError` and general exceptions, logging the error messages appropriately.\\n   \\n4. **Finalization:**\\n   - The Spark session is stopped in the `finally` block to ensure resources are released.\\n\\n### Logging:\\n- The script uses the Python `logging` module to log the start and end times of the process, as well as any errors that occur during the transfer.\\n\\n### Testing and Deployment:\\n- The script can be executed using `spark-submit` or within a PySpark environment.\\n- Unit and integration tests would involve checking the existence of the output file, verifying the contents, and ensuring the logs are correctly populated.\\n\\n### Maintenance:\\n- Regular checks of the log files are recommended to identify any errors or issues.\\n- Any changes in the file format or paths should be updated in the script and the documentation.\\n\\nThis script provides a robust and maintainable approach to transferring the `employee_data.csv` file using PySpark, with detailed logging and error handling as specified.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 648, 'prompt_tokens': 480, 'total_tokens': 1128, 'completion_time': 3.24, 'prompt_time': 0.028783407, 'queue_time': 0.05180976200000001, 'total_time': 3.268783407}, 'model_name': 'qwen-2.5-32b', 'system_fingerprint': 'fp_c527211fd1', 'finish_reason': 'stop', 'logprobs': None}, id='run-3672cbf7-983c-44c1-969d-b9ab0c4c240e-0', usage_metadata={'input_tokens': 480, 'output_tokens': 648, 'total_tokens': 1128})]\n",
      "{'messages': {'messages': [AIMessage(content='Based on the provided summaries, here\\'s a PySpark script that implements the file transfer process as described:\\n\\n```python\\nfrom pyspark.sql import SparkSession\\nimport logging\\nfrom datetime import datetime\\n\\n# Initialize Logger\\nlogging.basicConfig(level=logging.INFO)\\nlogger = logging.getLogger(__name__)\\n\\n# Define source and destination paths\\nsource_path = \"/source/employee_data.csv\"\\ndestination_path = \"/destination/employee_data.csv\"\\n\\ndef main():\\n    try:\\n        # Initialize Spark session\\n        spark = SparkSession.builder.appName(\"EmployeeDataTransfer\").getOrCreate()\\n        \\n        # Log start time\\n        start_time = datetime.now()\\n        logger.info(f\"Transfer process started at {start_time}\")\\n        \\n        # Read data from source\\n        logger.info(f\"Reading data from {source_path}\")\\n        df = spark.read.csv(source_path, header=True, inferSchema=True)\\n        \\n        # Write data to destination\\n        logger.info(f\"Writing data to {destination_path}\")\\n        df.write.csv(destination_path, header=True)\\n        \\n        # Log completion time and success message\\n        end_time = datetime.now()\\n        logger.info(f\"Transfer process completed at {end_time}\")\\n        logger.info(f\"Data transfer was successful. Time taken: {end_time - start_time}\")\\n    \\n    except FileNotFoundError as fnfe:\\n        logger.error(f\"File not found: {fnfe}\")\\n    \\n    except Exception as e:\\n        logger.error(f\"An error occurred: {e}\")\\n    \\n    finally:\\n        # Stop Spark session\\n        spark.stop()\\n\\nif __name__ == \"__main__\":\\n    main()\\n```\\n\\n### Explanation of the Script:\\n\\n1. **Initialization and Setup:**\\n   - The script initializes a PySpark session and sets up a logger for logging the process.\\n\\n2. **Main Function:**\\n   - The `main()` function orchestrates the file transfer process.\\n   - It starts by logging the start time.\\n   - It reads the CSV file from the source path using the `spark.read.csv` function.\\n   - It writes the DataFrame to the destination path using the `df.write.csv` function.\\n   - It logs the completion time and the status of the transfer process.\\n\\n3. **Error Handling:**\\n   - The script catches `FileNotFoundError` and general exceptions, logging the error messages appropriately.\\n   \\n4. **Finalization:**\\n   - The Spark session is stopped in the `finally` block to ensure resources are released.\\n\\n### Logging:\\n- The script uses the Python `logging` module to log the start and end times of the process, as well as any errors that occur during the transfer.\\n\\n### Testing and Deployment:\\n- The script can be executed using `spark-submit` or within a PySpark environment.\\n- Unit and integration tests would involve checking the existence of the output file, verifying the contents, and ensuring the logs are correctly populated.\\n\\n### Maintenance:\\n- Regular checks of the log files are recommended to identify any errors or issues.\\n- Any changes in the file format or paths should be updated in the script and the documentation.\\n\\nThis script provides a robust and maintainable approach to transferring the `employee_data.csv` file using PySpark, with detailed logging and error handling as specified.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 648, 'prompt_tokens': 480, 'total_tokens': 1128, 'completion_time': 3.24, 'prompt_time': 0.028783407, 'queue_time': 0.05180976200000001, 'total_time': 3.268783407}, 'model_name': 'qwen-2.5-32b', 'system_fingerprint': 'fp_c527211fd1', 'finish_reason': 'stop', 'logprobs': None}, id='run-3672cbf7-983c-44c1-969d-b9ab0c4c240e-0', usage_metadata={'input_tokens': 480, 'output_tokens': 648, 'total_tokens': 1128}), HumanMessage(content='It looks good now', additional_kwargs={}, response_metadata={})]}, 'user_stories': 'Certainly! Below is a user story that captures the requirement of transferring employee data files from one directory to another using PySpark.\\n\\n---\\n\\n**User Story: Transfer Employee Data File Using PySpark**\\n\\n**As a** Data Engineer  \\n**I want to** transfer an employee data file from one directory to another using PySpark  \\n**So that** the data can be processed and analyzed in the destination directory without manual intervention.\\n\\n**Acceptance Criteria:**\\n\\n1. **File Identification:**\\n   - The system should be able to identify the specific employee data file (e.g., `employee_data.csv`) in the source directory.\\n\\n2. **PySpark Implementation:**\\n   - The transfer process must be implemented using PySpark.\\n   - The PySpark job should read the employee data file from the source directory.\\n\\n3. **File Transfer:**\\n   - The system should transfer the identified file to the specified destination directory.\\n   - The file should be transferred in its original format (e.g., CSV).\\n\\n4. **Error Handling:**\\n   - If the file does not exist in the source directory, the system should log an error message.\\n   - If there is an error during the transfer process, it should be logged, and the job should fail gracefully.\\n\\n5. **Logging:**\\n   - The system should log the start and completion of the file transfer process.\\n   - The log should include the date, time, and status of the transfer.\\n\\n6. **Confirmation:**\\n   - After successful transfer, the system should confirm the file transfer by logging the success message.\\n\\n---\\n\\nThis user story covers the key aspects of the requirement while ensuring that the process is automated, reliable, and traceable through logging.', 'func_document': 'Great! I\\'m glad you found the user story clear and well-defined. Let\\'s proceed with creating a functional design document based on this user story. Below is a functional design document that outlines the detailed steps and components needed to implement the file transfer process using PySpark.\\n\\n---\\n\\n# Functional Design Document: Employee Data File Transfer Using PySpark\\n\\n## 1. Introduction\\n\\nThis document outlines the design for transferring an employee data file from a source directory to a destination directory using PySpark. The goal is to automate the transfer process, ensuring reliability and traceability through comprehensive logging.\\n\\n## 2. Requirements\\n\\n### 2.1 Functional Requirements\\n\\n1. **File Identification:**\\n   - Identify the specific employee data file (e.g., `employee_data.csv`) in the source directory.\\n\\n2. **PySpark Implementation:**\\n   - Implement the transfer process using PySpark.\\n   - Read the employee data file from the source directory using PySpark.\\n\\n3. **File Transfer:**\\n   - Transfer the identified file to the specified destination directory.\\n   - Maintain the original file format (e.g., CSV).\\n\\n4. **Error Handling:**\\n   - Log an error message if the file does not exist in the source directory.\\n   - Log and handle errors gracefully during the transfer process.\\n\\n5. **Logging:**\\n   - Log the start and completion of the file transfer process.\\n   - Include date, time, and status of the transfer in the logs.\\n\\n6. **Confirmation:**\\n   - Log a success message after the file is successfully transferred.\\n\\n### 2.2 Non-Functional Requirements\\n\\n1. **Performance:**\\n   - The transfer process should be efficient and should complete within a reasonable time frame.\\n\\n2. **Reliability:**\\n   - The system should handle errors gracefully and provide meaningful error messages.\\n\\n3. **Scalability:**\\n   - The solution should be scalable to handle larger files and increased data volume.\\n\\n## 3. System Architecture\\n\\n### 3.1 Components\\n\\n1. **Source Directory:**\\n   - Directory containing the employee data file (e.g., `/source/employee_data.csv`).\\n\\n2. **Destination Directory:**\\n   - Directory where the employee data file will be transferred (e.g., `/destination/employee_data.csv`).\\n\\n3. **PySpark Job:**\\n   - PySpark script to read the file from the source directory and write it to the destination directory.\\n\\n4. **Logging Mechanism:**\\n   - Mechanism to log the start, progress, and completion of the transfer process.\\n\\n### 3.2 Workflow\\n\\n1. **Initialization:**\\n   - Initialize the PySpark job.\\n   - Log the start of the job with the current timestamp.\\n\\n2. **File Identification:**\\n   - Identify the file in the source directory.\\n   - Log the file identification process.\\n\\n3. **File Reading:**\\n   - Use PySpark to read the file from the source directory.\\n   - Log the reading process.\\n\\n4. **File Writing:**\\n   - Use PySpark to write the file to the destination directory.\\n   - Log the writing process.\\n\\n5. **Completion:**\\n   - Log the completion of the job with the current timestamp.\\n   - Log a success message if the file is transferred successfully.\\n\\n6. **Error Handling:**\\n   - If the file does not exist in the source directory, log an error message.\\n   - If there is an error during the transfer process, log the error and handle it gracefully.\\n\\n## 4. Detailed Design\\n\\n### 4.1 PySpark Job Implementation\\n\\n```python\\nfrom pyspark.sql import SparkSession\\nimport logging\\nimport os\\n\\n# Initialize logging\\nlogging.basicConfig(level=logging.INFO)\\nlogger = logging.getLogger(__name__)\\n\\ndef transfer_employee_data(source_path, destination_path):\\n    try:\\n        # Initialize Spark session\\n        spark = SparkSession.builder.appName(\"EmployeeDataTransfer\").getOrCreate()\\n        \\n        # Log the start of the job\\n        logger.info(f\"Starting the transfer process at {os.path.basename(__file__)}\")\\n        \\n        # Read the file from the source directory\\n        df = spark.read.csv(source_path, header=True, inferSchema=True)\\n        logger.info(f\"File read from source directory: {source_path}\")\\n        \\n        # Write the file to the destination directory\\n        df.write.csv(destination_path, header=True, mode=\\'overwrite\\')\\n        logger.info(f\"File written to destination directory: {destination_path}\")\\n        \\n        # Log the completion of the job\\n        logger.info(f\"Transfer process completed successfully at {os.path.basename(__file__)}\")\\n    \\n    except FileNotFoundError as e:\\n        logger.error(f\"Error: File not found in source directory. Details: {e}\")\\n    except Exception as e:\\n        logger.error(f\"Error occurred during the transfer process. Details: {e}\")\\n\\nif __name__ == \"__main__\":\\n    source_path = \"/source/employee_data.csv\"\\n    destination_path = \"/destination/employee_data.csv\"\\n    transfer_employee_data(source_path, destination_path)\\n```\\n\\n### 4.2 Logging Mechanism\\n\\n- **Log File Configuration:**\\n  - Configure logging to capture the start, progress, and completion of the transfer process.\\n  - Logs should include timestamps, file paths, and status messages.\\n\\n### 4.3 Error Handling Mechanism\\n\\n- **File Not Found:**\\n  - If the file does not exist in the source directory, log an error message.\\n- **General Error Handling:**\\n  - Log any other exceptions that occur during the transfer process.\\n\\n## 5. Testing\\n\\n### 5.1 Unit Tests\\n\\n- **Test Case 1: Successful Transfer**\\n  - Verify that the file is successfully transferred from the source to the destination directory.\\n- **Test Case 2: File Not Found**\\n  - Verify that the system logs an error if the file does not exist in the source directory.\\n- **Test Case 3: General Error Handling**\\n  - Verify that the system logs and handles other exceptions gracefully.\\n\\n### 5.2 Integration Tests\\n\\n- **Test Case 1: End-to-End Transfer Process**\\n  - Verify the entire transfer process from initialization to completion, including logging and error handling.\\n\\n## 6. Deployment\\n\\n### 6.1 Deployment Steps\\n\\n1. **Environment Setup:**\\n   - Ensure that the PySpark environment is set up with necessary dependencies.\\n   \\n2. **Script Execution:**\\n   - Execute the PySpark script to transfer the file.\\n   - Use a scheduler (e.g., cron job) to run the script at regular intervals if needed.\\n\\n3. **Monitoring:**\\n   - Monitor the logs to ensure the process runs as expected.\\n   - Ensure the destination directory contains the transferred file.\\n\\n## 7. Maintenance\\n\\n### 7.1 Logging and Monitoring\\n\\n- Regularly review the logs to identify any issues or patterns.\\n- Set up alerts for critical errors.\\n\\n### 7.2 Documentation\\n\\n- Maintain up-to-date documentation of the script and the process flow.\\n- Document any changes or updates to the system.\\n\\n---\\n\\nThis functional design document provides a comprehensive guide to implementing and maintaining the file transfer process using PySpark. It covers the architecture, implementation details, testing, and maintenance steps to ensure the process is reliable and traceable.', 'tech_document': 'Glad to hear that the user story looks good! Let\\'s proceed with creating a technical design document based on the user story. This document will provide a detailed technical blueprint for implementing the file transfer process using PySpark.\\n\\n---\\n\\n# Technical Design Document\\n\\n## 1. Overview\\n\\nThis technical design document outlines the process of transferring an employee data file from a source directory to a destination directory using PySpark. The goal is to automate this process to ensure reliable and traceable data movement.\\n\\n## 2. Requirements\\n\\n### 2.1 Functional Requirements\\n\\n1. **File Identification:**\\n   - The system must identify the specific employee data file (e.g., `employee_data.csv`) in the source directory.\\n   \\n2. **PySpark Implementation:**\\n   - The file transfer process must be implemented using PySpark.\\n   - The PySpark job should read the employee data file from the source directory.\\n\\n3. **File Transfer:**\\n   - The system must transfer the identified file from the source directory to the specified destination directory.\\n   - The file should be transferred in its original format (e.g., CSV).\\n\\n### 2.2 Non-Functional Requirements\\n\\n1. **Error Handling:**\\n   - If the file does not exist in the source directory, the system should log an error message.\\n   - If there is an error during the transfer process, it should be logged, and the job should fail gracefully.\\n\\n2. **Logging:**\\n   - The system should log the start and completion of the file transfer process.\\n   - The log should include the date, time, and status of the transfer.\\n\\n3. **Confirmation:**\\n   - After a successful transfer, the system should confirm the file transfer by logging a success message.\\n\\n## 3. System Architecture\\n\\n### 3.1 Components\\n\\n1. **Source Directory:** The directory where the original employee data file resides.\\n2. **Destination Directory:** The directory where the employee data file needs to be transferred.\\n3. **PySpark Job:** A PySpark script that reads the file from the source directory and writes it to the destination directory.\\n4. **Logging Mechanism:** A logging system to record the process and any errors encountered.\\n\\n### 3.2 Workflow\\n\\n1. **Initialization:**\\n   - The PySpark job starts and initializes the logging mechanism.\\n\\n2. **File Identification:**\\n   - The job identifies the specific employee data file in the source directory.\\n\\n3. **File Transfer:**\\n   - The job reads the file from the source directory using PySpark.\\n   - The job writes the file to the destination directory.\\n\\n4. **Error Handling:**\\n   - If the file does not exist in the source directory, log an error message.\\n   - If there is an error during the transfer process, log the error and fail gracefully.\\n\\n5. **Logging:**\\n   - Log the start and completion of the file transfer process.\\n   - Include the date, time, and status of the transfer in the log.\\n\\n6. **Confirmation:**\\n   - Log a success message if the file is successfully transferred.\\n\\n## 4. Implementation Details\\n\\n### 4.1 Environment Setup\\n\\n1. **Development Environment:**\\n   - Ensure PySpark is installed and configured.\\n   - Set up a logging library (e.g., `logging` module in Python).\\n\\n2. **PySpark Configuration:**\\n   - Configure PySpark to read from and write to HDFS (Hadoop Distributed File System) or a local file system.\\n\\n### 4.2 Code Implementation\\n\\n#### 4.2.1 PySpark Script\\n\\n```python\\nimport logging\\nfrom pyspark.sql import SparkSession\\n\\n# Initialize logger\\nlogging.basicConfig(level=logging.INFO)\\nlogger = logging.getLogger()\\n\\ndef transfer_file(source_path, destination_path):\\n    try:\\n        # Initialize Spark session\\n        spark = SparkSession.builder.appName(\"EmployeeDataTransfer\").getOrCreate()\\n\\n        # Log the start of the process\\n        logger.info(f\"Starting file transfer from {source_path} to {destination_path}\")\\n\\n        # Read the file from the source directory\\n        df = spark.read.csv(source_path, header=True, inferSchema=True)\\n\\n        # Write the file to the destination directory\\n        df.write.csv(destination_path, header=True)\\n\\n        # Log the success of the process\\n        logger.info(f\"File successfully transferred from {source_path} to {destination_path}\")\\n\\n    except FileNotFoundError:\\n        # Log an error if the file does not exist in the source directory\\n        logger.error(f\"File not found at {source_path}\")\\n\\n    except Exception as e:\\n        # Log any other errors encountered during the transfer\\n        logger.error(f\"An error occurred during file transfer: {str(e)}\")\\n\\n    finally:\\n        # Stop the Spark session\\n        spark.stop()\\n\\nif __name__ == \"__main__\":\\n    # Define source and destination paths\\n    source_directory = \"/path/to/source/directory\"\\n    destination_directory = \"/path/to/destination/directory\"\\n\\n    # Call the transfer function\\n    transfer_file(f\"{source_directory}/employee_data.csv\", f\"{destination_directory}/employee_data.csv\")\\n```\\n\\n### 4.3 Logging Configuration\\n\\n- **Logging Level:** `INFO` and `ERROR`\\n- **Log File:** Optionally, configure the logger to write logs to a file (e.g., `transfer_logs.log`).\\n\\n### 4.4 Error Handling\\n\\n- **File Not Found:** Log an error if the file does not exist in the source directory.\\n- **General Errors:** Log any other errors encountered during the transfer process and handle them gracefully.\\n\\n### 4.5 Confirmation\\n\\n- After a successful transfer, log a success message indicating the completion of the transfer.\\n\\n## 5. Deployment and Testing\\n\\n### 5.1 Deployment\\n\\n1. **Environment Setup:**\\n   - Ensure the environment is set up with the necessary dependencies (PySpark, logging libraries).\\n   \\n2. **Execution:**\\n   - Run the PySpark script using the appropriate Spark-submit command or within a PySpark environment.\\n\\n### 5.2 Testing\\n\\n1. **Unit Testing:**\\n   - Write unit tests to verify the functionality of the PySpark script.\\n   - Test scenarios where the file exists and does not exist in the source directory.\\n\\n2. **Integration Testing:**\\n   - Perform integration testing to ensure the script works correctly with the source and destination directories.\\n\\n3. **Logging Verification:**\\n   - Verify that logs are generated correctly and logged messages are accurate.\\n\\n## 6. Maintenance and Monitoring\\n\\n### 6.1 Maintenance\\n\\n- Regularly check the logs to ensure the script runs without errors.\\n- Update the script in case of changes in file format or directory paths.\\n\\n### 6.2 Monitoring\\n\\n- Use monitoring tools to track the execution of the PySpark job and identify any potential issues.\\n\\n---\\n\\nThis technical design document provides a comprehensive guide for implementing the file transfer process using PySpark, including detailed steps, error handling, and logging mechanisms.', 'code_content': 'Based on the provided summaries, here\\'s a PySpark script that implements the file transfer process as described:\\n\\n```python\\nfrom pyspark.sql import SparkSession\\nimport logging\\nfrom datetime import datetime\\n\\n# Initialize Logger\\nlogging.basicConfig(level=logging.INFO)\\nlogger = logging.getLogger(__name__)\\n\\n# Define source and destination paths\\nsource_path = \"/source/employee_data.csv\"\\ndestination_path = \"/destination/employee_data.csv\"\\n\\ndef main():\\n    try:\\n        # Initialize Spark session\\n        spark = SparkSession.builder.appName(\"EmployeeDataTransfer\").getOrCreate()\\n        \\n        # Log start time\\n        start_time = datetime.now()\\n        logger.info(f\"Transfer process started at {start_time}\")\\n        \\n        # Read data from source\\n        logger.info(f\"Reading data from {source_path}\")\\n        df = spark.read.csv(source_path, header=True, inferSchema=True)\\n        \\n        # Write data to destination\\n        logger.info(f\"Writing data to {destination_path}\")\\n        df.write.csv(destination_path, header=True)\\n        \\n        # Log completion time and success message\\n        end_time = datetime.now()\\n        logger.info(f\"Transfer process completed at {end_time}\")\\n        logger.info(f\"Data transfer was successful. Time taken: {end_time - start_time}\")\\n    \\n    except FileNotFoundError as fnfe:\\n        logger.error(f\"File not found: {fnfe}\")\\n    \\n    except Exception as e:\\n        logger.error(f\"An error occurred: {e}\")\\n    \\n    finally:\\n        # Stop Spark session\\n        spark.stop()\\n\\nif __name__ == \"__main__\":\\n    main()\\n```\\n\\n### Explanation of the Script:\\n\\n1. **Initialization and Setup:**\\n   - The script initializes a PySpark session and sets up a logger for logging the process.\\n\\n2. **Main Function:**\\n   - The `main()` function orchestrates the file transfer process.\\n   - It starts by logging the start time.\\n   - It reads the CSV file from the source path using the `spark.read.csv` function.\\n   - It writes the DataFrame to the destination path using the `df.write.csv` function.\\n   - It logs the completion time and the status of the transfer process.\\n\\n3. **Error Handling:**\\n   - The script catches `FileNotFoundError` and general exceptions, logging the error messages appropriately.\\n   \\n4. **Finalization:**\\n   - The Spark session is stopped in the `finally` block to ensure resources are released.\\n\\n### Logging:\\n- The script uses the Python `logging` module to log the start and end times of the process, as well as any errors that occur during the transfer.\\n\\n### Testing and Deployment:\\n- The script can be executed using `spark-submit` or within a PySpark environment.\\n- Unit and integration tests would involve checking the existence of the output file, verifying the contents, and ensuring the logs are correctly populated.\\n\\n### Maintenance:\\n- Regular checks of the log files are recommended to identify any errors or issues.\\n- Any changes in the file format or paths should be updated in the script and the documentation.\\n\\nThis script provides a robust and maintainable approach to transferring the `employee_data.csv` file using PySpark, with detailed logging and error handling as specified.'}\n"
     ]
    }
   ],
   "source": [
    "# Update the graph with new feedback\n",
    "#graph.update_state(thread, {\"messages\": MessagesState(messages=[HumanMessage(content=\"It looks good now\")])})\n",
    "\n",
    "# Get the current state\n",
    "state_snapshot = graph.get_state(thread)\n",
    "state_values = state_snapshot.values\n",
    "\n",
    "# Since 'messages' is a list, we directly access and update it\n",
    "current_messages = state_values[\"messages\"]  # This is a list of messages\n",
    "print(current_messages)\n",
    "new_messages = current_messages + [HumanMessage(content=\"It looks good now\")]\n",
    "\n",
    "updated_messages = MessagesState(messages=new_messages)\n",
    "\n",
    "# Update the state with the modified messages list\n",
    "#graph.update_state(thread, {\"messages\": new_messages})\n",
    "graph.update_state(thread, {\"messages\": updated_messages})\n",
    "\n",
    "\n",
    "state_snapshot = graph.get_state(thread)\n",
    "state = state_snapshot.values  # Extract the underlying dictionary\n",
    "print(state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': {'messages': [AIMessage(content='Based on the provided summaries, here\\'s a PySpark script that implements the file transfer process as described:\\n\\n```python\\nfrom pyspark.sql import SparkSession\\nimport logging\\nfrom datetime import datetime\\n\\n# Initialize Logger\\nlogging.basicConfig(level=logging.INFO)\\nlogger = logging.getLogger(__name__)\\n\\n# Define source and destination paths\\nsource_path = \"/source/employee_data.csv\"\\ndestination_path = \"/destination/employee_data.csv\"\\n\\ndef main():\\n    try:\\n        # Initialize Spark session\\n        spark = SparkSession.builder.appName(\"EmployeeDataTransfer\").getOrCreate()\\n        \\n        # Log start time\\n        start_time = datetime.now()\\n        logger.info(f\"Transfer process started at {start_time}\")\\n        \\n        # Read data from source\\n        logger.info(f\"Reading data from {source_path}\")\\n        df = spark.read.csv(source_path, header=True, inferSchema=True)\\n        \\n        # Write data to destination\\n        logger.info(f\"Writing data to {destination_path}\")\\n        df.write.csv(destination_path, header=True)\\n        \\n        # Log completion time and success message\\n        end_time = datetime.now()\\n        logger.info(f\"Transfer process completed at {end_time}\")\\n        logger.info(f\"Data transfer was successful. Time taken: {end_time - start_time}\")\\n    \\n    except FileNotFoundError as fnfe:\\n        logger.error(f\"File not found: {fnfe}\")\\n    \\n    except Exception as e:\\n        logger.error(f\"An error occurred: {e}\")\\n    \\n    finally:\\n        # Stop Spark session\\n        spark.stop()\\n\\nif __name__ == \"__main__\":\\n    main()\\n```\\n\\n### Explanation of the Script:\\n\\n1. **Initialization and Setup:**\\n   - The script initializes a PySpark session and sets up a logger for logging the process.\\n\\n2. **Main Function:**\\n   - The `main()` function orchestrates the file transfer process.\\n   - It starts by logging the start time.\\n   - It reads the CSV file from the source path using the `spark.read.csv` function.\\n   - It writes the DataFrame to the destination path using the `df.write.csv` function.\\n   - It logs the completion time and the status of the transfer process.\\n\\n3. **Error Handling:**\\n   - The script catches `FileNotFoundError` and general exceptions, logging the error messages appropriately.\\n   \\n4. **Finalization:**\\n   - The Spark session is stopped in the `finally` block to ensure resources are released.\\n\\n### Logging:\\n- The script uses the Python `logging` module to log the start and end times of the process, as well as any errors that occur during the transfer.\\n\\n### Testing and Deployment:\\n- The script can be executed using `spark-submit` or within a PySpark environment.\\n- Unit and integration tests would involve checking the existence of the output file, verifying the contents, and ensuring the logs are correctly populated.\\n\\n### Maintenance:\\n- Regular checks of the log files are recommended to identify any errors or issues.\\n- Any changes in the file format or paths should be updated in the script and the documentation.\\n\\nThis script provides a robust and maintainable approach to transferring the `employee_data.csv` file using PySpark, with detailed logging and error handling as specified.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 648, 'prompt_tokens': 480, 'total_tokens': 1128, 'completion_time': 3.24, 'prompt_time': 0.028783407, 'queue_time': 0.05180976200000001, 'total_time': 3.268783407}, 'model_name': 'qwen-2.5-32b', 'system_fingerprint': 'fp_c527211fd1', 'finish_reason': 'stop', 'logprobs': None}, id='run-3672cbf7-983c-44c1-969d-b9ab0c4c240e-0', usage_metadata={'input_tokens': 480, 'output_tokens': 648, 'total_tokens': 1128}), HumanMessage(content='It looks good now', additional_kwargs={}, response_metadata={})]}, 'user_stories': 'Certainly! Below is a user story that captures the requirement of transferring employee data files from one directory to another using PySpark.\\n\\n---\\n\\n**User Story: Transfer Employee Data File Using PySpark**\\n\\n**As a** Data Engineer  \\n**I want to** transfer an employee data file from one directory to another using PySpark  \\n**So that** the data can be processed and analyzed in the destination directory without manual intervention.\\n\\n**Acceptance Criteria:**\\n\\n1. **File Identification:**\\n   - The system should be able to identify the specific employee data file (e.g., `employee_data.csv`) in the source directory.\\n\\n2. **PySpark Implementation:**\\n   - The transfer process must be implemented using PySpark.\\n   - The PySpark job should read the employee data file from the source directory.\\n\\n3. **File Transfer:**\\n   - The system should transfer the identified file to the specified destination directory.\\n   - The file should be transferred in its original format (e.g., CSV).\\n\\n4. **Error Handling:**\\n   - If the file does not exist in the source directory, the system should log an error message.\\n   - If there is an error during the transfer process, it should be logged, and the job should fail gracefully.\\n\\n5. **Logging:**\\n   - The system should log the start and completion of the file transfer process.\\n   - The log should include the date, time, and status of the transfer.\\n\\n6. **Confirmation:**\\n   - After successful transfer, the system should confirm the file transfer by logging the success message.\\n\\n---\\n\\nThis user story covers the key aspects of the requirement while ensuring that the process is automated, reliable, and traceable through logging.', 'func_document': 'Great! I\\'m glad you found the user story clear and well-defined. Let\\'s proceed with creating a functional design document based on this user story. Below is a functional design document that outlines the detailed steps and components needed to implement the file transfer process using PySpark.\\n\\n---\\n\\n# Functional Design Document: Employee Data File Transfer Using PySpark\\n\\n## 1. Introduction\\n\\nThis document outlines the design for transferring an employee data file from a source directory to a destination directory using PySpark. The goal is to automate the transfer process, ensuring reliability and traceability through comprehensive logging.\\n\\n## 2. Requirements\\n\\n### 2.1 Functional Requirements\\n\\n1. **File Identification:**\\n   - Identify the specific employee data file (e.g., `employee_data.csv`) in the source directory.\\n\\n2. **PySpark Implementation:**\\n   - Implement the transfer process using PySpark.\\n   - Read the employee data file from the source directory using PySpark.\\n\\n3. **File Transfer:**\\n   - Transfer the identified file to the specified destination directory.\\n   - Maintain the original file format (e.g., CSV).\\n\\n4. **Error Handling:**\\n   - Log an error message if the file does not exist in the source directory.\\n   - Log and handle errors gracefully during the transfer process.\\n\\n5. **Logging:**\\n   - Log the start and completion of the file transfer process.\\n   - Include date, time, and status of the transfer in the logs.\\n\\n6. **Confirmation:**\\n   - Log a success message after the file is successfully transferred.\\n\\n### 2.2 Non-Functional Requirements\\n\\n1. **Performance:**\\n   - The transfer process should be efficient and should complete within a reasonable time frame.\\n\\n2. **Reliability:**\\n   - The system should handle errors gracefully and provide meaningful error messages.\\n\\n3. **Scalability:**\\n   - The solution should be scalable to handle larger files and increased data volume.\\n\\n## 3. System Architecture\\n\\n### 3.1 Components\\n\\n1. **Source Directory:**\\n   - Directory containing the employee data file (e.g., `/source/employee_data.csv`).\\n\\n2. **Destination Directory:**\\n   - Directory where the employee data file will be transferred (e.g., `/destination/employee_data.csv`).\\n\\n3. **PySpark Job:**\\n   - PySpark script to read the file from the source directory and write it to the destination directory.\\n\\n4. **Logging Mechanism:**\\n   - Mechanism to log the start, progress, and completion of the transfer process.\\n\\n### 3.2 Workflow\\n\\n1. **Initialization:**\\n   - Initialize the PySpark job.\\n   - Log the start of the job with the current timestamp.\\n\\n2. **File Identification:**\\n   - Identify the file in the source directory.\\n   - Log the file identification process.\\n\\n3. **File Reading:**\\n   - Use PySpark to read the file from the source directory.\\n   - Log the reading process.\\n\\n4. **File Writing:**\\n   - Use PySpark to write the file to the destination directory.\\n   - Log the writing process.\\n\\n5. **Completion:**\\n   - Log the completion of the job with the current timestamp.\\n   - Log a success message if the file is transferred successfully.\\n\\n6. **Error Handling:**\\n   - If the file does not exist in the source directory, log an error message.\\n   - If there is an error during the transfer process, log the error and handle it gracefully.\\n\\n## 4. Detailed Design\\n\\n### 4.1 PySpark Job Implementation\\n\\n```python\\nfrom pyspark.sql import SparkSession\\nimport logging\\nimport os\\n\\n# Initialize logging\\nlogging.basicConfig(level=logging.INFO)\\nlogger = logging.getLogger(__name__)\\n\\ndef transfer_employee_data(source_path, destination_path):\\n    try:\\n        # Initialize Spark session\\n        spark = SparkSession.builder.appName(\"EmployeeDataTransfer\").getOrCreate()\\n        \\n        # Log the start of the job\\n        logger.info(f\"Starting the transfer process at {os.path.basename(__file__)}\")\\n        \\n        # Read the file from the source directory\\n        df = spark.read.csv(source_path, header=True, inferSchema=True)\\n        logger.info(f\"File read from source directory: {source_path}\")\\n        \\n        # Write the file to the destination directory\\n        df.write.csv(destination_path, header=True, mode=\\'overwrite\\')\\n        logger.info(f\"File written to destination directory: {destination_path}\")\\n        \\n        # Log the completion of the job\\n        logger.info(f\"Transfer process completed successfully at {os.path.basename(__file__)}\")\\n    \\n    except FileNotFoundError as e:\\n        logger.error(f\"Error: File not found in source directory. Details: {e}\")\\n    except Exception as e:\\n        logger.error(f\"Error occurred during the transfer process. Details: {e}\")\\n\\nif __name__ == \"__main__\":\\n    source_path = \"/source/employee_data.csv\"\\n    destination_path = \"/destination/employee_data.csv\"\\n    transfer_employee_data(source_path, destination_path)\\n```\\n\\n### 4.2 Logging Mechanism\\n\\n- **Log File Configuration:**\\n  - Configure logging to capture the start, progress, and completion of the transfer process.\\n  - Logs should include timestamps, file paths, and status messages.\\n\\n### 4.3 Error Handling Mechanism\\n\\n- **File Not Found:**\\n  - If the file does not exist in the source directory, log an error message.\\n- **General Error Handling:**\\n  - Log any other exceptions that occur during the transfer process.\\n\\n## 5. Testing\\n\\n### 5.1 Unit Tests\\n\\n- **Test Case 1: Successful Transfer**\\n  - Verify that the file is successfully transferred from the source to the destination directory.\\n- **Test Case 2: File Not Found**\\n  - Verify that the system logs an error if the file does not exist in the source directory.\\n- **Test Case 3: General Error Handling**\\n  - Verify that the system logs and handles other exceptions gracefully.\\n\\n### 5.2 Integration Tests\\n\\n- **Test Case 1: End-to-End Transfer Process**\\n  - Verify the entire transfer process from initialization to completion, including logging and error handling.\\n\\n## 6. Deployment\\n\\n### 6.1 Deployment Steps\\n\\n1. **Environment Setup:**\\n   - Ensure that the PySpark environment is set up with necessary dependencies.\\n   \\n2. **Script Execution:**\\n   - Execute the PySpark script to transfer the file.\\n   - Use a scheduler (e.g., cron job) to run the script at regular intervals if needed.\\n\\n3. **Monitoring:**\\n   - Monitor the logs to ensure the process runs as expected.\\n   - Ensure the destination directory contains the transferred file.\\n\\n## 7. Maintenance\\n\\n### 7.1 Logging and Monitoring\\n\\n- Regularly review the logs to identify any issues or patterns.\\n- Set up alerts for critical errors.\\n\\n### 7.2 Documentation\\n\\n- Maintain up-to-date documentation of the script and the process flow.\\n- Document any changes or updates to the system.\\n\\n---\\n\\nThis functional design document provides a comprehensive guide to implementing and maintaining the file transfer process using PySpark. It covers the architecture, implementation details, testing, and maintenance steps to ensure the process is reliable and traceable.', 'tech_document': 'Glad to hear that the user story looks good! Let\\'s proceed with creating a technical design document based on the user story. This document will provide a detailed technical blueprint for implementing the file transfer process using PySpark.\\n\\n---\\n\\n# Technical Design Document\\n\\n## 1. Overview\\n\\nThis technical design document outlines the process of transferring an employee data file from a source directory to a destination directory using PySpark. The goal is to automate this process to ensure reliable and traceable data movement.\\n\\n## 2. Requirements\\n\\n### 2.1 Functional Requirements\\n\\n1. **File Identification:**\\n   - The system must identify the specific employee data file (e.g., `employee_data.csv`) in the source directory.\\n   \\n2. **PySpark Implementation:**\\n   - The file transfer process must be implemented using PySpark.\\n   - The PySpark job should read the employee data file from the source directory.\\n\\n3. **File Transfer:**\\n   - The system must transfer the identified file from the source directory to the specified destination directory.\\n   - The file should be transferred in its original format (e.g., CSV).\\n\\n### 2.2 Non-Functional Requirements\\n\\n1. **Error Handling:**\\n   - If the file does not exist in the source directory, the system should log an error message.\\n   - If there is an error during the transfer process, it should be logged, and the job should fail gracefully.\\n\\n2. **Logging:**\\n   - The system should log the start and completion of the file transfer process.\\n   - The log should include the date, time, and status of the transfer.\\n\\n3. **Confirmation:**\\n   - After a successful transfer, the system should confirm the file transfer by logging a success message.\\n\\n## 3. System Architecture\\n\\n### 3.1 Components\\n\\n1. **Source Directory:** The directory where the original employee data file resides.\\n2. **Destination Directory:** The directory where the employee data file needs to be transferred.\\n3. **PySpark Job:** A PySpark script that reads the file from the source directory and writes it to the destination directory.\\n4. **Logging Mechanism:** A logging system to record the process and any errors encountered.\\n\\n### 3.2 Workflow\\n\\n1. **Initialization:**\\n   - The PySpark job starts and initializes the logging mechanism.\\n\\n2. **File Identification:**\\n   - The job identifies the specific employee data file in the source directory.\\n\\n3. **File Transfer:**\\n   - The job reads the file from the source directory using PySpark.\\n   - The job writes the file to the destination directory.\\n\\n4. **Error Handling:**\\n   - If the file does not exist in the source directory, log an error message.\\n   - If there is an error during the transfer process, log the error and fail gracefully.\\n\\n5. **Logging:**\\n   - Log the start and completion of the file transfer process.\\n   - Include the date, time, and status of the transfer in the log.\\n\\n6. **Confirmation:**\\n   - Log a success message if the file is successfully transferred.\\n\\n## 4. Implementation Details\\n\\n### 4.1 Environment Setup\\n\\n1. **Development Environment:**\\n   - Ensure PySpark is installed and configured.\\n   - Set up a logging library (e.g., `logging` module in Python).\\n\\n2. **PySpark Configuration:**\\n   - Configure PySpark to read from and write to HDFS (Hadoop Distributed File System) or a local file system.\\n\\n### 4.2 Code Implementation\\n\\n#### 4.2.1 PySpark Script\\n\\n```python\\nimport logging\\nfrom pyspark.sql import SparkSession\\n\\n# Initialize logger\\nlogging.basicConfig(level=logging.INFO)\\nlogger = logging.getLogger()\\n\\ndef transfer_file(source_path, destination_path):\\n    try:\\n        # Initialize Spark session\\n        spark = SparkSession.builder.appName(\"EmployeeDataTransfer\").getOrCreate()\\n\\n        # Log the start of the process\\n        logger.info(f\"Starting file transfer from {source_path} to {destination_path}\")\\n\\n        # Read the file from the source directory\\n        df = spark.read.csv(source_path, header=True, inferSchema=True)\\n\\n        # Write the file to the destination directory\\n        df.write.csv(destination_path, header=True)\\n\\n        # Log the success of the process\\n        logger.info(f\"File successfully transferred from {source_path} to {destination_path}\")\\n\\n    except FileNotFoundError:\\n        # Log an error if the file does not exist in the source directory\\n        logger.error(f\"File not found at {source_path}\")\\n\\n    except Exception as e:\\n        # Log any other errors encountered during the transfer\\n        logger.error(f\"An error occurred during file transfer: {str(e)}\")\\n\\n    finally:\\n        # Stop the Spark session\\n        spark.stop()\\n\\nif __name__ == \"__main__\":\\n    # Define source and destination paths\\n    source_directory = \"/path/to/source/directory\"\\n    destination_directory = \"/path/to/destination/directory\"\\n\\n    # Call the transfer function\\n    transfer_file(f\"{source_directory}/employee_data.csv\", f\"{destination_directory}/employee_data.csv\")\\n```\\n\\n### 4.3 Logging Configuration\\n\\n- **Logging Level:** `INFO` and `ERROR`\\n- **Log File:** Optionally, configure the logger to write logs to a file (e.g., `transfer_logs.log`).\\n\\n### 4.4 Error Handling\\n\\n- **File Not Found:** Log an error if the file does not exist in the source directory.\\n- **General Errors:** Log any other errors encountered during the transfer process and handle them gracefully.\\n\\n### 4.5 Confirmation\\n\\n- After a successful transfer, log a success message indicating the completion of the transfer.\\n\\n## 5. Deployment and Testing\\n\\n### 5.1 Deployment\\n\\n1. **Environment Setup:**\\n   - Ensure the environment is set up with the necessary dependencies (PySpark, logging libraries).\\n   \\n2. **Execution:**\\n   - Run the PySpark script using the appropriate Spark-submit command or within a PySpark environment.\\n\\n### 5.2 Testing\\n\\n1. **Unit Testing:**\\n   - Write unit tests to verify the functionality of the PySpark script.\\n   - Test scenarios where the file exists and does not exist in the source directory.\\n\\n2. **Integration Testing:**\\n   - Perform integration testing to ensure the script works correctly with the source and destination directories.\\n\\n3. **Logging Verification:**\\n   - Verify that logs are generated correctly and logged messages are accurate.\\n\\n## 6. Maintenance and Monitoring\\n\\n### 6.1 Maintenance\\n\\n- Regularly check the logs to ensure the script runs without errors.\\n- Update the script in case of changes in file format or directory paths.\\n\\n### 6.2 Monitoring\\n\\n- Use monitoring tools to track the execution of the PySpark job and identify any potential issues.\\n\\n---\\n\\nThis technical design document provides a comprehensive guide for implementing the file transfer process using PySpark, including detailed steps, error handling, and logging mechanisms.', 'code_content': 'Based on the provided summaries, here\\'s a PySpark script that implements the file transfer process as described:\\n\\n```python\\nfrom pyspark.sql import SparkSession\\nimport logging\\nfrom datetime import datetime\\n\\n# Initialize Logger\\nlogging.basicConfig(level=logging.INFO)\\nlogger = logging.getLogger(__name__)\\n\\n# Define source and destination paths\\nsource_path = \"/source/employee_data.csv\"\\ndestination_path = \"/destination/employee_data.csv\"\\n\\ndef main():\\n    try:\\n        # Initialize Spark session\\n        spark = SparkSession.builder.appName(\"EmployeeDataTransfer\").getOrCreate()\\n        \\n        # Log start time\\n        start_time = datetime.now()\\n        logger.info(f\"Transfer process started at {start_time}\")\\n        \\n        # Read data from source\\n        logger.info(f\"Reading data from {source_path}\")\\n        df = spark.read.csv(source_path, header=True, inferSchema=True)\\n        \\n        # Write data to destination\\n        logger.info(f\"Writing data to {destination_path}\")\\n        df.write.csv(destination_path, header=True)\\n        \\n        # Log completion time and success message\\n        end_time = datetime.now()\\n        logger.info(f\"Transfer process completed at {end_time}\")\\n        logger.info(f\"Data transfer was successful. Time taken: {end_time - start_time}\")\\n    \\n    except FileNotFoundError as fnfe:\\n        logger.error(f\"File not found: {fnfe}\")\\n    \\n    except Exception as e:\\n        logger.error(f\"An error occurred: {e}\")\\n    \\n    finally:\\n        # Stop Spark session\\n        spark.stop()\\n\\nif __name__ == \"__main__\":\\n    main()\\n```\\n\\n### Explanation of the Script:\\n\\n1. **Initialization and Setup:**\\n   - The script initializes a PySpark session and sets up a logger for logging the process.\\n\\n2. **Main Function:**\\n   - The `main()` function orchestrates the file transfer process.\\n   - It starts by logging the start time.\\n   - It reads the CSV file from the source path using the `spark.read.csv` function.\\n   - It writes the DataFrame to the destination path using the `df.write.csv` function.\\n   - It logs the completion time and the status of the transfer process.\\n\\n3. **Error Handling:**\\n   - The script catches `FileNotFoundError` and general exceptions, logging the error messages appropriately.\\n   \\n4. **Finalization:**\\n   - The Spark session is stopped in the `finally` block to ensure resources are released.\\n\\n### Logging:\\n- The script uses the Python `logging` module to log the start and end times of the process, as well as any errors that occur during the transfer.\\n\\n### Testing and Deployment:\\n- The script can be executed using `spark-submit` or within a PySpark environment.\\n- Unit and integration tests would involve checking the existence of the output file, verifying the contents, and ensuring the logs are correctly populated.\\n\\n### Maintenance:\\n- Regular checks of the log files are recommended to identify any errors or issues.\\n- Any changes in the file format or paths should be updated in the script and the documentation.\\n\\nThis script provides a robust and maintainable approach to transferring the `employee_data.csv` file using PySpark, with detailed logging and error handling as specified.'}\n",
      "Decision made by evaluator: positive\n",
      "{'messages': {'messages': [AIMessage(content='Based on the provided summaries, here\\'s a PySpark script that implements the file transfer process as described:\\n\\n```python\\nfrom pyspark.sql import SparkSession\\nimport logging\\nfrom datetime import datetime\\n\\n# Initialize Logger\\nlogging.basicConfig(level=logging.INFO)\\nlogger = logging.getLogger(__name__)\\n\\n# Define source and destination paths\\nsource_path = \"/source/employee_data.csv\"\\ndestination_path = \"/destination/employee_data.csv\"\\n\\ndef main():\\n    try:\\n        # Initialize Spark session\\n        spark = SparkSession.builder.appName(\"EmployeeDataTransfer\").getOrCreate()\\n        \\n        # Log start time\\n        start_time = datetime.now()\\n        logger.info(f\"Transfer process started at {start_time}\")\\n        \\n        # Read data from source\\n        logger.info(f\"Reading data from {source_path}\")\\n        df = spark.read.csv(source_path, header=True, inferSchema=True)\\n        \\n        # Write data to destination\\n        logger.info(f\"Writing data to {destination_path}\")\\n        df.write.csv(destination_path, header=True)\\n        \\n        # Log completion time and success message\\n        end_time = datetime.now()\\n        logger.info(f\"Transfer process completed at {end_time}\")\\n        logger.info(f\"Data transfer was successful. Time taken: {end_time - start_time}\")\\n    \\n    except FileNotFoundError as fnfe:\\n        logger.error(f\"File not found: {fnfe}\")\\n    \\n    except Exception as e:\\n        logger.error(f\"An error occurred: {e}\")\\n    \\n    finally:\\n        # Stop Spark session\\n        spark.stop()\\n\\nif __name__ == \"__main__\":\\n    main()\\n```\\n\\n### Explanation of the Script:\\n\\n1. **Initialization and Setup:**\\n   - The script initializes a PySpark session and sets up a logger for logging the process.\\n\\n2. **Main Function:**\\n   - The `main()` function orchestrates the file transfer process.\\n   - It starts by logging the start time.\\n   - It reads the CSV file from the source path using the `spark.read.csv` function.\\n   - It writes the DataFrame to the destination path using the `df.write.csv` function.\\n   - It logs the completion time and the status of the transfer process.\\n\\n3. **Error Handling:**\\n   - The script catches `FileNotFoundError` and general exceptions, logging the error messages appropriately.\\n   \\n4. **Finalization:**\\n   - The Spark session is stopped in the `finally` block to ensure resources are released.\\n\\n### Logging:\\n- The script uses the Python `logging` module to log the start and end times of the process, as well as any errors that occur during the transfer.\\n\\n### Testing and Deployment:\\n- The script can be executed using `spark-submit` or within a PySpark environment.\\n- Unit and integration tests would involve checking the existence of the output file, verifying the contents, and ensuring the logs are correctly populated.\\n\\n### Maintenance:\\n- Regular checks of the log files are recommended to identify any errors or issues.\\n- Any changes in the file format or paths should be updated in the script and the documentation.\\n\\nThis script provides a robust and maintainable approach to transferring the `employee_data.csv` file using PySpark, with detailed logging and error handling as specified.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 648, 'prompt_tokens': 480, 'total_tokens': 1128, 'completion_time': 3.24, 'prompt_time': 0.028783407, 'queue_time': 0.05180976200000001, 'total_time': 3.268783407}, 'model_name': 'qwen-2.5-32b', 'system_fingerprint': 'fp_c527211fd1', 'finish_reason': 'stop', 'logprobs': None}, id='run-3672cbf7-983c-44c1-969d-b9ab0c4c240e-0', usage_metadata={'input_tokens': 480, 'output_tokens': 648, 'total_tokens': 1128}), HumanMessage(content='It looks good now', additional_kwargs={}, response_metadata={})]}, 'user_stories': 'Certainly! Below is a user story that captures the requirement of transferring employee data files from one directory to another using PySpark.\\n\\n---\\n\\n**User Story: Transfer Employee Data File Using PySpark**\\n\\n**As a** Data Engineer  \\n**I want to** transfer an employee data file from one directory to another using PySpark  \\n**So that** the data can be processed and analyzed in the destination directory without manual intervention.\\n\\n**Acceptance Criteria:**\\n\\n1. **File Identification:**\\n   - The system should be able to identify the specific employee data file (e.g., `employee_data.csv`) in the source directory.\\n\\n2. **PySpark Implementation:**\\n   - The transfer process must be implemented using PySpark.\\n   - The PySpark job should read the employee data file from the source directory.\\n\\n3. **File Transfer:**\\n   - The system should transfer the identified file to the specified destination directory.\\n   - The file should be transferred in its original format (e.g., CSV).\\n\\n4. **Error Handling:**\\n   - If the file does not exist in the source directory, the system should log an error message.\\n   - If there is an error during the transfer process, it should be logged, and the job should fail gracefully.\\n\\n5. **Logging:**\\n   - The system should log the start and completion of the file transfer process.\\n   - The log should include the date, time, and status of the transfer.\\n\\n6. **Confirmation:**\\n   - After successful transfer, the system should confirm the file transfer by logging the success message.\\n\\n---\\n\\nThis user story covers the key aspects of the requirement while ensuring that the process is automated, reliable, and traceable through logging.', 'func_document': 'Great! I\\'m glad you found the user story clear and well-defined. Let\\'s proceed with creating a functional design document based on this user story. Below is a functional design document that outlines the detailed steps and components needed to implement the file transfer process using PySpark.\\n\\n---\\n\\n# Functional Design Document: Employee Data File Transfer Using PySpark\\n\\n## 1. Introduction\\n\\nThis document outlines the design for transferring an employee data file from a source directory to a destination directory using PySpark. The goal is to automate the transfer process, ensuring reliability and traceability through comprehensive logging.\\n\\n## 2. Requirements\\n\\n### 2.1 Functional Requirements\\n\\n1. **File Identification:**\\n   - Identify the specific employee data file (e.g., `employee_data.csv`) in the source directory.\\n\\n2. **PySpark Implementation:**\\n   - Implement the transfer process using PySpark.\\n   - Read the employee data file from the source directory using PySpark.\\n\\n3. **File Transfer:**\\n   - Transfer the identified file to the specified destination directory.\\n   - Maintain the original file format (e.g., CSV).\\n\\n4. **Error Handling:**\\n   - Log an error message if the file does not exist in the source directory.\\n   - Log and handle errors gracefully during the transfer process.\\n\\n5. **Logging:**\\n   - Log the start and completion of the file transfer process.\\n   - Include date, time, and status of the transfer in the logs.\\n\\n6. **Confirmation:**\\n   - Log a success message after the file is successfully transferred.\\n\\n### 2.2 Non-Functional Requirements\\n\\n1. **Performance:**\\n   - The transfer process should be efficient and should complete within a reasonable time frame.\\n\\n2. **Reliability:**\\n   - The system should handle errors gracefully and provide meaningful error messages.\\n\\n3. **Scalability:**\\n   - The solution should be scalable to handle larger files and increased data volume.\\n\\n## 3. System Architecture\\n\\n### 3.1 Components\\n\\n1. **Source Directory:**\\n   - Directory containing the employee data file (e.g., `/source/employee_data.csv`).\\n\\n2. **Destination Directory:**\\n   - Directory where the employee data file will be transferred (e.g., `/destination/employee_data.csv`).\\n\\n3. **PySpark Job:**\\n   - PySpark script to read the file from the source directory and write it to the destination directory.\\n\\n4. **Logging Mechanism:**\\n   - Mechanism to log the start, progress, and completion of the transfer process.\\n\\n### 3.2 Workflow\\n\\n1. **Initialization:**\\n   - Initialize the PySpark job.\\n   - Log the start of the job with the current timestamp.\\n\\n2. **File Identification:**\\n   - Identify the file in the source directory.\\n   - Log the file identification process.\\n\\n3. **File Reading:**\\n   - Use PySpark to read the file from the source directory.\\n   - Log the reading process.\\n\\n4. **File Writing:**\\n   - Use PySpark to write the file to the destination directory.\\n   - Log the writing process.\\n\\n5. **Completion:**\\n   - Log the completion of the job with the current timestamp.\\n   - Log a success message if the file is transferred successfully.\\n\\n6. **Error Handling:**\\n   - If the file does not exist in the source directory, log an error message.\\n   - If there is an error during the transfer process, log the error and handle it gracefully.\\n\\n## 4. Detailed Design\\n\\n### 4.1 PySpark Job Implementation\\n\\n```python\\nfrom pyspark.sql import SparkSession\\nimport logging\\nimport os\\n\\n# Initialize logging\\nlogging.basicConfig(level=logging.INFO)\\nlogger = logging.getLogger(__name__)\\n\\ndef transfer_employee_data(source_path, destination_path):\\n    try:\\n        # Initialize Spark session\\n        spark = SparkSession.builder.appName(\"EmployeeDataTransfer\").getOrCreate()\\n        \\n        # Log the start of the job\\n        logger.info(f\"Starting the transfer process at {os.path.basename(__file__)}\")\\n        \\n        # Read the file from the source directory\\n        df = spark.read.csv(source_path, header=True, inferSchema=True)\\n        logger.info(f\"File read from source directory: {source_path}\")\\n        \\n        # Write the file to the destination directory\\n        df.write.csv(destination_path, header=True, mode=\\'overwrite\\')\\n        logger.info(f\"File written to destination directory: {destination_path}\")\\n        \\n        # Log the completion of the job\\n        logger.info(f\"Transfer process completed successfully at {os.path.basename(__file__)}\")\\n    \\n    except FileNotFoundError as e:\\n        logger.error(f\"Error: File not found in source directory. Details: {e}\")\\n    except Exception as e:\\n        logger.error(f\"Error occurred during the transfer process. Details: {e}\")\\n\\nif __name__ == \"__main__\":\\n    source_path = \"/source/employee_data.csv\"\\n    destination_path = \"/destination/employee_data.csv\"\\n    transfer_employee_data(source_path, destination_path)\\n```\\n\\n### 4.2 Logging Mechanism\\n\\n- **Log File Configuration:**\\n  - Configure logging to capture the start, progress, and completion of the transfer process.\\n  - Logs should include timestamps, file paths, and status messages.\\n\\n### 4.3 Error Handling Mechanism\\n\\n- **File Not Found:**\\n  - If the file does not exist in the source directory, log an error message.\\n- **General Error Handling:**\\n  - Log any other exceptions that occur during the transfer process.\\n\\n## 5. Testing\\n\\n### 5.1 Unit Tests\\n\\n- **Test Case 1: Successful Transfer**\\n  - Verify that the file is successfully transferred from the source to the destination directory.\\n- **Test Case 2: File Not Found**\\n  - Verify that the system logs an error if the file does not exist in the source directory.\\n- **Test Case 3: General Error Handling**\\n  - Verify that the system logs and handles other exceptions gracefully.\\n\\n### 5.2 Integration Tests\\n\\n- **Test Case 1: End-to-End Transfer Process**\\n  - Verify the entire transfer process from initialization to completion, including logging and error handling.\\n\\n## 6. Deployment\\n\\n### 6.1 Deployment Steps\\n\\n1. **Environment Setup:**\\n   - Ensure that the PySpark environment is set up with necessary dependencies.\\n   \\n2. **Script Execution:**\\n   - Execute the PySpark script to transfer the file.\\n   - Use a scheduler (e.g., cron job) to run the script at regular intervals if needed.\\n\\n3. **Monitoring:**\\n   - Monitor the logs to ensure the process runs as expected.\\n   - Ensure the destination directory contains the transferred file.\\n\\n## 7. Maintenance\\n\\n### 7.1 Logging and Monitoring\\n\\n- Regularly review the logs to identify any issues or patterns.\\n- Set up alerts for critical errors.\\n\\n### 7.2 Documentation\\n\\n- Maintain up-to-date documentation of the script and the process flow.\\n- Document any changes or updates to the system.\\n\\n---\\n\\nThis functional design document provides a comprehensive guide to implementing and maintaining the file transfer process using PySpark. It covers the architecture, implementation details, testing, and maintenance steps to ensure the process is reliable and traceable.', 'tech_document': 'Glad to hear that the user story looks good! Let\\'s proceed with creating a technical design document based on the user story. This document will provide a detailed technical blueprint for implementing the file transfer process using PySpark.\\n\\n---\\n\\n# Technical Design Document\\n\\n## 1. Overview\\n\\nThis technical design document outlines the process of transferring an employee data file from a source directory to a destination directory using PySpark. The goal is to automate this process to ensure reliable and traceable data movement.\\n\\n## 2. Requirements\\n\\n### 2.1 Functional Requirements\\n\\n1. **File Identification:**\\n   - The system must identify the specific employee data file (e.g., `employee_data.csv`) in the source directory.\\n   \\n2. **PySpark Implementation:**\\n   - The file transfer process must be implemented using PySpark.\\n   - The PySpark job should read the employee data file from the source directory.\\n\\n3. **File Transfer:**\\n   - The system must transfer the identified file from the source directory to the specified destination directory.\\n   - The file should be transferred in its original format (e.g., CSV).\\n\\n### 2.2 Non-Functional Requirements\\n\\n1. **Error Handling:**\\n   - If the file does not exist in the source directory, the system should log an error message.\\n   - If there is an error during the transfer process, it should be logged, and the job should fail gracefully.\\n\\n2. **Logging:**\\n   - The system should log the start and completion of the file transfer process.\\n   - The log should include the date, time, and status of the transfer.\\n\\n3. **Confirmation:**\\n   - After a successful transfer, the system should confirm the file transfer by logging a success message.\\n\\n## 3. System Architecture\\n\\n### 3.1 Components\\n\\n1. **Source Directory:** The directory where the original employee data file resides.\\n2. **Destination Directory:** The directory where the employee data file needs to be transferred.\\n3. **PySpark Job:** A PySpark script that reads the file from the source directory and writes it to the destination directory.\\n4. **Logging Mechanism:** A logging system to record the process and any errors encountered.\\n\\n### 3.2 Workflow\\n\\n1. **Initialization:**\\n   - The PySpark job starts and initializes the logging mechanism.\\n\\n2. **File Identification:**\\n   - The job identifies the specific employee data file in the source directory.\\n\\n3. **File Transfer:**\\n   - The job reads the file from the source directory using PySpark.\\n   - The job writes the file to the destination directory.\\n\\n4. **Error Handling:**\\n   - If the file does not exist in the source directory, log an error message.\\n   - If there is an error during the transfer process, log the error and fail gracefully.\\n\\n5. **Logging:**\\n   - Log the start and completion of the file transfer process.\\n   - Include the date, time, and status of the transfer in the log.\\n\\n6. **Confirmation:**\\n   - Log a success message if the file is successfully transferred.\\n\\n## 4. Implementation Details\\n\\n### 4.1 Environment Setup\\n\\n1. **Development Environment:**\\n   - Ensure PySpark is installed and configured.\\n   - Set up a logging library (e.g., `logging` module in Python).\\n\\n2. **PySpark Configuration:**\\n   - Configure PySpark to read from and write to HDFS (Hadoop Distributed File System) or a local file system.\\n\\n### 4.2 Code Implementation\\n\\n#### 4.2.1 PySpark Script\\n\\n```python\\nimport logging\\nfrom pyspark.sql import SparkSession\\n\\n# Initialize logger\\nlogging.basicConfig(level=logging.INFO)\\nlogger = logging.getLogger()\\n\\ndef transfer_file(source_path, destination_path):\\n    try:\\n        # Initialize Spark session\\n        spark = SparkSession.builder.appName(\"EmployeeDataTransfer\").getOrCreate()\\n\\n        # Log the start of the process\\n        logger.info(f\"Starting file transfer from {source_path} to {destination_path}\")\\n\\n        # Read the file from the source directory\\n        df = spark.read.csv(source_path, header=True, inferSchema=True)\\n\\n        # Write the file to the destination directory\\n        df.write.csv(destination_path, header=True)\\n\\n        # Log the success of the process\\n        logger.info(f\"File successfully transferred from {source_path} to {destination_path}\")\\n\\n    except FileNotFoundError:\\n        # Log an error if the file does not exist in the source directory\\n        logger.error(f\"File not found at {source_path}\")\\n\\n    except Exception as e:\\n        # Log any other errors encountered during the transfer\\n        logger.error(f\"An error occurred during file transfer: {str(e)}\")\\n\\n    finally:\\n        # Stop the Spark session\\n        spark.stop()\\n\\nif __name__ == \"__main__\":\\n    # Define source and destination paths\\n    source_directory = \"/path/to/source/directory\"\\n    destination_directory = \"/path/to/destination/directory\"\\n\\n    # Call the transfer function\\n    transfer_file(f\"{source_directory}/employee_data.csv\", f\"{destination_directory}/employee_data.csv\")\\n```\\n\\n### 4.3 Logging Configuration\\n\\n- **Logging Level:** `INFO` and `ERROR`\\n- **Log File:** Optionally, configure the logger to write logs to a file (e.g., `transfer_logs.log`).\\n\\n### 4.4 Error Handling\\n\\n- **File Not Found:** Log an error if the file does not exist in the source directory.\\n- **General Errors:** Log any other errors encountered during the transfer process and handle them gracefully.\\n\\n### 4.5 Confirmation\\n\\n- After a successful transfer, log a success message indicating the completion of the transfer.\\n\\n## 5. Deployment and Testing\\n\\n### 5.1 Deployment\\n\\n1. **Environment Setup:**\\n   - Ensure the environment is set up with the necessary dependencies (PySpark, logging libraries).\\n   \\n2. **Execution:**\\n   - Run the PySpark script using the appropriate Spark-submit command or within a PySpark environment.\\n\\n### 5.2 Testing\\n\\n1. **Unit Testing:**\\n   - Write unit tests to verify the functionality of the PySpark script.\\n   - Test scenarios where the file exists and does not exist in the source directory.\\n\\n2. **Integration Testing:**\\n   - Perform integration testing to ensure the script works correctly with the source and destination directories.\\n\\n3. **Logging Verification:**\\n   - Verify that logs are generated correctly and logged messages are accurate.\\n\\n## 6. Maintenance and Monitoring\\n\\n### 6.1 Maintenance\\n\\n- Regularly check the logs to ensure the script runs without errors.\\n- Update the script in case of changes in file format or directory paths.\\n\\n### 6.2 Monitoring\\n\\n- Use monitoring tools to track the execution of the PySpark job and identify any potential issues.\\n\\n---\\n\\nThis technical design document provides a comprehensive guide for implementing the file transfer process using PySpark, including detailed steps, error handling, and logging mechanisms.', 'code_content': 'Based on the provided summaries, here\\'s a PySpark script that implements the file transfer process as described:\\n\\n```python\\nfrom pyspark.sql import SparkSession\\nimport logging\\nfrom datetime import datetime\\n\\n# Initialize Logger\\nlogging.basicConfig(level=logging.INFO)\\nlogger = logging.getLogger(__name__)\\n\\n# Define source and destination paths\\nsource_path = \"/source/employee_data.csv\"\\ndestination_path = \"/destination/employee_data.csv\"\\n\\ndef main():\\n    try:\\n        # Initialize Spark session\\n        spark = SparkSession.builder.appName(\"EmployeeDataTransfer\").getOrCreate()\\n        \\n        # Log start time\\n        start_time = datetime.now()\\n        logger.info(f\"Transfer process started at {start_time}\")\\n        \\n        # Read data from source\\n        logger.info(f\"Reading data from {source_path}\")\\n        df = spark.read.csv(source_path, header=True, inferSchema=True)\\n        \\n        # Write data to destination\\n        logger.info(f\"Writing data to {destination_path}\")\\n        df.write.csv(destination_path, header=True)\\n        \\n        # Log completion time and success message\\n        end_time = datetime.now()\\n        logger.info(f\"Transfer process completed at {end_time}\")\\n        logger.info(f\"Data transfer was successful. Time taken: {end_time - start_time}\")\\n    \\n    except FileNotFoundError as fnfe:\\n        logger.error(f\"File not found: {fnfe}\")\\n    \\n    except Exception as e:\\n        logger.error(f\"An error occurred: {e}\")\\n    \\n    finally:\\n        # Stop Spark session\\n        spark.stop()\\n\\nif __name__ == \"__main__\":\\n    main()\\n```\\n\\n### Explanation of the Script:\\n\\n1. **Initialization and Setup:**\\n   - The script initializes a PySpark session and sets up a logger for logging the process.\\n\\n2. **Main Function:**\\n   - The `main()` function orchestrates the file transfer process.\\n   - It starts by logging the start time.\\n   - It reads the CSV file from the source path using the `spark.read.csv` function.\\n   - It writes the DataFrame to the destination path using the `df.write.csv` function.\\n   - It logs the completion time and the status of the transfer process.\\n\\n3. **Error Handling:**\\n   - The script catches `FileNotFoundError` and general exceptions, logging the error messages appropriately.\\n   \\n4. **Finalization:**\\n   - The Spark session is stopped in the `finally` block to ensure resources are released.\\n\\n### Logging:\\n- The script uses the Python `logging` module to log the start and end times of the process, as well as any errors that occur during the transfer.\\n\\n### Testing and Deployment:\\n- The script can be executed using `spark-submit` or within a PySpark environment.\\n- Unit and integration tests would involve checking the existence of the output file, verifying the contents, and ensuring the logs are correctly populated.\\n\\n### Maintenance:\\n- Regular checks of the log files are recommended to identify any errors or issues.\\n- Any changes in the file format or paths should be updated in the script and the documentation.\\n\\nThis script provides a robust and maintainable approach to transferring the `employee_data.csv` file using PySpark, with detailed logging and error handling as specified.'}\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "21 validation errors for HumanMessage\ncontent.str\n  Input should be a valid string [type=string_type, input_value=AIMessage(content='Great!..., 'total_tokens': 2505}), input_type=AIMessage]\n    For further information visit https://errors.pydantic.dev/2.10/v/string_type\ncontent.list[union[str,dict[any,any]]].0.str\n  Input should be a valid string [type=string_type, input_value=('content', 'Great! Let\\'...cure and maintainable.'), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.10/v/string_type\ncontent.list[union[str,dict[any,any]]].0.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('content', 'Great! Let\\'...cure and maintainable.'), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.10/v/dict_type\ncontent.list[union[str,dict[any,any]]].1.str\n  Input should be a valid string [type=string_type, input_value=('additional_kwargs', {}), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.10/v/string_type\ncontent.list[union[str,dict[any,any]]].1.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('additional_kwargs', {}), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.10/v/dict_type\ncontent.list[union[str,dict[any,any]]].2.str\n  Input should be a valid string [type=string_type, input_value=('response_metadata', {'t...top', 'logprobs': None}), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.10/v/string_type\ncontent.list[union[str,dict[any,any]]].2.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('response_metadata', {'t...top', 'logprobs': None}), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.10/v/dict_type\ncontent.list[union[str,dict[any,any]]].3.str\n  Input should be a valid string [type=string_type, input_value=('type', 'ai'), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.10/v/string_type\ncontent.list[union[str,dict[any,any]]].3.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('type', 'ai'), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.10/v/dict_type\ncontent.list[union[str,dict[any,any]]].4.str\n  Input should be a valid string [type=string_type, input_value=('name', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.10/v/string_type\ncontent.list[union[str,dict[any,any]]].4.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('name', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.10/v/dict_type\ncontent.list[union[str,dict[any,any]]].5.str\n  Input should be a valid string [type=string_type, input_value=('id', 'run-79c4839d-08e1...25-a656-6ee0153cf5b2-0'), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.10/v/string_type\ncontent.list[union[str,dict[any,any]]].5.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('id', 'run-79c4839d-08e1...25-a656-6ee0153cf5b2-0'), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.10/v/dict_type\ncontent.list[union[str,dict[any,any]]].6.str\n  Input should be a valid string [type=string_type, input_value=('example', False), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.10/v/string_type\ncontent.list[union[str,dict[any,any]]].6.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('example', False), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.10/v/dict_type\ncontent.list[union[str,dict[any,any]]].7.str\n  Input should be a valid string [type=string_type, input_value=('tool_calls', []), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.10/v/string_type\ncontent.list[union[str,dict[any,any]]].7.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('tool_calls', []), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.10/v/dict_type\ncontent.list[union[str,dict[any,any]]].8.str\n  Input should be a valid string [type=string_type, input_value=('invalid_tool_calls', []), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.10/v/string_type\ncontent.list[union[str,dict[any,any]]].8.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('invalid_tool_calls', []), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.10/v/dict_type\ncontent.list[union[str,dict[any,any]]].9.str\n  Input should be a valid string [type=string_type, input_value=('usage_metadata', {'inpu..., 'total_tokens': 2505}), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.10/v/string_type\ncontent.list[union[str,dict[any,any]]].9.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('usage_metadata', {'inpu..., 'total_tokens': 2505}), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.10/v/dict_type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[905], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\AgenticAI\\venv1\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:1797\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[0;32m   1791\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[0;32m   1792\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[0;32m   1793\u001b[0m     \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[0;32m   1794\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[0;32m   1795\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[0;32m   1796\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[1;32m-> 1797\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1798\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1799\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1800\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1801\u001b[0m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1802\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1803\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[0;32m   1804\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1805\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\AgenticAI\\venv1\\Lib\\site-packages\\langgraph\\pregel\\runner.py:230\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[1;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[0;32m    228\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 230\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_SEND\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\AgenticAI\\venv1\\Lib\\site-packages\\langgraph\\pregel\\retry.py:40\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[1;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[0;32m     38\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     42\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[1;32mc:\\AgenticAI\\venv1\\Lib\\site-packages\\langgraph\\utils\\runnable.py:546\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    542\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[0;32m    543\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    544\u001b[0m )\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 546\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    548\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32mc:\\AgenticAI\\venv1\\Lib\\site-packages\\langgraph\\utils\\runnable.py:310\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    309\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m--> 310\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[1;32mIn[897], line 80\u001b[0m, in \u001b[0;36msecurity_feedback\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# Append the latest functional document to the list\u001b[39;00m\n\u001b[0;32m     78\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msecurity_comments\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mcontent  \n\u001b[1;32m---> 80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msecurity_comments\u001b[39m\u001b[38;5;124m\"\u001b[39m: result\u001b[38;5;241m.\u001b[39mcontent}\n",
      "File \u001b[1;32mc:\\AgenticAI\\venv1\\Lib\\site-packages\\langchain_core\\messages\\human.py:57\u001b[0m, in \u001b[0;36mHumanMessage.__init__\u001b[1;34m(self, content, **kwargs)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28mself\u001b[39m, content: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mlist\u001b[39m[Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m]]], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[0;32m     50\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Pass in content as positional arg.\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;124;03m        content: The string contents of the message.\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;124;03m        kwargs: Additional fields to pass to the message.\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\AgenticAI\\venv1\\Lib\\site-packages\\langchain_core\\messages\\base.py:76\u001b[0m, in \u001b[0;36mBaseMessage.__init__\u001b[1;34m(self, content, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m, content: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mlist\u001b[39m[Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m]]], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[0;32m     69\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Pass in content as positional arg.\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03m        content: The string contents of the message.\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;124;03m        kwargs: Additional fields to pass to the\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\AgenticAI\\venv1\\Lib\\site-packages\\langchain_core\\load\\serializable.py:125\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\AgenticAI\\venv1\\Lib\\site-packages\\pydantic\\main.py:214\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(self, **data)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[0;32m    213\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 214\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[0;32m    216\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    220\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    221\u001b[0m     )\n",
      "\u001b[1;31mValidationError\u001b[0m: 21 validation errors for HumanMessage\ncontent.str\n  Input should be a valid string [type=string_type, input_value=AIMessage(content='Great!..., 'total_tokens': 2505}), input_type=AIMessage]\n    For further information visit https://errors.pydantic.dev/2.10/v/string_type\ncontent.list[union[str,dict[any,any]]].0.str\n  Input should be a valid string [type=string_type, input_value=('content', 'Great! Let\\'...cure and maintainable.'), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.10/v/string_type\ncontent.list[union[str,dict[any,any]]].0.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('content', 'Great! Let\\'...cure and maintainable.'), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.10/v/dict_type\ncontent.list[union[str,dict[any,any]]].1.str\n  Input should be a valid string [type=string_type, input_value=('additional_kwargs', {}), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.10/v/string_type\ncontent.list[union[str,dict[any,any]]].1.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('additional_kwargs', {}), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.10/v/dict_type\ncontent.list[union[str,dict[any,any]]].2.str\n  Input should be a valid string [type=string_type, input_value=('response_metadata', {'t...top', 'logprobs': None}), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.10/v/string_type\ncontent.list[union[str,dict[any,any]]].2.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('response_metadata', {'t...top', 'logprobs': None}), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.10/v/dict_type\ncontent.list[union[str,dict[any,any]]].3.str\n  Input should be a valid string [type=string_type, input_value=('type', 'ai'), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.10/v/string_type\ncontent.list[union[str,dict[any,any]]].3.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('type', 'ai'), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.10/v/dict_type\ncontent.list[union[str,dict[any,any]]].4.str\n  Input should be a valid string [type=string_type, input_value=('name', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.10/v/string_type\ncontent.list[union[str,dict[any,any]]].4.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('name', None), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.10/v/dict_type\ncontent.list[union[str,dict[any,any]]].5.str\n  Input should be a valid string [type=string_type, input_value=('id', 'run-79c4839d-08e1...25-a656-6ee0153cf5b2-0'), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.10/v/string_type\ncontent.list[union[str,dict[any,any]]].5.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('id', 'run-79c4839d-08e1...25-a656-6ee0153cf5b2-0'), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.10/v/dict_type\ncontent.list[union[str,dict[any,any]]].6.str\n  Input should be a valid string [type=string_type, input_value=('example', False), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.10/v/string_type\ncontent.list[union[str,dict[any,any]]].6.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('example', False), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.10/v/dict_type\ncontent.list[union[str,dict[any,any]]].7.str\n  Input should be a valid string [type=string_type, input_value=('tool_calls', []), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.10/v/string_type\ncontent.list[union[str,dict[any,any]]].7.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('tool_calls', []), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.10/v/dict_type\ncontent.list[union[str,dict[any,any]]].8.str\n  Input should be a valid string [type=string_type, input_value=('invalid_tool_calls', []), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.10/v/string_type\ncontent.list[union[str,dict[any,any]]].8.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('invalid_tool_calls', []), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.10/v/dict_type\ncontent.list[union[str,dict[any,any]]].9.str\n  Input should be a valid string [type=string_type, input_value=('usage_metadata', {'inpu..., 'total_tokens': 2505}), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.10/v/string_type\ncontent.list[union[str,dict[any,any]]].9.dict[any,any]\n  Input should be a valid dictionary [type=dict_type, input_value=('usage_metadata', {'inpu..., 'total_tokens': 2505}), input_type=tuple]\n    For further information visit https://errors.pydantic.dev/2.10/v/dict_type",
      "\u001b[0mDuring task with name 'security_feedback' and id 'bd5177ea-25d2-1ba7-0ea0-7107b1543a1a'"
     ]
    }
   ],
   "source": [
    "\n",
    "for event in graph.stream(None,thread,stream_mode=\"values\"):\n",
    "    print(event)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [AIMessage(content=\"Great! \\n\\nI'm glad you think so. \\n\\nIf you'd like to move forward and flesh out the functional design document further, I can help with things like:\\n\\n* **Detailed Data Flow Diagram:**  Visualizing the movement of data from source to destination.\\n* **PySpark Code Snippets:**  Illustrating key components of the data transfer logic.\\n* **Error Handling Strategies:**  Defining specific actions to take in case of various data transfer issues.\\n* **Logging and Monitoring:**  Describing the format and purpose of logs generated during the process.\\n* **Security Considerations:**  Addressing any access control or data encryption requirements.\\n* **Deployment and Maintenance Plan:**  Outlining how the ETL process will be deployed and kept up-to-date.\\n\\n\\n\\nJust let me know what aspects you'd like to explore next!\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 180, 'prompt_tokens': 697, 'total_tokens': 877, 'completion_time': 0.327272727, 'prompt_time': 0.030936692, 'queue_time': 0.27171373600000004, 'total_time': 0.358209419}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-77101a2b-e19c-4fa5-a268-532f57cd1ded-0', usage_metadata={'input_tokens': 697, 'output_tokens': 180, 'total_tokens': 877})], 'user_stories': '## User Stories for Employee Data File Transfer with PySpark\\n\\n**As a Data Engineer,**\\n\\n* **I want to transfer employee data files from a source directory to a destination directory,** so I can ensure data availability for downstream processes.\\n* **I want to be able to schedule the data transfer automatically,** so I can avoid manual intervention and ensure timely data updates.\\n* **I want to track the status of the data transfer process,** so I can monitor its progress and identify any potential issues.\\n* **I want the data transfer process to be robust and handle potential errors gracefully,** so I can minimize data loss and downtime.\\n\\n**As a Data Scientist,**\\n\\n* **I want to be able to access the latest employee data from the destination directory,** so I can perform timely analysis and generate accurate insights.\\n\\n**Acceptance Criteria:**\\n\\n* The data transfer process should successfully move all employee data files from the source directory to the destination directory.\\n* The transfer process should be configurable to specify the source and destination directories.\\n* The data transfer should be scheduled using a cron job or similar mechanism.\\n* The process should log the status of the transfer, including start and end times, any errors encountered, and the number of files transferred.\\n* The system should handle file size limitations and potential concurrency issues.\\n* The process should be documented with clear instructions on how to configure and use it.\\n\\n\\nThese user stories provide a starting point for defining the requirements for the employee data file transfer process using PySpark. \\n\\nThey can be further refined and elaborated upon based on specific business needs and technical constraints.\\n', 'func_document': \"Great! \\n\\nI'm glad you think so. \\n\\nIf you'd like to move forward and flesh out the functional design document further, I can help with things like:\\n\\n* **Detailed Data Flow Diagram:**  Visualizing the movement of data from source to destination.\\n* **PySpark Code Snippets:**  Illustrating key components of the data transfer logic.\\n* **Error Handling Strategies:**  Defining specific actions to take in case of various data transfer issues.\\n* **Logging and Monitoring:**  Describing the format and purpose of logs generated during the process.\\n* **Security Considerations:**  Addressing any access control or data encryption requirements.\\n* **Deployment and Maintenance Plan:**  Outlining how the ETL process will be deployed and kept up-to-date.\\n\\n\\n\\nJust let me know what aspects you'd like to explore next!\\n\", 'tech_document': \"Great! \\n\\nI'm glad the user stories are helpful.  \\n\\nIf you'd like to move onto the technical design document itself, I can help you with that. What specific aspects of the design are you most interested in exploring? \\n\\nFor example, we could discuss:\\n\\n* **Data Ingestion:** How will the PySpark application read employee data files from the source directory? What file formats will it support?\\n* **Data Transformation:** Will any data transformations be performed during the transfer process?  If so, what will they be?\\n* **Data Loading:** How will the transformed data be loaded into the destination directory? Will it be written to new files, or updated into existing ones?\\n* **Error Handling:** What mechanisms will be in place to handle potential errors during the transfer process?\\n* **Scheduling:** How will the cron job be configured to schedule the data transfer? What frequency will be used?\\n* **Logging and Monitoring:**  What type of logging will be implemented? How will the status of the transfer be monitored?\\n\\n\\n\\nLet me know what you'd like to focus on first!  \\n\\n\"}\n"
     ]
    }
   ],
   "source": [
    "state_snapshot = graph.get_state(thread)\n",
    "state = state_snapshot.values  # Extract the underlying dictionary\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Here's a user story based on the requirement to transfer employee data files from one directory to another using PySpark.\n",
      "\n",
      "### User Story: Transfer Employee Data File\n",
      "\n",
      "**Role:** Data Engineer\n",
      "\n",
      "**As a** Data Engineer,  \n",
      "**I want** to transfer employee data files from the source directory to the destination directory using PySpark,  \n",
      "**So that** the employee data can be processed and utilized in downstream systems.\n",
      "\n",
      "**Acceptance Criteria:**\n",
      "\n",
      "1. **File Identification:**\n",
      "   - The system should identify and read the employee data file located in the source directory.\n",
      "   \n",
      "2. **Data Transfer:**\n",
      "   - The system should use PySpark to transfer the identified employee data file from the source directory to the destination directory.\n",
      "   \n",
      "3. **File Naming and Structure:**\n",
      "   - The file naming convention and structure should be preserved during the transfer.\n",
      "   \n",
      "4. **Error Handling:**\n",
      "   - The system should handle any errors during the transfer process and log the details of the errors for troubleshooting.\n",
      "   \n",
      "5. **Completion Notification:**\n",
      "   - Upon successful transfer, the system should notify the data engineer that the transfer is complete.\n",
      "   \n",
      "6. **Data Integrity:**\n",
      "   - The data integrity should be maintained during the transfer, ensuring no data loss or corruption occurs.\n",
      "\n",
      "### Notes:\n",
      "- The source and destination directories should be configurable via a configuration file or environment variables.\n",
      "- The transfer process should be automated and scheduled to run at specific intervals.\n",
      "- The PySpark job should be idempotent to avoid duplicating data in the destination directory.\n",
      "\n",
      "This user story covers the functional requirements and provides a clear understanding of what needs to be achieved and how it will benefit the data processing workflow.\n",
      "Glad to hear that! Let's proceed to create a functional design document based on the user story provided. This document will outline the detailed steps, configurations, and technical details required to implement the transfer of employee data files using PySpark.\n",
      "\n",
      "### Functional Design Document: Employee Data File Transfer Using PySpark\n",
      "\n",
      "#### 1. Introduction\n",
      "\n",
      "This document outlines the design for transferring employee data files from a source directory to a destination directory using PySpark. The goal is to ensure that the data is transferred accurately and efficiently while maintaining data integrity and providing error handling and notification mechanisms.\n",
      "\n",
      "#### 2. Scope\n",
      "\n",
      "- **Source Directory:** Directory where the employee data files are initially stored.\n",
      "- **Destination Directory:** Directory where the employee data files will be transferred.\n",
      "- **PySpark:** Used for processing and transferring the files.\n",
      "- **Configuration Management:** Configurations will be managed via configuration files or environment variables.\n",
      "- **Error Handling:** Mechanisms to handle and log errors during the transfer process.\n",
      "- **Notifications:** Notifications to be sent upon successful transfer or failure.\n",
      "\n",
      "#### 3. Requirements\n",
      "\n",
      "1. **File Identification:**\n",
      "   - Identify the employee data files in the source directory.\n",
      "   - Ensure that the files are correctly recognized and processed.\n",
      "\n",
      "2. **Data Transfer:**\n",
      "   - Use PySpark to read the data from the source directory.\n",
      "   - Transfer the data to the destination directory while preserving the file naming convention and structure.\n",
      "\n",
      "3. **File Naming and Structure:**\n",
      "   - Preserve the original file structure and naming conventions during the transfer.\n",
      "\n",
      "4. **Error Handling:**\n",
      "   - Implement error handling to manage any issues during the transfer process.\n",
      "   - Log detailed error messages for troubleshooting.\n",
      "\n",
      "5. **Completion Notification:**\n",
      "   - Notify the data engineer upon successful transfer or failure.\n",
      "   - Use email, logging, or other notification mechanisms.\n",
      "\n",
      "6. **Data Integrity:**\n",
      "   - Ensure that the data integrity is maintained during the transfer process.\n",
      "   - Verify that there is no data loss or corruption.\n",
      "\n",
      "7. **Configuration:**\n",
      "   - Configure source and destination directories via configuration files or environment variables.\n",
      "   - Allow for configurable scheduling intervals.\n",
      "\n",
      "#### 4. Design\n",
      "\n",
      "##### 4.1 Source and Destination Directories\n",
      "\n",
      "- **Source Directory:** Defined in `config.yaml` or environment variables.\n",
      "- **Destination Directory:** Defined in `config.yaml` or environment variables.\n",
      "\n",
      "Example `config.yaml`:\n",
      "```yaml\n",
      "source_directory: /path/to/source/directory\n",
      "destination_directory: /path/to/destination/directory\n",
      "```\n",
      "\n",
      "##### 4.2 PySpark Job\n",
      "\n",
      "- **Reading Files:**\n",
      "  - Use PySpark to read the files from the source directory.\n",
      "  - Use `spark.read` to handle different file formats (e.g., CSV, Parquet).\n",
      "\n",
      "- **Writing Files:**\n",
      "  - Use PySpark to write the files to the destination directory.\n",
      "  - Use `df.write` to write the data back to the destination directory.\n",
      "\n",
      "##### 4.3 Error Handling and Logging\n",
      "\n",
      "- **Error Handling:**\n",
      "  - Use try-except blocks to catch and handle errors.\n",
      "  - Log errors to a centralized logging system (e.g., ELK, Splunk).\n",
      "\n",
      "- **Logging:**\n",
      "  - Use logging libraries (e.g., Python's `logging` module) to log the progress and errors.\n",
      "  - Log messages should include timestamps and error details.\n",
      "\n",
      "##### 4.4 Completion Notification\n",
      "\n",
      "- **Notification Mechanism:**\n",
      "  - Use an email service or a message queue (e.g., SNS, Slack) to notify the data engineer.\n",
      "  - Implement notifications for both successful transfers and failures.\n",
      "\n",
      "- **Notification Content:**\n",
      "  - Include the status (success/failure), file names, and timestamps in the notification.\n",
      "\n",
      "##### 4.5 Data Integrity\n",
      "\n",
      "- **Checksum Verification:**\n",
      "  - Generate checksums for the source and destination files to ensure data integrity.\n",
      "  - Compare checksums to verify that the files are identical.\n",
      "\n",
      "- **Data Validation:**\n",
      "  - Perform basic data validation checks (e.g., row count, schema validation) to ensure data consistency.\n",
      "\n",
      "#### 5. Implementation Steps\n",
      "\n",
      "1. **Configuration Setup:**\n",
      "   - Set up the configuration file (`config.yaml`) or environment variables for source and destination directories.\n",
      "\n",
      "2. **PySpark Job Development:**\n",
      "   - Develop a PySpark job to read files from the source directory.\n",
      "   - Write the files to the destination directory while preserving the structure and naming conventions.\n",
      "\n",
      "3. **Error Handling:**\n",
      "   - Implement error handling and logging within the PySpark job.\n",
      "   - Log any errors to a centralized logging system.\n",
      "\n",
      "4. **Notification Mechanism:**\n",
      "   - Implement a notification mechanism to inform the data engineer of the job status (success or failure).\n",
      "\n",
      "5. **Data Integrity Checks:**\n",
      "   - Implement checksum verification and basic data validation checks to ensure data integrity.\n",
      "\n",
      "#### 6. Example PySpark Code\n",
      "\n",
      "```python\n",
      "import logging\n",
      "import os\n",
      "from pyspark.sql import SparkSession\n",
      "import yaml\n",
      "\n",
      "# Configure logging\n",
      "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
      "\n",
      "# Load configuration\n",
      "with open('config.yaml', 'r') as file:\n",
      "    config = yaml.safe_load(file)\n",
      "\n",
      "source_directory = config['source_directory']\n",
      "destination_directory = config['destination_directory']\n",
      "\n",
      "# Initialize Spark session\n",
      "spark = SparkSession.builder.appName(\"EmployeeDataTransfer\").getOrCreate()\n",
      "\n",
      "def transfer_files():\n",
      "    try:\n",
      "        # Read files from the source directory\n",
      "        df = spark.read.csv(source_directory, header=True, inferSchema=True)\n",
      "\n",
      "        # Write files to the destination directory\n",
      "        df.write.csv(destination_directory, mode='overwrite', header=True)\n",
      "\n",
      "        # Log success\n",
      "        logging.info(f\"Files successfully transferred from {source_directory} to {destination_directory}\")\n",
      "\n",
      "        # Notification\n",
      "        notify_success()\n",
      "    except Exception as e:\n",
      "        # Log error\n",
      "        logging.error(f\"Error during file transfer: {str(e)}\")\n",
      "\n",
      "        # Notification\n",
      "        notify_failure(str(e))\n",
      "\n",
      "def notify_success():\n",
      "    # Implement notification mechanism (e.g., email, Slack, etc.)\n",
      "    logging.info(\"Transfer successful. Notification sent.\")\n",
      "\n",
      "def notify_failure(error_message):\n",
      "    # Implement notification mechanism (e.g., email, Slack, etc.)\n",
      "    logging.error(f\"Transfer failed. Notification sent with error: {error_message}\")\n",
      "\n",
      "# Execute the transfer\n",
      "transfer_files()\n",
      "\n",
      "# Stop Spark session\n",
      "spark.stop()\n",
      "```\n",
      "\n",
      "#### 7. Scheduling\n",
      "\n",
      "- **Cron Job or Airflow:**\n",
      "  - Schedule the PySpark job to run at specific intervals using a cron job or Apache Airflow.\n",
      "\n",
      "#### 8. Monitoring and Maintenance\n",
      "\n",
      "- **Monitoring:**\n",
      "  - Monitor the logs for any errors or failures.\n",
      "  - Implement monitoring tools to track the job execution status.\n",
      "\n",
      "- **Maintenance:**\n",
      "  - Regularly review the configuration files and logs.\n",
      "  - Update the PySpark job as needed to handle new requirements or issues.\n",
      "\n",
      "#### 9. Conclusion\n",
      "\n",
      "This document provides a detailed design for transferring employee data files from a source directory to a destination directory using PySpark. The design ensures data integrity, error handling, and notification mechanisms to maintain a robust and reliable data transfer process.\n",
      "\n",
      "If you have any additional requirements or specific details to include, please let me know!\n",
      "Great! I'm glad you found the user story clear and comprehensive. Let's move forward with creating a technical design document based on the user story. Here's a detailed technical design document for the task of transferring employee data files from one directory to another using PySpark.\n",
      "\n",
      "### Technical Design Document: Employee Data File Transfer Using PySpark\n",
      "\n",
      "#### 1. Overview\n",
      "\n",
      "This document outlines the technical design for transferring employee data files from a source directory to a destination directory using PySpark. The design ensures data integrity, error handling, and automated execution.\n",
      "\n",
      "#### 2. Requirements\n",
      "\n",
      "- **File Identification:** Identify and read the employee data file from the source directory.\n",
      "- **Data Transfer:** Use PySpark to transfer the file from the source directory to the destination directory.\n",
      "- **File Naming and Structure:** Preserve the file naming convention and structure during the transfer.\n",
      "- **Error Handling:** Handle errors during the transfer process and log the details of the errors.\n",
      "- **Completion Notification:** Notify the data engineer upon successful transfer.\n",
      "- **Data Integrity:** Ensure no data loss or corruption occurs during the transfer.\n",
      "\n",
      "#### 3. Architecture\n",
      "\n",
      "The architecture involves the following components:\n",
      "\n",
      "1. **Configuration Management:**\n",
      "   - Source and destination directories are configurable via a configuration file or environment variables.\n",
      "   \n",
      "2. **PySpark Job:**\n",
      "   - PySpark job reads the file from the source directory, processes it, and writes it to the destination directory.\n",
      "\n",
      "3. **Error Logging:**\n",
      "   - Errors encountered during the transfer are logged for troubleshooting.\n",
      "\n",
      "4. **Notification Mechanism:**\n",
      "   - A notification mechanism is implemented to alert the data engineer upon successful transfer.\n",
      "\n",
      "#### 4. Detailed Design\n",
      "\n",
      "##### 4.1 Configuration Management\n",
      "\n",
      "- **Configuration File (config.yaml):**\n",
      "  ```yaml\n",
      "  source_directory: /path/to/source/directory\n",
      "  destination_directory: /path/to/destination/directory\n",
      "  log_file: /path/to/log/file.log\n",
      "  notification_email: dataengineer@example.com\n",
      "  ```\n",
      "\n",
      "- **Environment Variables:**\n",
      "  - `SOURCE_DIR`\n",
      "  - `DEST_DIR`\n",
      "  - `LOG_FILE`\n",
      "  - `NOTIFICATION_EMAIL`\n",
      "\n",
      "##### 4.2 PySpark Job\n",
      "\n",
      "- **PySpark Script (transfer_data.py):**\n",
      "  ```python\n",
      "  import os\n",
      "  import logging\n",
      "  import yaml\n",
      "  from pyspark.sql import SparkSession\n",
      "\n",
      "  def load_config(config_path):\n",
      "      with open(config_path, 'r') as file:\n",
      "          return yaml.safe_load(file)\n",
      "\n",
      "  def configure_logging(log_file):\n",
      "      logging.basicConfig(filename=log_file, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
      "\n",
      "  def notify_success(email):\n",
      "      # Placeholder for notification mechanism\n",
      "      print(f\"Notification sent to {email} - Transfer completed successfully.\")\n",
      "\n",
      "  def transfer_file(spark, source_dir, dest_dir, filename):\n",
      "      try:\n",
      "          df = spark.read.csv(os.path.join(source_dir, filename), header=True, inferSchema=True)\n",
      "          df.write.csv(os.path.join(dest_dir, filename), header=True, mode='overwrite')\n",
      "          logging.info(f\"File {filename} transferred successfully.\")\n",
      "      except Exception as e:\n",
      "          logging.error(f\"Error transferring file {filename}: {str(e)}\")\n",
      "\n",
      "  def main(config_path):\n",
      "      config = load_config(config_path)\n",
      "      source_dir = config['source_directory']\n",
      "      dest_dir = config['destination_directory']\n",
      "      log_file = config['log_file']\n",
      "      email = config['notification_email']\n",
      "\n",
      "      configure_logging(log_file)\n",
      "\n",
      "      spark = SparkSession.builder.appName(\"TransferEmployeeData\").getOrCreate()\n",
      "      logging.info(\"Spark session initialized.\")\n",
      "\n",
      "      # List files in the source directory\n",
      "      files = os.listdir(source_dir)\n",
      "\n",
      "      for file in files:\n",
      "          if file.endswith('.csv'):\n",
      "              transfer_file(spark, source_dir, dest_dir, file)\n",
      "\n",
      "      notify_success(email)\n",
      "      spark.stop()\n",
      "\n",
      "  if __name__ == \"__main__\":\n",
      "      config_path = \"config.yaml\"\n",
      "      main(config_path)\n",
      "  ```\n",
      "\n",
      "##### 4.3 Error Logging\n",
      "\n",
      "- **Logging Mechanism:**\n",
      "  - Use Python's `logging` module to log errors and information.\n",
      "\n",
      "##### 4.4 Notification Mechanism\n",
      "\n",
      "- **Email Notification:**\n",
      "  - Implement a simple email notification mechanism using a library like `smtplib` or an external service like AWS SNS.\n",
      "\n",
      "#### 5. Implementation Steps\n",
      "\n",
      "1. **Set Up Configuration File:**\n",
      "   - Create a `config.yaml` file with the required paths and settings.\n",
      "\n",
      "2. **Set Up PySpark Environment:**\n",
      "   - Ensure PySpark is installed and configured in the environment.\n",
      "   - Configure the Spark session in the script.\n",
      "\n",
      "3. **Run the PySpark Job:**\n",
      "   - Execute the PySpark job using a script or a scheduler like Apache Airflow or cron.\n",
      "\n",
      "4. **Automate the Job:**\n",
      "   - Schedule the job to run at specific intervals using a task scheduler.\n",
      "\n",
      "#### 6. Testing and Validation\n",
      "\n",
      "- **Unit Testing:**\n",
      "  - Write unit tests for the PySpark job to ensure it handles various edge cases and errors gracefully.\n",
      "\n",
      "- **Integration Testing:**\n",
      "  - Test the end-to-end process from file identification to successful transfer and notification.\n",
      "\n",
      "- **Validation:**\n",
      "  - Verify that the file structure and naming conventions are preserved.\n",
      "  - Ensure that the log file captures all necessary information.\n",
      "  - Confirm that the notification is sent upon successful completion.\n",
      "\n",
      "#### 7. Maintenance and Support\n",
      "\n",
      "- **Monitoring:**\n",
      "  - Implement monitoring to track the job's performance and status.\n",
      "\n",
      "- **Documentation:**\n",
      "  - Document the configuration, setup, and operational details for future reference and support.\n",
      "\n",
      "#### 8. Conclusion\n",
      "\n",
      "This technical design document provides a comprehensive approach to transferring employee data files from a source directory to a destination directory using PySpark. The design ensures data integrity, error handling, and automated notifications, making the process reliable and maintainable.\n",
      "\n",
      "Feel free to reach out if you need any further details or modifications!\n",
      "To implement the described ETL process, we will need to create a PySpark job that reads CSV files from a source directory, processes them, and writes them to a destination directory. We will also add error handling, logging, and notification mechanisms.\n",
      "\n",
      "Here's a step-by-step implementation:\n",
      "\n",
      "### 1. Configuration Management\n",
      "First, let's define a `config.yaml` file to hold our configuration settings:\n",
      "\n",
      "```yaml\n",
      "# config.yaml\n",
      "source_directory: /path/to/source\n",
      "destination_directory: /path/to/destination\n",
      "log_file: /path/to/logfile.log\n",
      "email: dataengineer@example.com\n",
      "```\n",
      "\n",
      "### 2. PySpark Script\n",
      "Let's create a PySpark script that will use the configuration settings and implement the file transfer with error handling and logging:\n",
      "\n",
      "```python\n",
      "import os\n",
      "import logging\n",
      "import yaml\n",
      "from pyspark.sql import SparkSession\n",
      "from pyspark.sql.utils import AnalysisException\n",
      "import smtplib\n",
      "from email.message import EmailMessage\n",
      "\n",
      "# Load configuration\n",
      "def load_config(config_path):\n",
      "    with open(config_path, 'r') as file:\n",
      "        return yaml.safe_load(file)\n",
      "\n",
      "# Set up logging\n",
      "def setup_logging(log_file):\n",
      "    logging.basicConfig(filename=log_file, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
      "\n",
      "# Send email notification\n",
      "def send_email(email, subject, body):\n",
      "    msg = EmailMessage()\n",
      "    msg.set_content(body)\n",
      "    msg['Subject'] = subject\n",
      "    msg['From'] = 'system@example.com'\n",
      "    msg['To'] = email\n",
      "\n",
      "    with smtplib.SMTP('localhost') as server:\n",
      "        server.send_message(msg)\n",
      "\n",
      "# Main ETL process\n",
      "def main():\n",
      "    config_path = os.getenv('CONFIG_PATH', 'config.yaml')\n",
      "    config = load_config(config_path)\n",
      "    \n",
      "    source_dir = config['source_directory']\n",
      "    dest_dir = config['destination_directory']\n",
      "    log_file = config['log_file']\n",
      "    email = config['email']\n",
      "\n",
      "    setup_logging(log_file)\n",
      "    \n",
      "    try:\n",
      "        spark = SparkSession.builder.appName('EmployeeDataTransfer').getOrCreate()\n",
      "        logging.info(f\"Reading CSV files from {source_dir}\")\n",
      "        \n",
      "        for filename in os.listdir(source_dir):\n",
      "            if filename.endswith('.csv'):\n",
      "                file_path = os.path.join(source_dir, filename)\n",
      "                dest_path = os.path.join(dest_dir, filename)\n",
      "                \n",
      "                df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
      "                df.write.csv(dest_path, header=True)\n",
      "                logging.info(f\"File {filename} transferred successfully to {dest_path}\")\n",
      "        \n",
      "        send_email(email, \"Data Transfer Successful\", \"All CSV files were transferred successfully.\")\n",
      "    except AnalysisException as e:\n",
      "        logging.error(f\"An error occurred: {e}\")\n",
      "        send_email(email, \"Data Transfer Failed\", f\"An error occurred during data transfer: {e}\")\n",
      "    except Exception as e:\n",
      "        logging.error(f\"An unexpected error occurred: {e}\")\n",
      "        send_email(email, \"Data Transfer Failed\", f\"An unexpected error occurred during data transfer: {e}\")\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "```\n",
      "\n",
      "### 3. Testing and Validation\n",
      "You should test the script by creating a sample set of CSV files in the source directory and ensuring the script reads, processes, and writes these files correctly. Also, verify that the error handling and logging work as expected.\n",
      "\n",
      "### 4. Automation\n",
      "To schedule this job, you can use a cron job or set up an Apache Airflow DAG to run this script at a scheduled interval.\n",
      "\n",
      "### 5. Monitoring and Maintenance\n",
      "Ensure that you regularly check the logs and maintain the system to address any issues that may arise.\n",
      "\n",
      "### Example Cron Job\n",
      "If using cron:\n",
      "```sh\n",
      "0 0 * * * /usr/bin/python3 /path/to/your_script.py >> /path/to/cron.log 2>&1\n",
      "```\n",
      "\n",
      "### Example Apache Airflow DAG\n",
      "If using Apache Airflow, you can set up a simple DAG:\n",
      "\n",
      "```python\n",
      "from airflow import DAG\n",
      "from airflow.operators.python_operator import PythonOperator\n",
      "from datetime import datetime\n",
      "\n",
      "def run_etl():\n",
      "    main()\n",
      "\n",
      "default_args = {\n",
      "    'owner': 'airflow',\n",
      "    'depends_on_past': False,\n",
      "    'start_date': datetime(2023, 10, 1),\n",
      "    'retries': 1,\n",
      "}\n",
      "\n",
      "dag = DAG(\n",
      "    'employee_data_transfer',\n",
      "    default_args=default_args,\n",
      "    description='ETL job to transfer employee data',\n",
      "    schedule_interval='@daily',\n",
      ")\n",
      "\n",
      "run_etl_task = PythonOperator(\n",
      "    task_id='run_etl',\n",
      "    python_callable=run_etl,\n",
      "    dag=dag,\n",
      ")\n",
      "```\n",
      "\n",
      "This script provides a comprehensive solution for transferring employee data files. Adjust paths and configurations as needed for your specific environment.\n",
      "Based on the summaries provided, here are detailed test cases for the PySpark job that transfers employee data files from a source directory to a destination directory:\n",
      "\n",
      "### Test Cases\n",
      "\n",
      "#### 1. Configuration Management\n",
      "\n",
      "**Test Case 1.1: Verify Configuration Loading**\n",
      "- **Objective:** Ensure that the PySpark job correctly loads the source and destination directories from `config.yaml`.\n",
      "- **Steps:**\n",
      "  1. Create a `config.yaml` file with valid source and destination directories.\n",
      "  2. Run the PySpark job.\n",
      "  3. Verify that the job reads the correct directories from `config.yaml`.\n",
      "- **Expected Result:** The job should read the correct directories specified in the `config.yaml` file.\n",
      "\n",
      "**Test Case 1.2: Verify Environment Variable Override**\n",
      "- **Objective:** Ensure that environment variables can override settings in `config.yaml`.\n",
      "- **Steps:**\n",
      "  1. Create a `config.yaml` file with default source and destination directories.\n",
      "  2. Set environment variables with different directory paths.\n",
      "  3. Run the PySpark job.\n",
      "  4. Verify that the job uses the directory paths from the environment variables.\n",
      "- **Expected Result:** The job should use the directory paths from the environment variables.\n",
      "\n",
      "#### 2. File Identification and Transfer\n",
      "\n",
      "**Test Case 2.1: Verify File Transfer**\n",
      "- **Objective:** Ensure that the PySpark job correctly identifies and transfers CSV files from the source directory to the destination directory.\n",
      "- **Steps:**\n",
      "  1. Create a source directory with sample CSV files.\n",
      "  2. Run the PySpark job.\n",
      "  3. Check the destination directory for the transferred files.\n",
      "- **Expected Result:** The files should be successfully transferred to the destination directory with the same structure and names.\n",
      "\n",
      "**Test Case 2.2: Verify Preservation of File Structure and Naming**\n",
      "- **Objective:** Ensure that the file structure and naming conventions are preserved during the transfer.\n",
      "- **Steps:**\n",
      "  1. Create a source directory with a nested directory structure and sample CSV files.\n",
      "  2. Run the PySpark job.\n",
      "  3. Verify that the nested directory structure and file names are preserved in the destination directory.\n",
      "- **Expected Result:** The destination directory should have the same directory structure and file names as the source directory.\n",
      "\n",
      "#### 3. Error Handling and Logging\n",
      "\n",
      "**Test Case 3.1: Verify Error Handling for Incorrect Source Directory**\n",
      "- **Objective:** Ensure that the PySpark job handles errors gracefully when the source directory does not exist.\n",
      "- **Steps:**\n",
      "  1. Set the source directory to a non-existent path in `config.yaml`.\n",
      "  2. Run the PySpark job.\n",
      "  3. Check the log file for error messages.\n",
      "- **Expected Result:** The log file should contain an error message indicating that the source directory does not exist.\n",
      "\n",
      "**Test Case 3.2: Verify Error Handling for Incorrect Destination Directory**\n",
      "- **Objective:** Ensure that the PySpark job handles errors gracefully when the destination directory is not writable.\n",
      "- **Steps:**\n",
      "  1. Set the destination directory to a non-writable path in `config.yaml`.\n",
      "  2. Run the PySpark job.\n",
      "  3. Check the log file for error messages.\n",
      "- **Expected Result:** The log file should contain an error message indicating that the destination directory is not writable.\n",
      "\n",
      "#### 4. Notifications\n",
      "\n",
      "**Test Case 4.1: Verify Successful Transfer Notification**\n",
      "- **Objective:** Ensure that an email notification is sent upon successful file transfer.\n",
      "- **Steps:**\n",
      "  1. Ensure the `config.yaml` has valid email settings.\n",
      "  2. Run the PySpark job with a successful transfer scenario.\n",
      "  3. Check the specified email address for a success notification.\n",
      "- **Expected Result:** A success notification email should be sent to the specified email address.\n",
      "\n",
      "**Test Case 4.2: Verify Failure Notification**\n",
      "- **Objective:** Ensure that an email notification is sent upon failure during file transfer.\n",
      "- **Steps:**\n",
      "  1. Ensure the `config.yaml` has valid email settings.\n",
      "  2. Create a scenario where the transfer fails (e.g., by setting an invalid source directory).\n",
      "  3. Run the PySpark job.\n",
      "  4. Check the specified email address for a failure notification.\n",
      "- **Expected Result:** A failure notification email should be sent to the specified email address.\n",
      "\n",
      "#### 5. Data Integrity\n",
      "\n",
      "**Test Case 5.1: Verify Checksum Verification**\n",
      "- **Objective:** Ensure that the transferred files have the same checksum as the original files.\n",
      "- **Steps:**\n",
      "  1. Create sample CSV files with known checksums.\n",
      "  2. Run the PySpark job.\n",
      "  3. Calculate the checksums of the transferred files in the destination directory.\n",
      "- **Expected Result:** The checksums of the transferred files should match the original files' checksums.\n",
      "\n",
      "**Test Case 5.2: Verify Row Count and Schema Validation**\n",
      "- **Objective:** Ensure that the row count and schema of the transferred files match the original files.\n",
      "- **Steps:**\n",
      "  1. Create sample CSV files with a known schema and row count.\n",
      "  2. Run the PySpark job.\n",
      "  3. Compare the row count and schema of the transferred files with the original files.\n",
      "- **Expected Result:** The row count and schema of the transferred files should match the original files.\n",
      "\n",
      "#### 6. Unit and Integration Testing\n",
      "\n",
      "**Test Case 6.1: Handle Empty Files**\n",
      "- **Objective:** Ensure that the PySpark job handles empty files gracefully.\n",
      "- **Steps:**\n",
      "  1. Create an empty CSV file in the source directory.\n",
      "  2. Run the PySpark job.\n",
      "  3. Check the destination directory and log file.\n",
      "- **Expected Result:** The empty file should be transferred, and a log entry should indicate that the file was successfully transferred.\n",
      "\n",
      "**Test Case 6.2: Handle Malformed CSV Files**\n",
      "- **Objective:** Ensure that the PySpark job handles malformed CSV files (e.g., missing headers, inconsistent row lengths).\n",
      "- **Steps:**\n",
      "  1. Create a malformed CSV file in the source directory.\n",
      "  2. Run the PySpark job.\n",
      "  3. Check the log file for error messages.\n",
      "- **Expected Result:** The log file should contain error messages indicating that the malformed file was not transferred.\n",
      "\n",
      "#### 7. Automation and Monitoring\n",
      "\n",
      "**Test Case 7.1: Schedule and Run Using Cron**\n",
      "- **Objective:** Verify that the PySpark job can be scheduled and run using cron.\n",
      "- **Steps:**\n",
      "  1. Set up a cron job to run the PySpark job at a specified time.\n",
      "  2. Wait for the scheduled time and check the destination directory and log files.\n",
      "- **Expected Result:** The job should run at the specified time, and the files should be transferred successfully.\n",
      "\n",
      "**Test Case 7.2: Schedule and Run Using Apache Airflow**\n",
      "- **Objective:** Verify that the PySpark job can be scheduled and run using Apache Airflow.\n",
      "- **Steps:**\n",
      "  1. Set up an Apache Airflow DAG to run the PySpark job daily.\n",
      "  2. Wait for the scheduled time and check the destination directory and log files.\n",
      "- **Expected Result:** The job should run as scheduled, and the files should be transferred successfully.\n",
      "\n",
      "**Test Case 7.3: Monitor Job Performance**\n",
      "- **Objective:** Ensure that the job's performance is monitored and issues are logged.\n",
      "- **Steps:**\n",
      "  1. Run the PySpark job and simulate a failure scenario.\n",
      "  2. Check the monitoring tool for job status and logs.\n",
      "- **Expected Result:** The monitoring tool should show the job status as failed, and the logs should capture the failure details.\n",
      "\n",
      "### Test Case Implementation\n",
      "\n",
      "**Test Case 8.1: Email Notification Unit Test**\n",
      "- **Objective:** Verify the email notification mechanism.\n",
      "- **Steps:**\n",
      "  1. Mock the email sending function.\n",
      "  2. Simulate a successful and a failed transfer scenario.\n",
      "  3. Verify that the email notification is sent in both scenarios.\n",
      "- **Expected Result:** The email notification should be sent with the correct status and details.\n",
      "\n",
      "**Test Case 8.2: Checksum Validation Unit Test**\n",
      "- **Objective:** Verify checksum validation.\n",
      "- **Steps:**\n",
      "  1. Create sample CSV files with known checksums.\n",
      "  2. Run the PySpark job.\n",
      "  3. Verify the checksum of the transferred files.\n",
      "- **Expected Result:** The transferred files should have the same checksum as the original files.\n",
      "\n",
      "These test cases cover the key aspects of the PySpark job as outlined in the functional and technical design documents, ensuring comprehensive testing of the file transfer process, error handling, notifications, data integrity, and automation.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'qa_test_comments'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[871], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(state_values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_cases_content\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#print(state_values[\"testcases_comments\"])\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mstate_values\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mqa_test_comments\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)   \n",
      "\u001b[1;31mKeyError\u001b[0m: 'qa_test_comments'"
     ]
    }
   ],
   "source": [
    "state=graph.get_state(thread)\n",
    "state_values = state.values  \n",
    "print(state_values[\"user_stories\"])  \n",
    "print(state_values[\"func_document\"]) \n",
    "print(state_values[\"tech_document\"])\n",
    "print(state_values[\"code_content\"]) \n",
    "print(state_values[\"test_cases_content\"])\n",
    "#print(state_values[\"testcases_comments\"])\n",
    "print(state_values[\"qa_test_comments\"])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state=graph.get_state(thread)\n",
    "state.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AIMessage(content='Great! Here is a more detailed functional design document based on the user story and acceptance criteria:\\n\\n### Functional Design Document: Transfer Employee Data File Using PySpark\\n\\n#### 1. Introduction\\nThis document outlines the design for the automated transfer of an employee data file from a source directory to a destination directory using PySpark. The process is designed to be fully automated, ensuring data integrity and providing error handling and logging.\\n\\n#### 2. Scope\\n- **Source Directory:** `/data/employee/source`\\n- **Destination Directory:** `/data/employee/destination`\\n- **File Naming Convention:** `employee_data_*.csv`\\n- **Execution Frequency:** Daily at 2 AM\\n\\n#### 3. Functional Requirements\\n\\n##### 3.1 File Identification\\n- The system must identify files in the source directory that match the naming convention `employee_data_*.csv`.\\n\\n##### 3.2 Data Transfer\\n- The system must read the identified file from the source directory and transfer it to the destination directory.\\n- The file transfer should preserve the original file\\'s content without any loss or corruption.\\n\\n##### 3.3 Automation\\n- The transfer process must be fully automated and executed without manual intervention.\\n- The process should be scheduled to run daily at 2 AM.\\n\\n##### 3.4 Error Handling\\n- The system must log any errors that occur during the transfer process.\\n- If an error occurs, the system must send an alert to the support team.\\n\\n##### 3.5 Testing\\n- The functionality must be tested to ensure that the file is correctly moved from the source to the destination directory.\\n\\n##### 3.6 Documentation\\n- Detailed documentation of the process and configurations must be provided for future reference.\\n\\n#### 4. Detailed Design\\n\\n##### 4.1 File Identification\\n- **Logic:** Use PySpark to list files in the `/data/employee/source` directory and filter by the naming convention `employee_data_*.csv`.\\n- **Example Code:**\\n  ```python\\n  from pyspark.sql import SparkSession\\n\\n  spark = SparkSession.builder.appName(\\'EmployeeDataTransfer\\').getOrCreate()\\n\\n  source_dir = \\'/data/employee/source\\'\\n  file_pattern = \\'employee_data_*.csv\\'\\n\\n  # List files in the source directory\\n  files = spark.sparkContext.wholeTextFiles(source_dir).map(lambda x: x[0]).collect()\\n  valid_files = [file for file in files if file_pattern in file]\\n  ```\\n\\n##### 4.2 Data Transfer\\n- **Logic:** For each valid file, read the file content and write it to the destination directory.\\n- **Example Code:**\\n  ```python\\n  from pyspark.sql import DataFrame\\n\\n  for file in valid_files:\\n      df = spark.read.csv(file, header=True, inferSchema=True)\\n      destination_path = file.replace(\\'/data/employee/source\\', \\'/data/employee/destination\\')\\n      df.write.csv(destination_path, header=True)\\n  ```\\n\\n##### 4.3 Automation\\n- **Logic:** Schedule the PySpark script using a scheduler (e.g., Apache Airflow, cron job).\\n- **Example Cron Job:**\\n  ```bash\\n  0 2 * * * /path/to/your/transfer_script.py\\n  ```\\n\\n##### 4.4 Error Handling\\n- **Logic:** Log any errors that occur during the transfer process and send an alert to the support team.\\n- **Example Code:**\\n  ```python\\n  import logging\\n\\n  logging.basicConfig(filename=\\'/path/to/logfile.log\\', level=logging.ERROR)\\n\\n  try:\\n      # Transfer logic here\\n      pass\\n  except Exception as e:\\n      logging.error(f\"Error during file transfer: {str(e)}\")\\n      # Send alert to support team\\n      # send_alert(e)\\n  ```\\n\\n##### 4.5 Testing\\n- **Test Cases:**\\n  - Verify that the file is correctly identified and transferred.\\n  - Verify that the file content is preserved.\\n  - Verify that the process runs without errors.\\n  - Verify that error handling and logging work as expected.\\n- **Test Data:**\\n  - Sample `employee_data_*.csv` file in the source directory.\\n  - Ensure the destination directory exists and has the necessary write permissions.\\n\\n#### 5. Documentation\\n- **Process and Configuration:**\\n  - Detailed steps and configurations for setting up the PySpark script and scheduler.\\n  - Instructions for maintaining and updating the process.\\n  - Sample code and configuration files.\\n- **Support Contact Information:**\\n  - Contact details for the support team to receive alerts.\\n\\n#### 6. Conclusion\\nThis design document covers the complete workflow for transferring employee data files from a source directory to a destination directory using PySpark. The process includes file identification, data transfer, automation, error handling, and testing, ensuring that the data is processed reliably and efficiently.\\n\\n---\\n\\nThis document should provide a comprehensive guide for implementing and maintaining the automated transfer process.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 1003, 'prompt_tokens': 721, 'total_tokens': 1724, 'completion_time': 5.015, 'prompt_time': 0.042662933, 'queue_time': 0.057103187, 'total_time': 5.057662933}, 'model_name': 'qwen-2.5-32b', 'system_fingerprint': 'fp_c527211fd1', 'finish_reason': 'stop', 'logprobs': None}, id='run-383de93c-c422-4b72-8d53-8c344123ac5c-0', usage_metadata={'input_tokens': 721, 'output_tokens': 1003, 'total_tokens': 1724})]\n",
      "{'messages': {'messages': [AIMessage(content='Great! Here is a more detailed functional design document based on the user story and acceptance criteria:\\n\\n### Functional Design Document: Transfer Employee Data File Using PySpark\\n\\n#### 1. Introduction\\nThis document outlines the design for the automated transfer of an employee data file from a source directory to a destination directory using PySpark. The process is designed to be fully automated, ensuring data integrity and providing error handling and logging.\\n\\n#### 2. Scope\\n- **Source Directory:** `/data/employee/source`\\n- **Destination Directory:** `/data/employee/destination`\\n- **File Naming Convention:** `employee_data_*.csv`\\n- **Execution Frequency:** Daily at 2 AM\\n\\n#### 3. Functional Requirements\\n\\n##### 3.1 File Identification\\n- The system must identify files in the source directory that match the naming convention `employee_data_*.csv`.\\n\\n##### 3.2 Data Transfer\\n- The system must read the identified file from the source directory and transfer it to the destination directory.\\n- The file transfer should preserve the original file\\'s content without any loss or corruption.\\n\\n##### 3.3 Automation\\n- The transfer process must be fully automated and executed without manual intervention.\\n- The process should be scheduled to run daily at 2 AM.\\n\\n##### 3.4 Error Handling\\n- The system must log any errors that occur during the transfer process.\\n- If an error occurs, the system must send an alert to the support team.\\n\\n##### 3.5 Testing\\n- The functionality must be tested to ensure that the file is correctly moved from the source to the destination directory.\\n\\n##### 3.6 Documentation\\n- Detailed documentation of the process and configurations must be provided for future reference.\\n\\n#### 4. Detailed Design\\n\\n##### 4.1 File Identification\\n- **Logic:** Use PySpark to list files in the `/data/employee/source` directory and filter by the naming convention `employee_data_*.csv`.\\n- **Example Code:**\\n  ```python\\n  from pyspark.sql import SparkSession\\n\\n  spark = SparkSession.builder.appName(\\'EmployeeDataTransfer\\').getOrCreate()\\n\\n  source_dir = \\'/data/employee/source\\'\\n  file_pattern = \\'employee_data_*.csv\\'\\n\\n  # List files in the source directory\\n  files = spark.sparkContext.wholeTextFiles(source_dir).map(lambda x: x[0]).collect()\\n  valid_files = [file for file in files if file_pattern in file]\\n  ```\\n\\n##### 4.2 Data Transfer\\n- **Logic:** For each valid file, read the file content and write it to the destination directory.\\n- **Example Code:**\\n  ```python\\n  from pyspark.sql import DataFrame\\n\\n  for file in valid_files:\\n      df = spark.read.csv(file, header=True, inferSchema=True)\\n      destination_path = file.replace(\\'/data/employee/source\\', \\'/data/employee/destination\\')\\n      df.write.csv(destination_path, header=True)\\n  ```\\n\\n##### 4.3 Automation\\n- **Logic:** Schedule the PySpark script using a scheduler (e.g., Apache Airflow, cron job).\\n- **Example Cron Job:**\\n  ```bash\\n  0 2 * * * /path/to/your/transfer_script.py\\n  ```\\n\\n##### 4.4 Error Handling\\n- **Logic:** Log any errors that occur during the transfer process and send an alert to the support team.\\n- **Example Code:**\\n  ```python\\n  import logging\\n\\n  logging.basicConfig(filename=\\'/path/to/logfile.log\\', level=logging.ERROR)\\n\\n  try:\\n      # Transfer logic here\\n      pass\\n  except Exception as e:\\n      logging.error(f\"Error during file transfer: {str(e)}\")\\n      # Send alert to support team\\n      # send_alert(e)\\n  ```\\n\\n##### 4.5 Testing\\n- **Test Cases:**\\n  - Verify that the file is correctly identified and transferred.\\n  - Verify that the file content is preserved.\\n  - Verify that the process runs without errors.\\n  - Verify that error handling and logging work as expected.\\n- **Test Data:**\\n  - Sample `employee_data_*.csv` file in the source directory.\\n  - Ensure the destination directory exists and has the necessary write permissions.\\n\\n#### 5. Documentation\\n- **Process and Configuration:**\\n  - Detailed steps and configurations for setting up the PySpark script and scheduler.\\n  - Instructions for maintaining and updating the process.\\n  - Sample code and configuration files.\\n- **Support Contact Information:**\\n  - Contact details for the support team to receive alerts.\\n\\n#### 6. Conclusion\\nThis design document covers the complete workflow for transferring employee data files from a source directory to a destination directory using PySpark. The process includes file identification, data transfer, automation, error handling, and testing, ensuring that the data is processed reliably and efficiently.\\n\\n---\\n\\nThis document should provide a comprehensive guide for implementing and maintaining the automated transfer process.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 1003, 'prompt_tokens': 721, 'total_tokens': 1724, 'completion_time': 5.015, 'prompt_time': 0.042662933, 'queue_time': 0.057103187, 'total_time': 5.057662933}, 'model_name': 'qwen-2.5-32b', 'system_fingerprint': 'fp_c527211fd1', 'finish_reason': 'stop', 'logprobs': None}, id='run-383de93c-c422-4b72-8d53-8c344123ac5c-0', usage_metadata={'input_tokens': 721, 'output_tokens': 1003, 'total_tokens': 1724}), HumanMessage(content='It looks good', additional_kwargs={}, response_metadata={})]}, 'user_stories': '### User Story: Transfer Employee Data File Using PySpark\\n\\n**As a** Data Engineer,  \\n**I want to** transfer an employee data file from one directory to another using PySpark,  \\n**So that** the data can be processed in a new directory without manual intervention.\\n\\n**Acceptance Criteria:**\\n\\n1. **File Identification:**\\n   - The system should be able to identify the file containing employee data.\\n   - The file should have a specific naming convention (e.g., `employee_data_*.csv`).\\n\\n2. **Source Directory:**\\n   - The source directory from which the file will be read is `/data/employee/source`.\\n\\n3. **Destination Directory:**\\n   - The destination directory where the file will be transferred is `/data/employee/destination`.\\n\\n4. **Data Integrity:**\\n   - The file should be transferred without any loss or corruption of data.\\n\\n5. **Automation:**\\n   - The transfer process should be fully automated and not require manual intervention.\\n\\n6. **Error Handling:**\\n   - The system should log any errors that occur during the transfer process.\\n   - If an error occurs, the system should send an alert to the support team.\\n\\n7. **Execution Frequency:**\\n   - The transfer process should be executed daily at 2 AM.\\n\\n8. **Testing:**\\n   - The functionality should be tested to ensure the file is correctly moved from the source to the destination directory.\\n\\n9. **Documentation:**\\n   - The process and any necessary configurations should be documented for future reference.\\n\\nThis user story covers the essential aspects of the requirement and can be further broken down into specific tasks and sub-tasks for implementation.', 'func_document': 'Great! Here is a more detailed functional design document based on the user story and acceptance criteria:\\n\\n### Functional Design Document: Transfer Employee Data File Using PySpark\\n\\n#### 1. Introduction\\nThis document outlines the design for the automated transfer of an employee data file from a source directory to a destination directory using PySpark. The process is designed to be fully automated, ensuring data integrity and providing error handling and logging.\\n\\n#### 2. Scope\\n- **Source Directory:** `/data/employee/source`\\n- **Destination Directory:** `/data/employee/destination`\\n- **File Naming Convention:** `employee_data_*.csv`\\n- **Execution Frequency:** Daily at 2 AM\\n\\n#### 3. Functional Requirements\\n\\n##### 3.1 File Identification\\n- The system must identify files in the source directory that match the naming convention `employee_data_*.csv`.\\n\\n##### 3.2 Data Transfer\\n- The system must read the identified file from the source directory and transfer it to the destination directory.\\n- The file transfer should preserve the original file\\'s content without any loss or corruption.\\n\\n##### 3.3 Automation\\n- The transfer process must be fully automated and executed without manual intervention.\\n- The process should be scheduled to run daily at 2 AM.\\n\\n##### 3.4 Error Handling\\n- The system must log any errors that occur during the transfer process.\\n- If an error occurs, the system must send an alert to the support team.\\n\\n##### 3.5 Testing\\n- The functionality must be tested to ensure that the file is correctly moved from the source to the destination directory.\\n\\n##### 3.6 Documentation\\n- Detailed documentation of the process and configurations must be provided for future reference.\\n\\n#### 4. Detailed Design\\n\\n##### 4.1 File Identification\\n- **Logic:** Use PySpark to list files in the `/data/employee/source` directory and filter by the naming convention `employee_data_*.csv`.\\n- **Example Code:**\\n  ```python\\n  from pyspark.sql import SparkSession\\n\\n  spark = SparkSession.builder.appName(\\'EmployeeDataTransfer\\').getOrCreate()\\n\\n  source_dir = \\'/data/employee/source\\'\\n  file_pattern = \\'employee_data_*.csv\\'\\n\\n  # List files in the source directory\\n  files = spark.sparkContext.wholeTextFiles(source_dir).map(lambda x: x[0]).collect()\\n  valid_files = [file for file in files if file_pattern in file]\\n  ```\\n\\n##### 4.2 Data Transfer\\n- **Logic:** For each valid file, read the file content and write it to the destination directory.\\n- **Example Code:**\\n  ```python\\n  from pyspark.sql import DataFrame\\n\\n  for file in valid_files:\\n      df = spark.read.csv(file, header=True, inferSchema=True)\\n      destination_path = file.replace(\\'/data/employee/source\\', \\'/data/employee/destination\\')\\n      df.write.csv(destination_path, header=True)\\n  ```\\n\\n##### 4.3 Automation\\n- **Logic:** Schedule the PySpark script using a scheduler (e.g., Apache Airflow, cron job).\\n- **Example Cron Job:**\\n  ```bash\\n  0 2 * * * /path/to/your/transfer_script.py\\n  ```\\n\\n##### 4.4 Error Handling\\n- **Logic:** Log any errors that occur during the transfer process and send an alert to the support team.\\n- **Example Code:**\\n  ```python\\n  import logging\\n\\n  logging.basicConfig(filename=\\'/path/to/logfile.log\\', level=logging.ERROR)\\n\\n  try:\\n      # Transfer logic here\\n      pass\\n  except Exception as e:\\n      logging.error(f\"Error during file transfer: {str(e)}\")\\n      # Send alert to support team\\n      # send_alert(e)\\n  ```\\n\\n##### 4.5 Testing\\n- **Test Cases:**\\n  - Verify that the file is correctly identified and transferred.\\n  - Verify that the file content is preserved.\\n  - Verify that the process runs without errors.\\n  - Verify that error handling and logging work as expected.\\n- **Test Data:**\\n  - Sample `employee_data_*.csv` file in the source directory.\\n  - Ensure the destination directory exists and has the necessary write permissions.\\n\\n#### 5. Documentation\\n- **Process and Configuration:**\\n  - Detailed steps and configurations for setting up the PySpark script and scheduler.\\n  - Instructions for maintaining and updating the process.\\n  - Sample code and configuration files.\\n- **Support Contact Information:**\\n  - Contact details for the support team to receive alerts.\\n\\n#### 6. Conclusion\\nThis design document covers the complete workflow for transferring employee data files from a source directory to a destination directory using PySpark. The process includes file identification, data transfer, automation, error handling, and testing, ensuring that the data is processed reliably and efficiently.\\n\\n---\\n\\nThis document should provide a comprehensive guide for implementing and maintaining the automated transfer process.', 'tech_document': 'Great! Let\\'s formalize this into a technical design document for the user story.\\n\\n### Technical Design Document: Transfer Employee Data File Using PySpark\\n\\n#### 1. Introduction\\nThis document outlines the technical design for transferring an employee data file from a source directory to a destination directory using PySpark. The goal is to automate the process to ensure data integrity and reduce manual intervention.\\n\\n#### 2. Requirements\\n- **Source Directory:** `/data/employee/source`\\n- **Destination Directory:** `/data/employee/destination`\\n- **File Naming Convention:** `employee_data_*.csv`\\n- **Data Integrity:** Ensure no loss or corruption of data during the transfer.\\n- **Automation:** Fully automated process with no manual intervention.\\n- **Error Handling:** Log errors and alert the support team if any issues occur.\\n- **Execution Frequency:** Daily at 2 AM.\\n- **Testing:** Ensure the file is correctly moved from the source to the destination directory.\\n- **Documentation:** Document the process and configurations for future reference.\\n\\n#### 3. System Architecture\\n- **Data Source:** `/data/employee/source`\\n- **Data Destination:** `/data/employee/destination`\\n- **Programming Language:** Python (PySpark)\\n- **Scheduling Tool:** Cron (or equivalent scheduling tool)\\n- **Logging and Monitoring:** Use logging to track the process and send alerts for errors.\\n\\n#### 4. Detailed Design\\n\\n##### 4.1 File Identification\\n- **Pattern Matching:** Use regular expressions to identify files matching the `employee_data_*.csv` pattern.\\n\\n##### 4.2 PySpark Implementation\\n- **Reading the File:**\\n  ```python\\n  from pyspark.sql import SparkSession\\n\\n  spark = SparkSession.builder.appName(\"EmployeeDataTransfer\").getOrCreate()\\n\\n  source_dir = \"/data/employee/source\"\\n  df = spark.read.csv(f\"{source_dir}/employee_data_*.csv\", header=True, inferSchema=True)\\n  ```\\n\\n- **Writing the File:**\\n  ```python\\n  dest_dir = \"/data/employee/destination\"\\n  df.write.csv(dest_dir, header=True, mode=\"overwrite\")\\n  ```\\n\\n##### 4.3 Error Handling\\n- **Logging:**\\n  ```python\\n  import logging\\n\\n  logging.basicConfig(filename=\\'employee_data_transfer.log\\', level=logging.INFO)\\n\\n  try:\\n      # File transfer logic\\n      pass\\n  except Exception as e:\\n      logging.error(f\"Error during file transfer: {e}\")\\n      # Send alert to support team\\n      send_alert(f\"Error during file transfer: {e}\")\\n  ```\\n\\n- **Alerting:**\\n  ```python\\n  def send_alert(message):\\n      # Implement the logic to send an alert (e.g., email or messaging service)\\n      pass\\n  ```\\n\\n##### 4.4 Scheduling\\n- **Cron Job:**\\n  - Create a cron job to run the PySpark script daily at 2 AM.\\n  - Example cron job entry:\\n    ```bash\\n    0 2 * * * /path/to/your/script.py >> /path/to/logfile.log 2>&1\\n    ```\\n\\n#### 5. Testing\\n- **Unit Tests:**\\n  - Verify that the file is correctly read and written.\\n  - Ensure that the file contents are not corrupted during the transfer.\\n\\n- **Integration Tests:**\\n  - Test the entire process to ensure the file is moved from the source to the destination directory.\\n  - Verify that the file is transferred daily at the specified time.\\n\\n#### 6. Documentation\\n- **Process Documentation:**\\n  - Document the steps involved in the process.\\n  - Include details on how to run the script and configure the cron job.\\n\\n- **Configuration Documentation:**\\n  - Document the configuration settings for PySpark and the cron job.\\n  - Include any necessary environment variables and dependencies.\\n\\n#### 7. Conclusion\\nThis technical design document provides a comprehensive approach to automating the transfer of employee data files using PySpark. The design ensures data integrity, error handling, and automated scheduling, meeting the specified requirements.\\n\\n---\\n\\n### Example Script\\nHere is an example script that implements the above design:\\n\\n```python\\nimport os\\nimport logging\\nfrom pyspark.sql import SparkSession\\n\\n# Set up logging\\nlogging.basicConfig(filename=\\'employee_data_transfer.log\\', level=logging.INFO)\\n\\n# Initialize Spark session\\nspark = SparkSession.builder.appName(\"EmployeeDataTransfer\").getOrCreate()\\n\\n# Define directories\\nsource_dir = \"/data/employee/source\"\\ndest_dir = \"/data/employee/destination\"\\n\\ndef send_alert(message):\\n    # Implement the logic to send an alert (e.g., email or messaging service)\\n    pass\\n\\ntry:\\n    # Read the file\\n    df = spark.read.csv(f\"{source_dir}/employee_data_*.csv\", header=True, inferSchema=True)\\n    \\n    # Write the file\\n    df.write.csv(dest_dir, header=True, mode=\"overwrite\")\\n    \\n    logging.info(\"File transfer completed successfully.\")\\nexcept Exception as e:\\n    logging.error(f\"Error during file transfer: {e}\")\\n    send_alert(f\"Error during file transfer: {e}\")\\n\\n# Stop Spark session\\nspark.stop()\\n```\\n\\n### Cron Job Configuration\\nAdd the following cron job entry to run the script daily at 2 AM:\\n\\n```bash\\n0 2 * * * /usr/bin/python3 /path/to/your/script.py >> /path/to/logfile.log 2>&1\\n```\\n\\nThis ensures the script runs as scheduled, and logs are recorded for monitoring and troubleshooting.'}\n"
     ]
    }
   ],
   "source": [
    "# Get the current state\n",
    "state_snapshot = graph.get_state(thread)\n",
    "state_values = state_snapshot.values\n",
    "\n",
    "# Since 'messages' is a list, we directly access and update it\n",
    "current_messages = state_values[\"messages\"]  # This is a list of messages\n",
    "print(current_messages)\n",
    "new_messages = current_messages + [HumanMessage(content=\"It looks good\")]\n",
    "\n",
    "updated_messages = MessagesState(messages=new_messages)\n",
    "\n",
    "# Update the state with the modified messages list\n",
    "#graph.update_state(thread, {\"messages\": new_messages})\n",
    "graph.update_state(thread, {\"messages\": updated_messages})\n",
    "\n",
    "\n",
    "state_snapshot = graph.get_state(thread)\n",
    "state = state_snapshot.values  # Extract the underlying dictionary\n",
    "print(state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': {'messages': [AIMessage(content='Great! Here is a more detailed functional design document based on the user story and acceptance criteria:\\n\\n### Functional Design Document: Transfer Employee Data File Using PySpark\\n\\n#### 1. Introduction\\nThis document outlines the design for the automated transfer of an employee data file from a source directory to a destination directory using PySpark. The process is designed to be fully automated, ensuring data integrity and providing error handling and logging.\\n\\n#### 2. Scope\\n- **Source Directory:** `/data/employee/source`\\n- **Destination Directory:** `/data/employee/destination`\\n- **File Naming Convention:** `employee_data_*.csv`\\n- **Execution Frequency:** Daily at 2 AM\\n\\n#### 3. Functional Requirements\\n\\n##### 3.1 File Identification\\n- The system must identify files in the source directory that match the naming convention `employee_data_*.csv`.\\n\\n##### 3.2 Data Transfer\\n- The system must read the identified file from the source directory and transfer it to the destination directory.\\n- The file transfer should preserve the original file\\'s content without any loss or corruption.\\n\\n##### 3.3 Automation\\n- The transfer process must be fully automated and executed without manual intervention.\\n- The process should be scheduled to run daily at 2 AM.\\n\\n##### 3.4 Error Handling\\n- The system must log any errors that occur during the transfer process.\\n- If an error occurs, the system must send an alert to the support team.\\n\\n##### 3.5 Testing\\n- The functionality must be tested to ensure that the file is correctly moved from the source to the destination directory.\\n\\n##### 3.6 Documentation\\n- Detailed documentation of the process and configurations must be provided for future reference.\\n\\n#### 4. Detailed Design\\n\\n##### 4.1 File Identification\\n- **Logic:** Use PySpark to list files in the `/data/employee/source` directory and filter by the naming convention `employee_data_*.csv`.\\n- **Example Code:**\\n  ```python\\n  from pyspark.sql import SparkSession\\n\\n  spark = SparkSession.builder.appName(\\'EmployeeDataTransfer\\').getOrCreate()\\n\\n  source_dir = \\'/data/employee/source\\'\\n  file_pattern = \\'employee_data_*.csv\\'\\n\\n  # List files in the source directory\\n  files = spark.sparkContext.wholeTextFiles(source_dir).map(lambda x: x[0]).collect()\\n  valid_files = [file for file in files if file_pattern in file]\\n  ```\\n\\n##### 4.2 Data Transfer\\n- **Logic:** For each valid file, read the file content and write it to the destination directory.\\n- **Example Code:**\\n  ```python\\n  from pyspark.sql import DataFrame\\n\\n  for file in valid_files:\\n      df = spark.read.csv(file, header=True, inferSchema=True)\\n      destination_path = file.replace(\\'/data/employee/source\\', \\'/data/employee/destination\\')\\n      df.write.csv(destination_path, header=True)\\n  ```\\n\\n##### 4.3 Automation\\n- **Logic:** Schedule the PySpark script using a scheduler (e.g., Apache Airflow, cron job).\\n- **Example Cron Job:**\\n  ```bash\\n  0 2 * * * /path/to/your/transfer_script.py\\n  ```\\n\\n##### 4.4 Error Handling\\n- **Logic:** Log any errors that occur during the transfer process and send an alert to the support team.\\n- **Example Code:**\\n  ```python\\n  import logging\\n\\n  logging.basicConfig(filename=\\'/path/to/logfile.log\\', level=logging.ERROR)\\n\\n  try:\\n      # Transfer logic here\\n      pass\\n  except Exception as e:\\n      logging.error(f\"Error during file transfer: {str(e)}\")\\n      # Send alert to support team\\n      # send_alert(e)\\n  ```\\n\\n##### 4.5 Testing\\n- **Test Cases:**\\n  - Verify that the file is correctly identified and transferred.\\n  - Verify that the file content is preserved.\\n  - Verify that the process runs without errors.\\n  - Verify that error handling and logging work as expected.\\n- **Test Data:**\\n  - Sample `employee_data_*.csv` file in the source directory.\\n  - Ensure the destination directory exists and has the necessary write permissions.\\n\\n#### 5. Documentation\\n- **Process and Configuration:**\\n  - Detailed steps and configurations for setting up the PySpark script and scheduler.\\n  - Instructions for maintaining and updating the process.\\n  - Sample code and configuration files.\\n- **Support Contact Information:**\\n  - Contact details for the support team to receive alerts.\\n\\n#### 6. Conclusion\\nThis design document covers the complete workflow for transferring employee data files from a source directory to a destination directory using PySpark. The process includes file identification, data transfer, automation, error handling, and testing, ensuring that the data is processed reliably and efficiently.\\n\\n---\\n\\nThis document should provide a comprehensive guide for implementing and maintaining the automated transfer process.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 1003, 'prompt_tokens': 721, 'total_tokens': 1724, 'completion_time': 5.015, 'prompt_time': 0.042662933, 'queue_time': 0.057103187, 'total_time': 5.057662933}, 'model_name': 'qwen-2.5-32b', 'system_fingerprint': 'fp_c527211fd1', 'finish_reason': 'stop', 'logprobs': None}, id='run-383de93c-c422-4b72-8d53-8c344123ac5c-0', usage_metadata={'input_tokens': 721, 'output_tokens': 1003, 'total_tokens': 1724}), HumanMessage(content='It looks good', additional_kwargs={}, response_metadata={})]}, 'user_stories': '### User Story: Transfer Employee Data File Using PySpark\\n\\n**As a** Data Engineer,  \\n**I want to** transfer an employee data file from one directory to another using PySpark,  \\n**So that** the data can be processed in a new directory without manual intervention.\\n\\n**Acceptance Criteria:**\\n\\n1. **File Identification:**\\n   - The system should be able to identify the file containing employee data.\\n   - The file should have a specific naming convention (e.g., `employee_data_*.csv`).\\n\\n2. **Source Directory:**\\n   - The source directory from which the file will be read is `/data/employee/source`.\\n\\n3. **Destination Directory:**\\n   - The destination directory where the file will be transferred is `/data/employee/destination`.\\n\\n4. **Data Integrity:**\\n   - The file should be transferred without any loss or corruption of data.\\n\\n5. **Automation:**\\n   - The transfer process should be fully automated and not require manual intervention.\\n\\n6. **Error Handling:**\\n   - The system should log any errors that occur during the transfer process.\\n   - If an error occurs, the system should send an alert to the support team.\\n\\n7. **Execution Frequency:**\\n   - The transfer process should be executed daily at 2 AM.\\n\\n8. **Testing:**\\n   - The functionality should be tested to ensure the file is correctly moved from the source to the destination directory.\\n\\n9. **Documentation:**\\n   - The process and any necessary configurations should be documented for future reference.\\n\\nThis user story covers the essential aspects of the requirement and can be further broken down into specific tasks and sub-tasks for implementation.', 'func_document': 'Great! Here is a more detailed functional design document based on the user story and acceptance criteria:\\n\\n### Functional Design Document: Transfer Employee Data File Using PySpark\\n\\n#### 1. Introduction\\nThis document outlines the design for the automated transfer of an employee data file from a source directory to a destination directory using PySpark. The process is designed to be fully automated, ensuring data integrity and providing error handling and logging.\\n\\n#### 2. Scope\\n- **Source Directory:** `/data/employee/source`\\n- **Destination Directory:** `/data/employee/destination`\\n- **File Naming Convention:** `employee_data_*.csv`\\n- **Execution Frequency:** Daily at 2 AM\\n\\n#### 3. Functional Requirements\\n\\n##### 3.1 File Identification\\n- The system must identify files in the source directory that match the naming convention `employee_data_*.csv`.\\n\\n##### 3.2 Data Transfer\\n- The system must read the identified file from the source directory and transfer it to the destination directory.\\n- The file transfer should preserve the original file\\'s content without any loss or corruption.\\n\\n##### 3.3 Automation\\n- The transfer process must be fully automated and executed without manual intervention.\\n- The process should be scheduled to run daily at 2 AM.\\n\\n##### 3.4 Error Handling\\n- The system must log any errors that occur during the transfer process.\\n- If an error occurs, the system must send an alert to the support team.\\n\\n##### 3.5 Testing\\n- The functionality must be tested to ensure that the file is correctly moved from the source to the destination directory.\\n\\n##### 3.6 Documentation\\n- Detailed documentation of the process and configurations must be provided for future reference.\\n\\n#### 4. Detailed Design\\n\\n##### 4.1 File Identification\\n- **Logic:** Use PySpark to list files in the `/data/employee/source` directory and filter by the naming convention `employee_data_*.csv`.\\n- **Example Code:**\\n  ```python\\n  from pyspark.sql import SparkSession\\n\\n  spark = SparkSession.builder.appName(\\'EmployeeDataTransfer\\').getOrCreate()\\n\\n  source_dir = \\'/data/employee/source\\'\\n  file_pattern = \\'employee_data_*.csv\\'\\n\\n  # List files in the source directory\\n  files = spark.sparkContext.wholeTextFiles(source_dir).map(lambda x: x[0]).collect()\\n  valid_files = [file for file in files if file_pattern in file]\\n  ```\\n\\n##### 4.2 Data Transfer\\n- **Logic:** For each valid file, read the file content and write it to the destination directory.\\n- **Example Code:**\\n  ```python\\n  from pyspark.sql import DataFrame\\n\\n  for file in valid_files:\\n      df = spark.read.csv(file, header=True, inferSchema=True)\\n      destination_path = file.replace(\\'/data/employee/source\\', \\'/data/employee/destination\\')\\n      df.write.csv(destination_path, header=True)\\n  ```\\n\\n##### 4.3 Automation\\n- **Logic:** Schedule the PySpark script using a scheduler (e.g., Apache Airflow, cron job).\\n- **Example Cron Job:**\\n  ```bash\\n  0 2 * * * /path/to/your/transfer_script.py\\n  ```\\n\\n##### 4.4 Error Handling\\n- **Logic:** Log any errors that occur during the transfer process and send an alert to the support team.\\n- **Example Code:**\\n  ```python\\n  import logging\\n\\n  logging.basicConfig(filename=\\'/path/to/logfile.log\\', level=logging.ERROR)\\n\\n  try:\\n      # Transfer logic here\\n      pass\\n  except Exception as e:\\n      logging.error(f\"Error during file transfer: {str(e)}\")\\n      # Send alert to support team\\n      # send_alert(e)\\n  ```\\n\\n##### 4.5 Testing\\n- **Test Cases:**\\n  - Verify that the file is correctly identified and transferred.\\n  - Verify that the file content is preserved.\\n  - Verify that the process runs without errors.\\n  - Verify that error handling and logging work as expected.\\n- **Test Data:**\\n  - Sample `employee_data_*.csv` file in the source directory.\\n  - Ensure the destination directory exists and has the necessary write permissions.\\n\\n#### 5. Documentation\\n- **Process and Configuration:**\\n  - Detailed steps and configurations for setting up the PySpark script and scheduler.\\n  - Instructions for maintaining and updating the process.\\n  - Sample code and configuration files.\\n- **Support Contact Information:**\\n  - Contact details for the support team to receive alerts.\\n\\n#### 6. Conclusion\\nThis design document covers the complete workflow for transferring employee data files from a source directory to a destination directory using PySpark. The process includes file identification, data transfer, automation, error handling, and testing, ensuring that the data is processed reliably and efficiently.\\n\\n---\\n\\nThis document should provide a comprehensive guide for implementing and maintaining the automated transfer process.', 'tech_document': 'Great! Let\\'s formalize this into a technical design document for the user story.\\n\\n### Technical Design Document: Transfer Employee Data File Using PySpark\\n\\n#### 1. Introduction\\nThis document outlines the technical design for transferring an employee data file from a source directory to a destination directory using PySpark. The goal is to automate the process to ensure data integrity and reduce manual intervention.\\n\\n#### 2. Requirements\\n- **Source Directory:** `/data/employee/source`\\n- **Destination Directory:** `/data/employee/destination`\\n- **File Naming Convention:** `employee_data_*.csv`\\n- **Data Integrity:** Ensure no loss or corruption of data during the transfer.\\n- **Automation:** Fully automated process with no manual intervention.\\n- **Error Handling:** Log errors and alert the support team if any issues occur.\\n- **Execution Frequency:** Daily at 2 AM.\\n- **Testing:** Ensure the file is correctly moved from the source to the destination directory.\\n- **Documentation:** Document the process and configurations for future reference.\\n\\n#### 3. System Architecture\\n- **Data Source:** `/data/employee/source`\\n- **Data Destination:** `/data/employee/destination`\\n- **Programming Language:** Python (PySpark)\\n- **Scheduling Tool:** Cron (or equivalent scheduling tool)\\n- **Logging and Monitoring:** Use logging to track the process and send alerts for errors.\\n\\n#### 4. Detailed Design\\n\\n##### 4.1 File Identification\\n- **Pattern Matching:** Use regular expressions to identify files matching the `employee_data_*.csv` pattern.\\n\\n##### 4.2 PySpark Implementation\\n- **Reading the File:**\\n  ```python\\n  from pyspark.sql import SparkSession\\n\\n  spark = SparkSession.builder.appName(\"EmployeeDataTransfer\").getOrCreate()\\n\\n  source_dir = \"/data/employee/source\"\\n  df = spark.read.csv(f\"{source_dir}/employee_data_*.csv\", header=True, inferSchema=True)\\n  ```\\n\\n- **Writing the File:**\\n  ```python\\n  dest_dir = \"/data/employee/destination\"\\n  df.write.csv(dest_dir, header=True, mode=\"overwrite\")\\n  ```\\n\\n##### 4.3 Error Handling\\n- **Logging:**\\n  ```python\\n  import logging\\n\\n  logging.basicConfig(filename=\\'employee_data_transfer.log\\', level=logging.INFO)\\n\\n  try:\\n      # File transfer logic\\n      pass\\n  except Exception as e:\\n      logging.error(f\"Error during file transfer: {e}\")\\n      # Send alert to support team\\n      send_alert(f\"Error during file transfer: {e}\")\\n  ```\\n\\n- **Alerting:**\\n  ```python\\n  def send_alert(message):\\n      # Implement the logic to send an alert (e.g., email or messaging service)\\n      pass\\n  ```\\n\\n##### 4.4 Scheduling\\n- **Cron Job:**\\n  - Create a cron job to run the PySpark script daily at 2 AM.\\n  - Example cron job entry:\\n    ```bash\\n    0 2 * * * /path/to/your/script.py >> /path/to/logfile.log 2>&1\\n    ```\\n\\n#### 5. Testing\\n- **Unit Tests:**\\n  - Verify that the file is correctly read and written.\\n  - Ensure that the file contents are not corrupted during the transfer.\\n\\n- **Integration Tests:**\\n  - Test the entire process to ensure the file is moved from the source to the destination directory.\\n  - Verify that the file is transferred daily at the specified time.\\n\\n#### 6. Documentation\\n- **Process Documentation:**\\n  - Document the steps involved in the process.\\n  - Include details on how to run the script and configure the cron job.\\n\\n- **Configuration Documentation:**\\n  - Document the configuration settings for PySpark and the cron job.\\n  - Include any necessary environment variables and dependencies.\\n\\n#### 7. Conclusion\\nThis technical design document provides a comprehensive approach to automating the transfer of employee data files using PySpark. The design ensures data integrity, error handling, and automated scheduling, meeting the specified requirements.\\n\\n---\\n\\n### Example Script\\nHere is an example script that implements the above design:\\n\\n```python\\nimport os\\nimport logging\\nfrom pyspark.sql import SparkSession\\n\\n# Set up logging\\nlogging.basicConfig(filename=\\'employee_data_transfer.log\\', level=logging.INFO)\\n\\n# Initialize Spark session\\nspark = SparkSession.builder.appName(\"EmployeeDataTransfer\").getOrCreate()\\n\\n# Define directories\\nsource_dir = \"/data/employee/source\"\\ndest_dir = \"/data/employee/destination\"\\n\\ndef send_alert(message):\\n    # Implement the logic to send an alert (e.g., email or messaging service)\\n    pass\\n\\ntry:\\n    # Read the file\\n    df = spark.read.csv(f\"{source_dir}/employee_data_*.csv\", header=True, inferSchema=True)\\n    \\n    # Write the file\\n    df.write.csv(dest_dir, header=True, mode=\"overwrite\")\\n    \\n    logging.info(\"File transfer completed successfully.\")\\nexcept Exception as e:\\n    logging.error(f\"Error during file transfer: {e}\")\\n    send_alert(f\"Error during file transfer: {e}\")\\n\\n# Stop Spark session\\nspark.stop()\\n```\\n\\n### Cron Job Configuration\\nAdd the following cron job entry to run the script daily at 2 AM:\\n\\n```bash\\n0 2 * * * /usr/bin/python3 /path/to/your/script.py >> /path/to/logfile.log 2>&1\\n```\\n\\nThis ensures the script runs as scheduled, and logs are recorded for monitoring and troubleshooting.'}\n",
      "Decision made by evaluator: positive\n",
      "{'messages': {'messages': [AIMessage(content='Great! Here is a more detailed functional design document based on the user story and acceptance criteria:\\n\\n### Functional Design Document: Transfer Employee Data File Using PySpark\\n\\n#### 1. Introduction\\nThis document outlines the design for the automated transfer of an employee data file from a source directory to a destination directory using PySpark. The process is designed to be fully automated, ensuring data integrity and providing error handling and logging.\\n\\n#### 2. Scope\\n- **Source Directory:** `/data/employee/source`\\n- **Destination Directory:** `/data/employee/destination`\\n- **File Naming Convention:** `employee_data_*.csv`\\n- **Execution Frequency:** Daily at 2 AM\\n\\n#### 3. Functional Requirements\\n\\n##### 3.1 File Identification\\n- The system must identify files in the source directory that match the naming convention `employee_data_*.csv`.\\n\\n##### 3.2 Data Transfer\\n- The system must read the identified file from the source directory and transfer it to the destination directory.\\n- The file transfer should preserve the original file\\'s content without any loss or corruption.\\n\\n##### 3.3 Automation\\n- The transfer process must be fully automated and executed without manual intervention.\\n- The process should be scheduled to run daily at 2 AM.\\n\\n##### 3.4 Error Handling\\n- The system must log any errors that occur during the transfer process.\\n- If an error occurs, the system must send an alert to the support team.\\n\\n##### 3.5 Testing\\n- The functionality must be tested to ensure that the file is correctly moved from the source to the destination directory.\\n\\n##### 3.6 Documentation\\n- Detailed documentation of the process and configurations must be provided for future reference.\\n\\n#### 4. Detailed Design\\n\\n##### 4.1 File Identification\\n- **Logic:** Use PySpark to list files in the `/data/employee/source` directory and filter by the naming convention `employee_data_*.csv`.\\n- **Example Code:**\\n  ```python\\n  from pyspark.sql import SparkSession\\n\\n  spark = SparkSession.builder.appName(\\'EmployeeDataTransfer\\').getOrCreate()\\n\\n  source_dir = \\'/data/employee/source\\'\\n  file_pattern = \\'employee_data_*.csv\\'\\n\\n  # List files in the source directory\\n  files = spark.sparkContext.wholeTextFiles(source_dir).map(lambda x: x[0]).collect()\\n  valid_files = [file for file in files if file_pattern in file]\\n  ```\\n\\n##### 4.2 Data Transfer\\n- **Logic:** For each valid file, read the file content and write it to the destination directory.\\n- **Example Code:**\\n  ```python\\n  from pyspark.sql import DataFrame\\n\\n  for file in valid_files:\\n      df = spark.read.csv(file, header=True, inferSchema=True)\\n      destination_path = file.replace(\\'/data/employee/source\\', \\'/data/employee/destination\\')\\n      df.write.csv(destination_path, header=True)\\n  ```\\n\\n##### 4.3 Automation\\n- **Logic:** Schedule the PySpark script using a scheduler (e.g., Apache Airflow, cron job).\\n- **Example Cron Job:**\\n  ```bash\\n  0 2 * * * /path/to/your/transfer_script.py\\n  ```\\n\\n##### 4.4 Error Handling\\n- **Logic:** Log any errors that occur during the transfer process and send an alert to the support team.\\n- **Example Code:**\\n  ```python\\n  import logging\\n\\n  logging.basicConfig(filename=\\'/path/to/logfile.log\\', level=logging.ERROR)\\n\\n  try:\\n      # Transfer logic here\\n      pass\\n  except Exception as e:\\n      logging.error(f\"Error during file transfer: {str(e)}\")\\n      # Send alert to support team\\n      # send_alert(e)\\n  ```\\n\\n##### 4.5 Testing\\n- **Test Cases:**\\n  - Verify that the file is correctly identified and transferred.\\n  - Verify that the file content is preserved.\\n  - Verify that the process runs without errors.\\n  - Verify that error handling and logging work as expected.\\n- **Test Data:**\\n  - Sample `employee_data_*.csv` file in the source directory.\\n  - Ensure the destination directory exists and has the necessary write permissions.\\n\\n#### 5. Documentation\\n- **Process and Configuration:**\\n  - Detailed steps and configurations for setting up the PySpark script and scheduler.\\n  - Instructions for maintaining and updating the process.\\n  - Sample code and configuration files.\\n- **Support Contact Information:**\\n  - Contact details for the support team to receive alerts.\\n\\n#### 6. Conclusion\\nThis design document covers the complete workflow for transferring employee data files from a source directory to a destination directory using PySpark. The process includes file identification, data transfer, automation, error handling, and testing, ensuring that the data is processed reliably and efficiently.\\n\\n---\\n\\nThis document should provide a comprehensive guide for implementing and maintaining the automated transfer process.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 1003, 'prompt_tokens': 721, 'total_tokens': 1724, 'completion_time': 5.015, 'prompt_time': 0.042662933, 'queue_time': 0.057103187, 'total_time': 5.057662933}, 'model_name': 'qwen-2.5-32b', 'system_fingerprint': 'fp_c527211fd1', 'finish_reason': 'stop', 'logprobs': None}, id='run-383de93c-c422-4b72-8d53-8c344123ac5c-0', usage_metadata={'input_tokens': 721, 'output_tokens': 1003, 'total_tokens': 1724}), HumanMessage(content='It looks good', additional_kwargs={}, response_metadata={})]}, 'user_stories': '### User Story: Transfer Employee Data File Using PySpark\\n\\n**As a** Data Engineer,  \\n**I want to** transfer an employee data file from one directory to another using PySpark,  \\n**So that** the data can be processed in a new directory without manual intervention.\\n\\n**Acceptance Criteria:**\\n\\n1. **File Identification:**\\n   - The system should be able to identify the file containing employee data.\\n   - The file should have a specific naming convention (e.g., `employee_data_*.csv`).\\n\\n2. **Source Directory:**\\n   - The source directory from which the file will be read is `/data/employee/source`.\\n\\n3. **Destination Directory:**\\n   - The destination directory where the file will be transferred is `/data/employee/destination`.\\n\\n4. **Data Integrity:**\\n   - The file should be transferred without any loss or corruption of data.\\n\\n5. **Automation:**\\n   - The transfer process should be fully automated and not require manual intervention.\\n\\n6. **Error Handling:**\\n   - The system should log any errors that occur during the transfer process.\\n   - If an error occurs, the system should send an alert to the support team.\\n\\n7. **Execution Frequency:**\\n   - The transfer process should be executed daily at 2 AM.\\n\\n8. **Testing:**\\n   - The functionality should be tested to ensure the file is correctly moved from the source to the destination directory.\\n\\n9. **Documentation:**\\n   - The process and any necessary configurations should be documented for future reference.\\n\\nThis user story covers the essential aspects of the requirement and can be further broken down into specific tasks and sub-tasks for implementation.', 'func_document': 'Great! Here is a more detailed functional design document based on the user story and acceptance criteria:\\n\\n### Functional Design Document: Transfer Employee Data File Using PySpark\\n\\n#### 1. Introduction\\nThis document outlines the design for the automated transfer of an employee data file from a source directory to a destination directory using PySpark. The process is designed to be fully automated, ensuring data integrity and providing error handling and logging.\\n\\n#### 2. Scope\\n- **Source Directory:** `/data/employee/source`\\n- **Destination Directory:** `/data/employee/destination`\\n- **File Naming Convention:** `employee_data_*.csv`\\n- **Execution Frequency:** Daily at 2 AM\\n\\n#### 3. Functional Requirements\\n\\n##### 3.1 File Identification\\n- The system must identify files in the source directory that match the naming convention `employee_data_*.csv`.\\n\\n##### 3.2 Data Transfer\\n- The system must read the identified file from the source directory and transfer it to the destination directory.\\n- The file transfer should preserve the original file\\'s content without any loss or corruption.\\n\\n##### 3.3 Automation\\n- The transfer process must be fully automated and executed without manual intervention.\\n- The process should be scheduled to run daily at 2 AM.\\n\\n##### 3.4 Error Handling\\n- The system must log any errors that occur during the transfer process.\\n- If an error occurs, the system must send an alert to the support team.\\n\\n##### 3.5 Testing\\n- The functionality must be tested to ensure that the file is correctly moved from the source to the destination directory.\\n\\n##### 3.6 Documentation\\n- Detailed documentation of the process and configurations must be provided for future reference.\\n\\n#### 4. Detailed Design\\n\\n##### 4.1 File Identification\\n- **Logic:** Use PySpark to list files in the `/data/employee/source` directory and filter by the naming convention `employee_data_*.csv`.\\n- **Example Code:**\\n  ```python\\n  from pyspark.sql import SparkSession\\n\\n  spark = SparkSession.builder.appName(\\'EmployeeDataTransfer\\').getOrCreate()\\n\\n  source_dir = \\'/data/employee/source\\'\\n  file_pattern = \\'employee_data_*.csv\\'\\n\\n  # List files in the source directory\\n  files = spark.sparkContext.wholeTextFiles(source_dir).map(lambda x: x[0]).collect()\\n  valid_files = [file for file in files if file_pattern in file]\\n  ```\\n\\n##### 4.2 Data Transfer\\n- **Logic:** For each valid file, read the file content and write it to the destination directory.\\n- **Example Code:**\\n  ```python\\n  from pyspark.sql import DataFrame\\n\\n  for file in valid_files:\\n      df = spark.read.csv(file, header=True, inferSchema=True)\\n      destination_path = file.replace(\\'/data/employee/source\\', \\'/data/employee/destination\\')\\n      df.write.csv(destination_path, header=True)\\n  ```\\n\\n##### 4.3 Automation\\n- **Logic:** Schedule the PySpark script using a scheduler (e.g., Apache Airflow, cron job).\\n- **Example Cron Job:**\\n  ```bash\\n  0 2 * * * /path/to/your/transfer_script.py\\n  ```\\n\\n##### 4.4 Error Handling\\n- **Logic:** Log any errors that occur during the transfer process and send an alert to the support team.\\n- **Example Code:**\\n  ```python\\n  import logging\\n\\n  logging.basicConfig(filename=\\'/path/to/logfile.log\\', level=logging.ERROR)\\n\\n  try:\\n      # Transfer logic here\\n      pass\\n  except Exception as e:\\n      logging.error(f\"Error during file transfer: {str(e)}\")\\n      # Send alert to support team\\n      # send_alert(e)\\n  ```\\n\\n##### 4.5 Testing\\n- **Test Cases:**\\n  - Verify that the file is correctly identified and transferred.\\n  - Verify that the file content is preserved.\\n  - Verify that the process runs without errors.\\n  - Verify that error handling and logging work as expected.\\n- **Test Data:**\\n  - Sample `employee_data_*.csv` file in the source directory.\\n  - Ensure the destination directory exists and has the necessary write permissions.\\n\\n#### 5. Documentation\\n- **Process and Configuration:**\\n  - Detailed steps and configurations for setting up the PySpark script and scheduler.\\n  - Instructions for maintaining and updating the process.\\n  - Sample code and configuration files.\\n- **Support Contact Information:**\\n  - Contact details for the support team to receive alerts.\\n\\n#### 6. Conclusion\\nThis design document covers the complete workflow for transferring employee data files from a source directory to a destination directory using PySpark. The process includes file identification, data transfer, automation, error handling, and testing, ensuring that the data is processed reliably and efficiently.\\n\\n---\\n\\nThis document should provide a comprehensive guide for implementing and maintaining the automated transfer process.', 'tech_document': 'Great! Let\\'s formalize this into a technical design document for the user story.\\n\\n### Technical Design Document: Transfer Employee Data File Using PySpark\\n\\n#### 1. Introduction\\nThis document outlines the technical design for transferring an employee data file from a source directory to a destination directory using PySpark. The goal is to automate the process to ensure data integrity and reduce manual intervention.\\n\\n#### 2. Requirements\\n- **Source Directory:** `/data/employee/source`\\n- **Destination Directory:** `/data/employee/destination`\\n- **File Naming Convention:** `employee_data_*.csv`\\n- **Data Integrity:** Ensure no loss or corruption of data during the transfer.\\n- **Automation:** Fully automated process with no manual intervention.\\n- **Error Handling:** Log errors and alert the support team if any issues occur.\\n- **Execution Frequency:** Daily at 2 AM.\\n- **Testing:** Ensure the file is correctly moved from the source to the destination directory.\\n- **Documentation:** Document the process and configurations for future reference.\\n\\n#### 3. System Architecture\\n- **Data Source:** `/data/employee/source`\\n- **Data Destination:** `/data/employee/destination`\\n- **Programming Language:** Python (PySpark)\\n- **Scheduling Tool:** Cron (or equivalent scheduling tool)\\n- **Logging and Monitoring:** Use logging to track the process and send alerts for errors.\\n\\n#### 4. Detailed Design\\n\\n##### 4.1 File Identification\\n- **Pattern Matching:** Use regular expressions to identify files matching the `employee_data_*.csv` pattern.\\n\\n##### 4.2 PySpark Implementation\\n- **Reading the File:**\\n  ```python\\n  from pyspark.sql import SparkSession\\n\\n  spark = SparkSession.builder.appName(\"EmployeeDataTransfer\").getOrCreate()\\n\\n  source_dir = \"/data/employee/source\"\\n  df = spark.read.csv(f\"{source_dir}/employee_data_*.csv\", header=True, inferSchema=True)\\n  ```\\n\\n- **Writing the File:**\\n  ```python\\n  dest_dir = \"/data/employee/destination\"\\n  df.write.csv(dest_dir, header=True, mode=\"overwrite\")\\n  ```\\n\\n##### 4.3 Error Handling\\n- **Logging:**\\n  ```python\\n  import logging\\n\\n  logging.basicConfig(filename=\\'employee_data_transfer.log\\', level=logging.INFO)\\n\\n  try:\\n      # File transfer logic\\n      pass\\n  except Exception as e:\\n      logging.error(f\"Error during file transfer: {e}\")\\n      # Send alert to support team\\n      send_alert(f\"Error during file transfer: {e}\")\\n  ```\\n\\n- **Alerting:**\\n  ```python\\n  def send_alert(message):\\n      # Implement the logic to send an alert (e.g., email or messaging service)\\n      pass\\n  ```\\n\\n##### 4.4 Scheduling\\n- **Cron Job:**\\n  - Create a cron job to run the PySpark script daily at 2 AM.\\n  - Example cron job entry:\\n    ```bash\\n    0 2 * * * /path/to/your/script.py >> /path/to/logfile.log 2>&1\\n    ```\\n\\n#### 5. Testing\\n- **Unit Tests:**\\n  - Verify that the file is correctly read and written.\\n  - Ensure that the file contents are not corrupted during the transfer.\\n\\n- **Integration Tests:**\\n  - Test the entire process to ensure the file is moved from the source to the destination directory.\\n  - Verify that the file is transferred daily at the specified time.\\n\\n#### 6. Documentation\\n- **Process Documentation:**\\n  - Document the steps involved in the process.\\n  - Include details on how to run the script and configure the cron job.\\n\\n- **Configuration Documentation:**\\n  - Document the configuration settings for PySpark and the cron job.\\n  - Include any necessary environment variables and dependencies.\\n\\n#### 7. Conclusion\\nThis technical design document provides a comprehensive approach to automating the transfer of employee data files using PySpark. The design ensures data integrity, error handling, and automated scheduling, meeting the specified requirements.\\n\\n---\\n\\n### Example Script\\nHere is an example script that implements the above design:\\n\\n```python\\nimport os\\nimport logging\\nfrom pyspark.sql import SparkSession\\n\\n# Set up logging\\nlogging.basicConfig(filename=\\'employee_data_transfer.log\\', level=logging.INFO)\\n\\n# Initialize Spark session\\nspark = SparkSession.builder.appName(\"EmployeeDataTransfer\").getOrCreate()\\n\\n# Define directories\\nsource_dir = \"/data/employee/source\"\\ndest_dir = \"/data/employee/destination\"\\n\\ndef send_alert(message):\\n    # Implement the logic to send an alert (e.g., email or messaging service)\\n    pass\\n\\ntry:\\n    # Read the file\\n    df = spark.read.csv(f\"{source_dir}/employee_data_*.csv\", header=True, inferSchema=True)\\n    \\n    # Write the file\\n    df.write.csv(dest_dir, header=True, mode=\"overwrite\")\\n    \\n    logging.info(\"File transfer completed successfully.\")\\nexcept Exception as e:\\n    logging.error(f\"Error during file transfer: {e}\")\\n    send_alert(f\"Error during file transfer: {e}\")\\n\\n# Stop Spark session\\nspark.stop()\\n```\\n\\n### Cron Job Configuration\\nAdd the following cron job entry to run the script daily at 2 AM:\\n\\n```bash\\n0 2 * * * /usr/bin/python3 /path/to/your/script.py >> /path/to/logfile.log 2>&1\\n```\\n\\nThis ensures the script runs as scheduled, and logs are recorded for monitoring and troubleshooting.'}\n",
      "{'messages': [AIMessage(content='I\\'m glad you think the document looks good! If you need any further refinements or additional details, feel free to let me know. Here is a consolidated version of the code and documentation based on the provided functional and technical design documents:\\n\\n### Functional Design Document: Transfer Employee Data File Using PySpark\\n\\n#### 1. Introduction\\nThis document outlines the design for the automated transfer of an employee data file from a source directory to a destination directory using PySpark. The process is designed to be fully automated, ensuring data integrity and providing error handling and logging.\\n\\n#### 2. Scope\\n- **Source Directory:** `/data/employee/source`\\n- **Destination Directory:** `/data/employee/destination`\\n- **File Naming Convention:** `employee_data_*.csv`\\n- **Execution Frequency:** Daily at 2 AM\\n\\n#### 3. Functional Requirements\\n\\n##### 3.1 File Identification\\n- The system must identify files in the source directory that match the naming convention `employee_data_*.csv`.\\n\\n##### 3.2 Data Transfer\\n- The system must read the identified file from the source directory and transfer it to the destination directory.\\n- The file transfer should preserve the original file\\'s content without any loss or corruption.\\n\\n##### 3.3 Automation\\n- The transfer process must be fully automated and executed without manual intervention.\\n- The process should be scheduled to run daily at 2 AM.\\n\\n##### 3.4 Error Handling\\n- The system must log any errors that occur during the transfer process.\\n- If an error occurs, the system must send an alert to the support team.\\n\\n##### 3.5 Testing\\n- The functionality must be tested to ensure that the file is correctly moved from the source to the destination directory.\\n\\n##### 3.6 Documentation\\n- Detailed documentation of the process and configurations must be provided for future reference.\\n\\n#### 4. Detailed Design\\n\\n##### 4.1 File Identification\\n- **Logic:** Use PySpark to list files in the `/data/employee/source` directory and filter by the naming convention `employee_data_*.csv`.\\n- **Example Code:**\\n  ```python\\n  from pyspark.sql import SparkSession\\n  import os\\n  import glob\\n\\n  spark = SparkSession.builder.appName(\"EmployeeDataTransfer\").getOrCreate()\\n\\n  source_dir = \"/data/employee/source\"\\n  file_pattern = \"employee_data_*.csv\"\\n\\n  # List files in the source directory\\n  valid_files = glob.glob(os.path.join(source_dir, file_pattern))\\n  ```\\n\\n##### 4.2 Data Transfer\\n- **Logic:** For each valid file, read the file content and write it to the destination directory.\\n- **Example Code:**\\n  ```python\\n  from pyspark.sql import DataFrame\\n\\n  dest_dir = \"/data/employee/destination\"\\n\\n  for file in valid_files:\\n      df = spark.read.csv(file, header=True, inferSchema=True)\\n      destination_path = os.path.join(dest_dir, os.path.basename(file))\\n      df.write.csv(destination_path, header=True, mode=\"overwrite\")\\n  ```\\n\\n##### 4.3 Automation\\n- **Logic:** Schedule the PySpark script using a scheduler (e.g., Apache Airflow, cron job).\\n- **Example Cron Job:**\\n  ```bash\\n  0 2 * * * /usr/bin/python3 /path/to/your/transfer_script.py >> /path/to/logfile.log 2>&1\\n  ```\\n\\n##### 4.4 Error Handling\\n- **Logic:** Log any errors that occur during the transfer process and send an alert to the support team.\\n- **Example Code:**\\n  ```python\\n  import logging\\n\\n  logging.basicConfig(filename=\\'/path/to/logfile.log\\', level=logging.ERROR)\\n\\n  def send_alert(message):\\n      # Implement the logic to send an alert (e.g., email or messaging service)\\n      pass\\n\\n  try:\\n      # Transfer logic\\n      source_dir = \"/data/employee/source\"\\n      dest_dir = \"/data/employee/destination\"\\n      file_pattern = \"employee_data_*.csv\"\\n\\n      valid_files = glob.glob(os.path.join(source_dir, file_pattern))\\n\\n      for file in valid_files:\\n          df = spark.read.csv(file, header=True, inferSchema=True)\\n          destination_path = os.path.join(dest_dir, os.path.basename(file))\\n          df.write.csv(destination_path, header=True, mode=\"overwrite\")\\n      \\n      logging.info(\"File transfer completed successfully.\")\\n  except Exception as e:\\n      logging.error(f\"Error during file transfer: {str(e)}\")\\n      send_alert(f\"Error during file transfer: {str(e)}\")\\n\\n  # Stop Spark session\\n  spark.stop()\\n  ```\\n\\n#### 5. Testing\\n- **Test Cases:**\\n  - Verify that the file is correctly identified and transferred.\\n  - Verify that the file content is preserved.\\n  - Verify that the process runs without errors.\\n  - Verify that error handling and logging work as expected.\\n- **Test Data:**\\n  - Sample `employee_data_*.csv` file in the source directory.\\n  - Ensure the destination directory exists and has the necessary write permissions.\\n\\n#### 6. Documentation\\n- **Process and Configuration:**\\n  - Detailed steps and configurations for setting up the PySpark script and scheduler.\\n  - Instructions for maintaining and updating the process.\\n  - Sample code and configuration files.\\n- **Support Contact Information:**\\n  - Contact details for the support team to receive alerts.\\n\\n#### 7. Conclusion\\nThis design document covers the complete workflow for transferring employee data files from a source directory to a destination directory using PySpark. The process includes file identification, data transfer, automation, error handling, and testing, ensuring that the data is processed reliably and efficiently.\\n\\n### Example Script\\n\\nHere is the complete example script:\\n\\n```python\\nimport os\\nimport glob\\nimport logging\\nfrom pyspark.sql import SparkSession\\n\\n# Set up logging\\nlogging.basicConfig(filename=\\'/path/to/logfile.log\\', level=logging.ERROR)\\n\\ndef send_alert(message):\\n    # Implement the logic to send an alert (e.g., email or messaging service)\\n    pass\\n\\ndef transfer_files():\\n    spark = SparkSession.builder.appName(\\'EmployeeDataTransfer\\').getOrCreate()\\n\\n    source_dir = \\'/data/employee/source\\'\\n    dest_dir = \\'/data/employee/destination\\'\\n    file_pattern = \\'employee_data_*.csv\\'\\n\\n    try:\\n        # List files in the source directory\\n        valid_files = glob.glob(os.path.join(source_dir, file_pattern))\\n\\n        for file in valid_files:\\n            df = spark.read.csv(file, header=True, inferSchema=True)\\n            destination_path = os.path.join(dest_dir, os.path.basename(file))\\n            df.write.csv(destination_path, header=True, mode=\"overwrite\")\\n        \\n        logging.info(\"File transfer completed successfully.\")\\n    except Exception as e:\\n        logging.error(f\"Error during file transfer: {str(e)}\")\\n        send_alert(f\"Error during file transfer: {str(e)}\")\\n\\n    spark.stop()\\n\\nif __name__ == \"__main__\":\\n    transfer_files()\\n```\\n\\n### Cron Job Configuration\\nAdd the following cron job entry to run the script daily at 2 AM:\\n\\n```bash\\n0 2 * * * /usr/bin/python3 /path/to/your/transfer_script.py >> /path/to/logfile.log 2>&1\\n```\\n\\n### Summary\\nThis document and script cover the complete workflow for transferring employee data files from a source directory to a destination directory using PySpark. The process includes file identification, data transfer, automation, error handling, and testing, ensuring that the data is processed reliably and efficiently.\\n\\nIf you need any further enhancements or additional details, please let me know!', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 1565, 'prompt_tokens': 3193, 'total_tokens': 4758, 'completion_time': 7.825, 'prompt_time': 0.187541884, 'queue_time': 0.09697252900000003, 'total_time': 8.012541884}, 'model_name': 'qwen-2.5-32b', 'system_fingerprint': 'fp_c527211fd1', 'finish_reason': 'stop', 'logprobs': None}, id='run-91a2c4a2-0339-4c49-9435-fa03ec18b9d0-0', usage_metadata={'input_tokens': 3193, 'output_tokens': 1565, 'total_tokens': 4758})], 'user_stories': '### User Story: Transfer Employee Data File Using PySpark\\n\\n**As a** Data Engineer,  \\n**I want to** transfer an employee data file from one directory to another using PySpark,  \\n**So that** the data can be processed in a new directory without manual intervention.\\n\\n**Acceptance Criteria:**\\n\\n1. **File Identification:**\\n   - The system should be able to identify the file containing employee data.\\n   - The file should have a specific naming convention (e.g., `employee_data_*.csv`).\\n\\n2. **Source Directory:**\\n   - The source directory from which the file will be read is `/data/employee/source`.\\n\\n3. **Destination Directory:**\\n   - The destination directory where the file will be transferred is `/data/employee/destination`.\\n\\n4. **Data Integrity:**\\n   - The file should be transferred without any loss or corruption of data.\\n\\n5. **Automation:**\\n   - The transfer process should be fully automated and not require manual intervention.\\n\\n6. **Error Handling:**\\n   - The system should log any errors that occur during the transfer process.\\n   - If an error occurs, the system should send an alert to the support team.\\n\\n7. **Execution Frequency:**\\n   - The transfer process should be executed daily at 2 AM.\\n\\n8. **Testing:**\\n   - The functionality should be tested to ensure the file is correctly moved from the source to the destination directory.\\n\\n9. **Documentation:**\\n   - The process and any necessary configurations should be documented for future reference.\\n\\nThis user story covers the essential aspects of the requirement and can be further broken down into specific tasks and sub-tasks for implementation.', 'func_document': 'Great! Here is a more detailed functional design document based on the user story and acceptance criteria:\\n\\n### Functional Design Document: Transfer Employee Data File Using PySpark\\n\\n#### 1. Introduction\\nThis document outlines the design for the automated transfer of an employee data file from a source directory to a destination directory using PySpark. The process is designed to be fully automated, ensuring data integrity and providing error handling and logging.\\n\\n#### 2. Scope\\n- **Source Directory:** `/data/employee/source`\\n- **Destination Directory:** `/data/employee/destination`\\n- **File Naming Convention:** `employee_data_*.csv`\\n- **Execution Frequency:** Daily at 2 AM\\n\\n#### 3. Functional Requirements\\n\\n##### 3.1 File Identification\\n- The system must identify files in the source directory that match the naming convention `employee_data_*.csv`.\\n\\n##### 3.2 Data Transfer\\n- The system must read the identified file from the source directory and transfer it to the destination directory.\\n- The file transfer should preserve the original file\\'s content without any loss or corruption.\\n\\n##### 3.3 Automation\\n- The transfer process must be fully automated and executed without manual intervention.\\n- The process should be scheduled to run daily at 2 AM.\\n\\n##### 3.4 Error Handling\\n- The system must log any errors that occur during the transfer process.\\n- If an error occurs, the system must send an alert to the support team.\\n\\n##### 3.5 Testing\\n- The functionality must be tested to ensure that the file is correctly moved from the source to the destination directory.\\n\\n##### 3.6 Documentation\\n- Detailed documentation of the process and configurations must be provided for future reference.\\n\\n#### 4. Detailed Design\\n\\n##### 4.1 File Identification\\n- **Logic:** Use PySpark to list files in the `/data/employee/source` directory and filter by the naming convention `employee_data_*.csv`.\\n- **Example Code:**\\n  ```python\\n  from pyspark.sql import SparkSession\\n\\n  spark = SparkSession.builder.appName(\\'EmployeeDataTransfer\\').getOrCreate()\\n\\n  source_dir = \\'/data/employee/source\\'\\n  file_pattern = \\'employee_data_*.csv\\'\\n\\n  # List files in the source directory\\n  files = spark.sparkContext.wholeTextFiles(source_dir).map(lambda x: x[0]).collect()\\n  valid_files = [file for file in files if file_pattern in file]\\n  ```\\n\\n##### 4.2 Data Transfer\\n- **Logic:** For each valid file, read the file content and write it to the destination directory.\\n- **Example Code:**\\n  ```python\\n  from pyspark.sql import DataFrame\\n\\n  for file in valid_files:\\n      df = spark.read.csv(file, header=True, inferSchema=True)\\n      destination_path = file.replace(\\'/data/employee/source\\', \\'/data/employee/destination\\')\\n      df.write.csv(destination_path, header=True)\\n  ```\\n\\n##### 4.3 Automation\\n- **Logic:** Schedule the PySpark script using a scheduler (e.g., Apache Airflow, cron job).\\n- **Example Cron Job:**\\n  ```bash\\n  0 2 * * * /path/to/your/transfer_script.py\\n  ```\\n\\n##### 4.4 Error Handling\\n- **Logic:** Log any errors that occur during the transfer process and send an alert to the support team.\\n- **Example Code:**\\n  ```python\\n  import logging\\n\\n  logging.basicConfig(filename=\\'/path/to/logfile.log\\', level=logging.ERROR)\\n\\n  try:\\n      # Transfer logic here\\n      pass\\n  except Exception as e:\\n      logging.error(f\"Error during file transfer: {str(e)}\")\\n      # Send alert to support team\\n      # send_alert(e)\\n  ```\\n\\n##### 4.5 Testing\\n- **Test Cases:**\\n  - Verify that the file is correctly identified and transferred.\\n  - Verify that the file content is preserved.\\n  - Verify that the process runs without errors.\\n  - Verify that error handling and logging work as expected.\\n- **Test Data:**\\n  - Sample `employee_data_*.csv` file in the source directory.\\n  - Ensure the destination directory exists and has the necessary write permissions.\\n\\n#### 5. Documentation\\n- **Process and Configuration:**\\n  - Detailed steps and configurations for setting up the PySpark script and scheduler.\\n  - Instructions for maintaining and updating the process.\\n  - Sample code and configuration files.\\n- **Support Contact Information:**\\n  - Contact details for the support team to receive alerts.\\n\\n#### 6. Conclusion\\nThis design document covers the complete workflow for transferring employee data files from a source directory to a destination directory using PySpark. The process includes file identification, data transfer, automation, error handling, and testing, ensuring that the data is processed reliably and efficiently.\\n\\n---\\n\\nThis document should provide a comprehensive guide for implementing and maintaining the automated transfer process.', 'tech_document': 'Great! Let\\'s formalize this into a technical design document for the user story.\\n\\n### Technical Design Document: Transfer Employee Data File Using PySpark\\n\\n#### 1. Introduction\\nThis document outlines the technical design for transferring an employee data file from a source directory to a destination directory using PySpark. The goal is to automate the process to ensure data integrity and reduce manual intervention.\\n\\n#### 2. Requirements\\n- **Source Directory:** `/data/employee/source`\\n- **Destination Directory:** `/data/employee/destination`\\n- **File Naming Convention:** `employee_data_*.csv`\\n- **Data Integrity:** Ensure no loss or corruption of data during the transfer.\\n- **Automation:** Fully automated process with no manual intervention.\\n- **Error Handling:** Log errors and alert the support team if any issues occur.\\n- **Execution Frequency:** Daily at 2 AM.\\n- **Testing:** Ensure the file is correctly moved from the source to the destination directory.\\n- **Documentation:** Document the process and configurations for future reference.\\n\\n#### 3. System Architecture\\n- **Data Source:** `/data/employee/source`\\n- **Data Destination:** `/data/employee/destination`\\n- **Programming Language:** Python (PySpark)\\n- **Scheduling Tool:** Cron (or equivalent scheduling tool)\\n- **Logging and Monitoring:** Use logging to track the process and send alerts for errors.\\n\\n#### 4. Detailed Design\\n\\n##### 4.1 File Identification\\n- **Pattern Matching:** Use regular expressions to identify files matching the `employee_data_*.csv` pattern.\\n\\n##### 4.2 PySpark Implementation\\n- **Reading the File:**\\n  ```python\\n  from pyspark.sql import SparkSession\\n\\n  spark = SparkSession.builder.appName(\"EmployeeDataTransfer\").getOrCreate()\\n\\n  source_dir = \"/data/employee/source\"\\n  df = spark.read.csv(f\"{source_dir}/employee_data_*.csv\", header=True, inferSchema=True)\\n  ```\\n\\n- **Writing the File:**\\n  ```python\\n  dest_dir = \"/data/employee/destination\"\\n  df.write.csv(dest_dir, header=True, mode=\"overwrite\")\\n  ```\\n\\n##### 4.3 Error Handling\\n- **Logging:**\\n  ```python\\n  import logging\\n\\n  logging.basicConfig(filename=\\'employee_data_transfer.log\\', level=logging.INFO)\\n\\n  try:\\n      # File transfer logic\\n      pass\\n  except Exception as e:\\n      logging.error(f\"Error during file transfer: {e}\")\\n      # Send alert to support team\\n      send_alert(f\"Error during file transfer: {e}\")\\n  ```\\n\\n- **Alerting:**\\n  ```python\\n  def send_alert(message):\\n      # Implement the logic to send an alert (e.g., email or messaging service)\\n      pass\\n  ```\\n\\n##### 4.4 Scheduling\\n- **Cron Job:**\\n  - Create a cron job to run the PySpark script daily at 2 AM.\\n  - Example cron job entry:\\n    ```bash\\n    0 2 * * * /path/to/your/script.py >> /path/to/logfile.log 2>&1\\n    ```\\n\\n#### 5. Testing\\n- **Unit Tests:**\\n  - Verify that the file is correctly read and written.\\n  - Ensure that the file contents are not corrupted during the transfer.\\n\\n- **Integration Tests:**\\n  - Test the entire process to ensure the file is moved from the source to the destination directory.\\n  - Verify that the file is transferred daily at the specified time.\\n\\n#### 6. Documentation\\n- **Process Documentation:**\\n  - Document the steps involved in the process.\\n  - Include details on how to run the script and configure the cron job.\\n\\n- **Configuration Documentation:**\\n  - Document the configuration settings for PySpark and the cron job.\\n  - Include any necessary environment variables and dependencies.\\n\\n#### 7. Conclusion\\nThis technical design document provides a comprehensive approach to automating the transfer of employee data files using PySpark. The design ensures data integrity, error handling, and automated scheduling, meeting the specified requirements.\\n\\n---\\n\\n### Example Script\\nHere is an example script that implements the above design:\\n\\n```python\\nimport os\\nimport logging\\nfrom pyspark.sql import SparkSession\\n\\n# Set up logging\\nlogging.basicConfig(filename=\\'employee_data_transfer.log\\', level=logging.INFO)\\n\\n# Initialize Spark session\\nspark = SparkSession.builder.appName(\"EmployeeDataTransfer\").getOrCreate()\\n\\n# Define directories\\nsource_dir = \"/data/employee/source\"\\ndest_dir = \"/data/employee/destination\"\\n\\ndef send_alert(message):\\n    # Implement the logic to send an alert (e.g., email or messaging service)\\n    pass\\n\\ntry:\\n    # Read the file\\n    df = spark.read.csv(f\"{source_dir}/employee_data_*.csv\", header=True, inferSchema=True)\\n    \\n    # Write the file\\n    df.write.csv(dest_dir, header=True, mode=\"overwrite\")\\n    \\n    logging.info(\"File transfer completed successfully.\")\\nexcept Exception as e:\\n    logging.error(f\"Error during file transfer: {e}\")\\n    send_alert(f\"Error during file transfer: {e}\")\\n\\n# Stop Spark session\\nspark.stop()\\n```\\n\\n### Cron Job Configuration\\nAdd the following cron job entry to run the script daily at 2 AM:\\n\\n```bash\\n0 2 * * * /usr/bin/python3 /path/to/your/script.py >> /path/to/logfile.log 2>&1\\n```\\n\\nThis ensures the script runs as scheduled, and logs are recorded for monitoring and troubleshooting.', 'code_content': 'I\\'m glad you think the document looks good! If you need any further refinements or additional details, feel free to let me know. Here is a consolidated version of the code and documentation based on the provided functional and technical design documents:\\n\\n### Functional Design Document: Transfer Employee Data File Using PySpark\\n\\n#### 1. Introduction\\nThis document outlines the design for the automated transfer of an employee data file from a source directory to a destination directory using PySpark. The process is designed to be fully automated, ensuring data integrity and providing error handling and logging.\\n\\n#### 2. Scope\\n- **Source Directory:** `/data/employee/source`\\n- **Destination Directory:** `/data/employee/destination`\\n- **File Naming Convention:** `employee_data_*.csv`\\n- **Execution Frequency:** Daily at 2 AM\\n\\n#### 3. Functional Requirements\\n\\n##### 3.1 File Identification\\n- The system must identify files in the source directory that match the naming convention `employee_data_*.csv`.\\n\\n##### 3.2 Data Transfer\\n- The system must read the identified file from the source directory and transfer it to the destination directory.\\n- The file transfer should preserve the original file\\'s content without any loss or corruption.\\n\\n##### 3.3 Automation\\n- The transfer process must be fully automated and executed without manual intervention.\\n- The process should be scheduled to run daily at 2 AM.\\n\\n##### 3.4 Error Handling\\n- The system must log any errors that occur during the transfer process.\\n- If an error occurs, the system must send an alert to the support team.\\n\\n##### 3.5 Testing\\n- The functionality must be tested to ensure that the file is correctly moved from the source to the destination directory.\\n\\n##### 3.6 Documentation\\n- Detailed documentation of the process and configurations must be provided for future reference.\\n\\n#### 4. Detailed Design\\n\\n##### 4.1 File Identification\\n- **Logic:** Use PySpark to list files in the `/data/employee/source` directory and filter by the naming convention `employee_data_*.csv`.\\n- **Example Code:**\\n  ```python\\n  from pyspark.sql import SparkSession\\n  import os\\n  import glob\\n\\n  spark = SparkSession.builder.appName(\"EmployeeDataTransfer\").getOrCreate()\\n\\n  source_dir = \"/data/employee/source\"\\n  file_pattern = \"employee_data_*.csv\"\\n\\n  # List files in the source directory\\n  valid_files = glob.glob(os.path.join(source_dir, file_pattern))\\n  ```\\n\\n##### 4.2 Data Transfer\\n- **Logic:** For each valid file, read the file content and write it to the destination directory.\\n- **Example Code:**\\n  ```python\\n  from pyspark.sql import DataFrame\\n\\n  dest_dir = \"/data/employee/destination\"\\n\\n  for file in valid_files:\\n      df = spark.read.csv(file, header=True, inferSchema=True)\\n      destination_path = os.path.join(dest_dir, os.path.basename(file))\\n      df.write.csv(destination_path, header=True, mode=\"overwrite\")\\n  ```\\n\\n##### 4.3 Automation\\n- **Logic:** Schedule the PySpark script using a scheduler (e.g., Apache Airflow, cron job).\\n- **Example Cron Job:**\\n  ```bash\\n  0 2 * * * /usr/bin/python3 /path/to/your/transfer_script.py >> /path/to/logfile.log 2>&1\\n  ```\\n\\n##### 4.4 Error Handling\\n- **Logic:** Log any errors that occur during the transfer process and send an alert to the support team.\\n- **Example Code:**\\n  ```python\\n  import logging\\n\\n  logging.basicConfig(filename=\\'/path/to/logfile.log\\', level=logging.ERROR)\\n\\n  def send_alert(message):\\n      # Implement the logic to send an alert (e.g., email or messaging service)\\n      pass\\n\\n  try:\\n      # Transfer logic\\n      source_dir = \"/data/employee/source\"\\n      dest_dir = \"/data/employee/destination\"\\n      file_pattern = \"employee_data_*.csv\"\\n\\n      valid_files = glob.glob(os.path.join(source_dir, file_pattern))\\n\\n      for file in valid_files:\\n          df = spark.read.csv(file, header=True, inferSchema=True)\\n          destination_path = os.path.join(dest_dir, os.path.basename(file))\\n          df.write.csv(destination_path, header=True, mode=\"overwrite\")\\n      \\n      logging.info(\"File transfer completed successfully.\")\\n  except Exception as e:\\n      logging.error(f\"Error during file transfer: {str(e)}\")\\n      send_alert(f\"Error during file transfer: {str(e)}\")\\n\\n  # Stop Spark session\\n  spark.stop()\\n  ```\\n\\n#### 5. Testing\\n- **Test Cases:**\\n  - Verify that the file is correctly identified and transferred.\\n  - Verify that the file content is preserved.\\n  - Verify that the process runs without errors.\\n  - Verify that error handling and logging work as expected.\\n- **Test Data:**\\n  - Sample `employee_data_*.csv` file in the source directory.\\n  - Ensure the destination directory exists and has the necessary write permissions.\\n\\n#### 6. Documentation\\n- **Process and Configuration:**\\n  - Detailed steps and configurations for setting up the PySpark script and scheduler.\\n  - Instructions for maintaining and updating the process.\\n  - Sample code and configuration files.\\n- **Support Contact Information:**\\n  - Contact details for the support team to receive alerts.\\n\\n#### 7. Conclusion\\nThis design document covers the complete workflow for transferring employee data files from a source directory to a destination directory using PySpark. The process includes file identification, data transfer, automation, error handling, and testing, ensuring that the data is processed reliably and efficiently.\\n\\n### Example Script\\n\\nHere is the complete example script:\\n\\n```python\\nimport os\\nimport glob\\nimport logging\\nfrom pyspark.sql import SparkSession\\n\\n# Set up logging\\nlogging.basicConfig(filename=\\'/path/to/logfile.log\\', level=logging.ERROR)\\n\\ndef send_alert(message):\\n    # Implement the logic to send an alert (e.g., email or messaging service)\\n    pass\\n\\ndef transfer_files():\\n    spark = SparkSession.builder.appName(\\'EmployeeDataTransfer\\').getOrCreate()\\n\\n    source_dir = \\'/data/employee/source\\'\\n    dest_dir = \\'/data/employee/destination\\'\\n    file_pattern = \\'employee_data_*.csv\\'\\n\\n    try:\\n        # List files in the source directory\\n        valid_files = glob.glob(os.path.join(source_dir, file_pattern))\\n\\n        for file in valid_files:\\n            df = spark.read.csv(file, header=True, inferSchema=True)\\n            destination_path = os.path.join(dest_dir, os.path.basename(file))\\n            df.write.csv(destination_path, header=True, mode=\"overwrite\")\\n        \\n        logging.info(\"File transfer completed successfully.\")\\n    except Exception as e:\\n        logging.error(f\"Error during file transfer: {str(e)}\")\\n        send_alert(f\"Error during file transfer: {str(e)}\")\\n\\n    spark.stop()\\n\\nif __name__ == \"__main__\":\\n    transfer_files()\\n```\\n\\n### Cron Job Configuration\\nAdd the following cron job entry to run the script daily at 2 AM:\\n\\n```bash\\n0 2 * * * /usr/bin/python3 /path/to/your/transfer_script.py >> /path/to/logfile.log 2>&1\\n```\\n\\n### Summary\\nThis document and script cover the complete workflow for transferring employee data files from a source directory to a destination directory using PySpark. The process includes file identification, data transfer, automation, error handling, and testing, ensuring that the data is processed reliably and efficiently.\\n\\nIf you need any further enhancements or additional details, please let me know!'}\n"
     ]
    }
   ],
   "source": [
    "for event in graph.stream(None,thread,stream_mode=\"values\"):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Here's a user story based on the requirement to transfer employee data files from one directory to another using PySpark:\n",
      "\n",
      "---\n",
      "\n",
      "**As a** data administrator,  \n",
      "**I want to** transfer employee data files from the source directory to the destination directory using PySpark,  \n",
      "**so that** the data can be processed and analyzed in the correct environment efficiently.\n",
      "\n",
      "**Acceptance Criteria:**\n",
      "\n",
      "1. **Data Integrity**: The transferred files must be an exact copy of the original files, ensuring no data loss or corruption.\n",
      "2. **File Identification**: The system should be able to identify all files related to employee data in the source directory.\n",
      "3. **Job Logging**: The PySpark job should log the start and end time of the transfer process, along with any errors encountered.\n",
      "4. **Error Handling**: If a file fails to transfer, the system should log the error and continue processing other files.\n",
      "5. **Performance**: The transfer should complete within a reasonable time frame, given the size of the dataset.\n",
      "6. **Security**: The transfer process should comply with the companys data security policies, ensuring data is not exposed during transfer.\n",
      "7. **Automation**: The process should be automated to run on a scheduled basis (e.g., nightly) and should be triggered manually when needed.\n",
      "8. **Validation**: After the transfer is complete, the system should validate that all intended files have been successfully moved to the destination directory.\n",
      "\n",
      "---\n",
      "\n",
      "This user story and its acceptance criteria should help clarify the scope and requirements for developing the PySpark job to transfer the employee data files.\n",
      "Great! I'm glad you found the user story and acceptance criteria clear. Let's proceed with creating a functional design document based on these user stories and criteria. Here's a detailed functional design document:\n",
      "\n",
      "---\n",
      "\n",
      "# Functional Design Document: Employee Data File Transfer Using PySpark\n",
      "\n",
      "## 1. Introduction\n",
      "\n",
      "This document outlines the functional design for transferring employee data files from a source directory to a destination directory using PySpark. The goal is to ensure data integrity, efficient processing, and compliance with security policies.\n",
      "\n",
      "## 2. Scope\n",
      "\n",
      "- **Source Directory**: Contains raw employee data files.\n",
      "- **Destination Directory**: Target location for the processed data files.\n",
      "- **Tool**: PySpark for data processing and transfer.\n",
      "\n",
      "## 3. Functional Requirements\n",
      "\n",
      "### 3.1 Data Integrity\n",
      "- Ensure that the transferred files are exact copies of the original files.\n",
      "- Verify that no data is lost or corrupted during the transfer process.\n",
      "\n",
      "### 3.2 File Identification\n",
      "- Identify all files related to employee data in the source directory.\n",
      "- Ensure that only relevant files are processed and transferred.\n",
      "\n",
      "### 3.3 Job Logging\n",
      "- Log the start and end time of the transfer process.\n",
      "- Log any errors encountered during the transfer process.\n",
      "\n",
      "### 3.4 Error Handling\n",
      "- Log errors if a file fails to transfer.\n",
      "- Continue processing other files even if one file fails.\n",
      "\n",
      "### 3.5 Performance\n",
      "- Ensure the transfer process completes within a reasonable time frame, considering the size of the dataset.\n",
      "- Optimize the process to handle large volumes of data efficiently.\n",
      "\n",
      "### 3.6 Security\n",
      "- Ensure that the transfer process complies with the companys data security policies.\n",
      "- Secure data during the transfer process to prevent unauthorized access.\n",
      "\n",
      "### 3.7 Automation\n",
      "- Automate the process to run on a scheduled basis (e.g., nightly).\n",
      "- Allow manual triggering of the process when needed.\n",
      "\n",
      "### 3.8 Validation\n",
      "- Validate that all intended files have been successfully moved to the destination directory after the transfer is complete.\n",
      "\n",
      "## 4. Technical Design\n",
      "\n",
      "### 4.1 Data Identification and Transfer\n",
      "- Use PySpark to read files from the source directory.\n",
      "- Filter and identify files related to employee data.\n",
      "- Transfer the identified files to the destination directory.\n",
      "\n",
      "### 4.2 Logging and Monitoring\n",
      "- Implement logging using PySparks built-in logging capabilities.\n",
      "- Log the start and end times of the job.\n",
      "- Log any errors encountered during the process.\n",
      "\n",
      "### 4.3 Error Handling\n",
      "- Implement error handling to log and skip files that fail to transfer.\n",
      "- Ensure the process continues for other files even if some files fail.\n",
      "\n",
      "### 4.4 Performance Optimization\n",
      "- Use PySparks parallel processing capabilities to handle large datasets efficiently.\n",
      "- Optimize file reading and writing operations to minimize processing time.\n",
      "\n",
      "### 4.5 Security\n",
      "- Ensure that the transfer process complies with company security policies.\n",
      "- Use secure file transfer protocols and secure credentials for authentication.\n",
      "\n",
      "### 4.6 Automation\n",
      "- Schedule the PySpark job to run nightly using a job scheduler like Airflow or cron.\n",
      "- Implement a manual trigger option to run the job on-demand.\n",
      "\n",
      "### 4.7 Validation\n",
      "- After the transfer, validate the files in the destination directory against the source directory to ensure all files have been successfully moved.\n",
      "- Generate a report summarizing the transfer process and any issues encountered.\n",
      "\n",
      "## 5. Implementation Steps\n",
      "\n",
      "### 5.1 Step 1: Set Up Environment\n",
      "- Install PySpark and any necessary dependencies.\n",
      "- Configure the PySpark environment to connect to the source and destination directories.\n",
      "\n",
      "### 5.2 Step 2: Identify and Transfer Files\n",
      "- Write a PySpark script to read files from the source directory.\n",
      "- Filter the files to identify those related to employee data.\n",
      "- Transfer the identified files to the destination directory.\n",
      "\n",
      "### 5.3 Step 3: Logging and Monitoring\n",
      "- Implement logging using PySparks logging capabilities.\n",
      "- Log the start and end times of the job.\n",
      "- Log any errors encountered during the transfer process.\n",
      "\n",
      "### 5.4 Step 4: Error Handling\n",
      "- Implement error handling to log and skip files that fail to transfer.\n",
      "- Ensure the process continues for other files even if some files fail.\n",
      "\n",
      "### 5.5 Step 5: Performance Optimization\n",
      "- Use PySparks parallel processing capabilities to handle large datasets efficiently.\n",
      "- Optimize file reading and writing operations to minimize processing time.\n",
      "\n",
      "### 5.6 Step 6: Security\n",
      "- Ensure that the transfer process complies with company security policies.\n",
      "- Use secure file transfer protocols and secure credentials for authentication.\n",
      "\n",
      "### 5.7 Step 7: Automation\n",
      "- Schedule the PySpark job to run nightly using a job scheduler like Airflow or cron.\n",
      "- Implement a manual trigger option to run the job on-demand.\n",
      "\n",
      "### 5.8 Step 8: Validation\n",
      "- After the transfer, validate the files in the destination directory against the source directory to ensure all files have been successfully moved.\n",
      "- Generate a report summarizing the transfer process and any issues encountered.\n",
      "\n",
      "## 6. User Interface\n",
      "\n",
      "- **Manual Trigger**: Provide a simple UI or command-line interface to manually trigger the transfer process.\n",
      "- **Job Status**: Provide a mechanism to check the status of the transfer job, including start time, end time, and any errors.\n",
      "\n",
      "## 7. Data Flow Diagram\n",
      "\n",
      "1. **Source Directory**: Contains raw employee data files.\n",
      "2. **PySpark Job**:\n",
      "   - Reads files from the source directory.\n",
      "   - Filters and identifies employee data files.\n",
      "   - Transfers identified files to the destination directory.\n",
      "3. **Destination Directory**: Stores the transferred employee data files.\n",
      "4. **Logging and Monitoring**: Logs the start and end times, and any errors.\n",
      "5. **Validation**: Validates the transferred files and generates a summary report.\n",
      "\n",
      "## 8. Data Schema\n",
      "\n",
      "- **Source Directory**: Files in CSV, JSON, or Parquet format.\n",
      "- **Destination Directory**: Files in the same format as the source files.\n",
      "- **Log Files**: Contains log entries with timestamps, file names, and error messages if any.\n",
      "\n",
      "## 9. Implementation Details\n",
      "\n",
      "### 9.1 PySpark Script\n",
      "\n",
      "```python\n",
      "from pyspark.sql import SparkSession\n",
      "import os\n",
      "import logging\n",
      "\n",
      "# Initialize Spark session\n",
      "spark = SparkSession.builder.appName(\"EmployeeDataTransfer\").getOrCreate()\n",
      "\n",
      "# Set up logging\n",
      "logging.basicConfig(filename='transfer_log.log', level=logging.INFO)\n",
      "\n",
      "def transfer_files(source_dir, dest_dir):\n",
      "    try:\n",
      "        logging.info(f\"Job started at {datetime.now()}\")\n",
      "        \n",
      "        # List all files in the source directory\n",
      "        files = os.listdir(source_dir)\n",
      "        \n",
      "        for file in files:\n",
      "            if file.endswith(\".csv\") or file.endswith(\".json\") or file.endswith(\".parquet\"):\n",
      "                source_path = os.path.join(source_dir, file)\n",
      "                dest_path = os.path.join(dest_dir, file)\n",
      "                \n",
      "                # Read file using PySpark\n",
      "                if file.endswith(\".csv\"):\n",
      "                    df = spark.read.csv(source_path, header=True, inferSchema=True)\n",
      "                elif file.endswith(\".json\"):\n",
      "                    df = spark.read.json(source_path)\n",
      "                elif file.endswith(\".parquet\"):\n",
      "                    df = spark.read.parquet(source_path)\n",
      "                \n",
      "                # Write file to destination directory\n",
      "                df.write.mode(\"overwrite\").save(dest_path)\n",
      "                \n",
      "                logging.info(f\"File {file} transferred successfully to {dest_path}\")\n",
      "            else:\n",
      "                logging.info(f\"File {file} is not an employee data file and will not be transferred.\")\n",
      "                \n",
      "        logging.info(f\"Job completed at {datetime.now()}\")\n",
      "    except Exception as e:\n",
      "        logging.error(f\"Error occurred: {str(e)}\")\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    source_directory = \"/path/to/source_directory\"\n",
      "    destination_directory = \"/path/to/destination_directory\"\n",
      "    \n",
      "    transfer_files(source_directory, destination_directory)\n",
      "```\n",
      "\n",
      "### 9.2 Scheduling and Automation\n",
      "\n",
      "- **Cron Job**: Schedule the PySpark job using a cron job to run nightly.\n",
      "- **Airflow**: Alternatively, set up the job in Apache Airflow to run nightly and provide a manual trigger option.\n",
      "\n",
      "### 9.3 Validation\n",
      "\n",
      "- **Validation Script**: After the transfer, validate the transferred files by comparing the file list and content in the source and destination directories.\n",
      "- **Summary Report**: Generate a summary report that includes the start and end times, files transferred, and any errors encountered.\n",
      "\n",
      "## 10. Security\n",
      "\n",
      "- **Secure Credentials**: Use secure credentials for accessing the source and destination directories.\n",
      "- **Data Encryption**: Ensure data is encrypted during transfer if required by company policy.\n",
      "\n",
      "## 11. User Interface\n",
      "\n",
      "- **Manual Trigger**: Provide a simple command-line interface or a web-based interface to manually trigger the transfer process.\n",
      "- **Job Status**: Provide a mechanism to check the status of the transfer job, including start time, end time, and any errors.\n",
      "\n",
      "## 12. Conclusion\n",
      "\n",
      "This functional design document provides a comprehensive approach to transferring employee data files from a source directory to a destination directory using PySpark. The design ensures data integrity, efficient processing, and compliance with security policies.\n",
      "\n",
      "---\n",
      "\n",
      "Feel free to adjust any details as needed to fit your specific environment and requirements.\n",
      "Great! Let's proceed with creating a technical design document based on the user story and acceptance criteria you provided. Here's a detailed technical design document:\n",
      "\n",
      "---\n",
      "\n",
      "# Technical Design Document\n",
      "\n",
      "## 1. Introduction\n",
      "\n",
      "This document outlines the technical design for a PySpark job to transfer employee data files from a source directory to a destination directory. The design ensures that the data is transferred securely, efficiently, and with high integrity.\n",
      "\n",
      "## 2. Scope\n",
      "\n",
      "The scope of this project includes:\n",
      "- Identifying and transferring employee data files from the source directory to the destination directory.\n",
      "- Ensuring data integrity and security during the transfer process.\n",
      "- Logging the job execution details and handling errors.\n",
      "- Automating the job to run on a scheduled basis and allowing manual triggering.\n",
      "\n",
      "## 3. Requirements\n",
      "\n",
      "### 3.1 Data Integrity\n",
      "- The transferred files must be an exact copy of the original files, ensuring no data loss or corruption.\n",
      "\n",
      "### 3.2 File Identification\n",
      "- The system should be able to identify all files related to employee data in the source directory.\n",
      "\n",
      "### 3.3 Job Logging\n",
      "- The PySpark job should log the start and end time of the transfer process, along with any errors encountered.\n",
      "\n",
      "### 3.4 Error Handling\n",
      "- If a file fails to transfer, the system should log the error and continue processing other files.\n",
      "\n",
      "### 3.5 Performance\n",
      "- The transfer should complete within a reasonable time frame, given the size of the dataset.\n",
      "\n",
      "### 3.6 Security\n",
      "- The transfer process should comply with the companys data security policies, ensuring data is not exposed during transfer.\n",
      "\n",
      "### 3.7 Automation\n",
      "- The process should be automated to run on a scheduled basis (e.g., nightly) and should be triggered manually when needed.\n",
      "\n",
      "### 3.8 Validation\n",
      "- After the transfer is complete, the system should validate that all intended files have been successfully moved to the destination directory.\n",
      "\n",
      "## 4. System Architecture\n",
      "\n",
      "### 4.1 Components\n",
      "1. **Source Directory**: Contains the original employee data files.\n",
      "2. **Destination Directory**: The target location for the transferred files.\n",
      "3. **PySpark Job**: The job responsible for identifying, transferring, and validating the files.\n",
      "4. **Logging System**: Records the job execution details and errors.\n",
      "5. **Scheduler**: Automates the job execution based on a schedule.\n",
      "\n",
      "### 4.2 Workflow\n",
      "1. **Initialization**: The PySpark job starts and logs the start time.\n",
      "2. **File Identification**: The job identifies all relevant employee data files in the source directory.\n",
      "3. **Data Transfer**: The job transfers the identified files to the destination directory.\n",
      "4. **Error Handling**: The job logs any errors encountered during the transfer and continues processing other files.\n",
      "5. **Validation**: The job validates that all files have been successfully transferred.\n",
      "6. **Logging**: The job logs the end time and any errors encountered.\n",
      "7. **Automation**: The job is scheduled to run nightly and can be manually triggered as needed.\n",
      "\n",
      "## 5. Implementation Details\n",
      "\n",
      "### 5.1 PySpark Job\n",
      "\n",
      "#### 5.1.1 File Identification\n",
      "- Use `os` or `glob` library to list all files in the source directory.\n",
      "- Filter the files based on a specific pattern or naming convention for employee data files.\n",
      "\n",
      "#### 5.1.2 Data Transfer\n",
      "- Use PySpark's `pyspark.sql.DataFrameReader` to read the files.\n",
      "- Use PySpark's `DataFrameWriter` to write the files to the destination directory.\n",
      "\n",
      "#### 5.1.3 Error Handling\n",
      "- Implement try-except blocks to catch and log any errors during the transfer process.\n",
      "\n",
      "#### 5.1.4 Logging\n",
      "- Use PySpark's logging mechanism to log job start and end times, along with any errors.\n",
      "\n",
      "#### 5.1.5 Validation\n",
      "- Compare the list of files in the destination directory with the list of files that were supposed to be transferred.\n",
      "\n",
      "### 5.2 Automation and Scheduling\n",
      "- Use a scheduler like `cron` or Apache Airflow to automate the job execution.\n",
      "- Implement a manual trigger mechanism using a web API or command-line interface.\n",
      "\n",
      "### 5.3 Security\n",
      "- Ensure that the transfer process complies with the companys data security policies.\n",
      "- Use secure protocols (e.g., SFTP, HTTPS) for transferring files if they are not on the same local system.\n",
      "\n",
      "## 6. Technical Details\n",
      "\n",
      "### 6.1 Environment Setup\n",
      "- **Python Version**: Python 3.8 or higher\n",
      "- **PySpark Version**: PySpark 3.x\n",
      "- **Dependencies**: `pyspark`, `os`, `logging`, `glob`\n",
      "\n",
      "### 6.2 Code Structure\n",
      "\n",
      "#### 6.2.1 File Identification\n",
      "```python\n",
      "import os\n",
      "import glob\n",
      "\n",
      "def identify_files(source_dir):\n",
      "    return glob.glob(os.path.join(source_dir, \"employee_data*.csv\"))\n",
      "```\n",
      "\n",
      "#### 6.2.2 Data Transfer\n",
      "```python\n",
      "from pyspark.sql import SparkSession\n",
      "\n",
      "def transfer_files(spark, source_files, dest_dir):\n",
      "    for file in source_files:\n",
      "        df = spark.read.csv(file, header=True, inferSchema=True)\n",
      "        dest_file = os.path.join(dest_dir, os.path.basename(file))\n",
      "        df.write.csv(dest_file, header=True)\n",
      "```\n",
      "\n",
      "#### 6.2.3 Error Handling and Logging\n",
      "```python\n",
      "import logging\n",
      "\n",
      "def setup_logging():\n",
      "    logging.basicConfig(level=logging.INFO)\n",
      "    logger = logging.getLogger(__name__)\n",
      "    return logger\n",
      "\n",
      "def transfer_with_logging(spark, source_files, dest_dir, logger):\n",
      "    for file in source_files:\n",
      "        try:\n",
      "            df = spark.read.csv(file, header=True, inferSchema=True)\n",
      "            dest_file = os.path.join(dest_dir, os.path.basename(file))\n",
      "            df.write.csv(dest_file, header=True)\n",
      "            logger.info(f\"File {file} transferred successfully.\")\n",
      "        except Exception as e:\n",
      "            logger.error(f\"Error transferring file {file}: {str(e)}\")\n",
      "```\n",
      "\n",
      "#### 6.2.4 Validation\n",
      "```python\n",
      "def validate_transfer(source_files, dest_dir):\n",
      "    source_file_names = set(os.path.basename(file) for file in source_files)\n",
      "    dest_file_names = set(os.path.basename(file) for file in glob.glob(os.path.join(dest_dir, \"employee_data*.csv\")))\n",
      "    \n",
      "    if source_file_names == dest_file_names:\n",
      "        logging.info(\"All files have been successfully transferred.\")\n",
      "    else:\n",
      "        missing_files = source_file_names - dest_file_names\n",
      "        logging.error(f\"Files not transferred: {missing_files}\")\n",
      "```\n",
      "\n",
      "### 6.3 Automation\n",
      "- **Scheduler**: Use Apache Airflow or a cron job to schedule the PySpark job.\n",
      "- **Manual Trigger**: Implement a simple API endpoint using Flask or FastAPI to allow manual triggering of the job.\n",
      "\n",
      "### 6.4 Security\n",
      "- Ensure the transfer process uses secure protocols and adheres to company security policies.\n",
      "- Use encrypted connections if the files are transferred over a network.\n",
      "\n",
      "## 7. Testing\n",
      "\n",
      "### 7.1 Unit Tests\n",
      "- Write unit tests to ensure that the file identification, data transfer, and validation functions work as expected.\n",
      "\n",
      "### 7.2 Integration Tests\n",
      "- Test the entire workflow to ensure that the job runs successfully from start to finish.\n",
      "- Validate that the job logs are generated correctly and that errors are handled gracefully.\n",
      "\n",
      "## 8. Deployment\n",
      "\n",
      "### 8.1 Deployment Environment\n",
      "- The PySpark job will run on a cluster managed by Apache Spark.\n",
      "- The job will be scheduled to run nightly and can be manually triggered when needed.\n",
      "\n",
      "### 8.2 Deployment Steps\n",
      "1. **Prepare the Environment**: Ensure the environment has the required Python version, PySpark, and other dependencies.\n",
      "2. **Deploy the Job**: Package the PySpark job as a script or JAR file.\n",
      "3. **Schedule the Job**: Use Apache Airflow or cron to schedule the job to run nightly.\n",
      "4. **Manual Trigger**: Implement a simple API or command-line interface for manual triggering.\n",
      "\n",
      "## 9. Monitoring and Maintenance\n",
      "\n",
      "### 9.1 Monitoring\n",
      "- Use monitoring tools like Apache Spark's built-in monitoring or third-party tools like Prometheus and Grafana to monitor job execution and performance.\n",
      "- Regularly check logs for any errors or issues.\n",
      "\n",
      "### 9.2 Maintenance\n",
      "- Regularly review and update the job to handle any new requirements or security updates.\n",
      "- Maintain the logging system to ensure all relevant information is captured.\n",
      "\n",
      "---\n",
      "\n",
      "This technical design document provides a comprehensive plan for developing and implementing the PySpark job to transfer employee data files. If you have any additional requirements or questions, please let me know!\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'code_content'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[600], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(state_values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunc_document\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(state_values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtech_document\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mstate_values\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcode_content\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m) \n",
      "\u001b[1;31mKeyError\u001b[0m: 'code_content'"
     ]
    }
   ],
   "source": [
    "state=graph.get_state(thread)\n",
    "state_values = state.values  \n",
    "print(state_values[\"user_stories\"])  \n",
    "print(state_values[\"func_document\"]) \n",
    "print(state_values[\"tech_document\"])\n",
    "print(state_values[\"code_content\"])\n",
    "print(state_values[\"code_content\"])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langgraph in c:\\agenticai\\venv1\\lib\\site-packages (0.2.73)\n",
      "Requirement already satisfied: langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43 in c:\\agenticai\\venv1\\lib\\site-packages (from langgraph) (0.3.40)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.10 in c:\\agenticai\\venv1\\lib\\site-packages (from langgraph) (2.0.15)\n",
      "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in c:\\agenticai\\venv1\\lib\\site-packages (from langgraph) (0.1.51)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\agenticai\\venv1\\lib\\site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (0.3.4)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\agenticai\\venv1\\lib\\site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\agenticai\\venv1\\lib\\site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\agenticai\\venv1\\lib\\site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\agenticai\\venv1\\lib\\site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\agenticai\\venv1\\lib\\site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (4.12.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in c:\\agenticai\\venv1\\lib\\site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (2.10.6)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in c:\\agenticai\\venv1\\lib\\site-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph) (1.1.0)\n",
      "Requirement already satisfied: httpx>=0.25.2 in c:\\agenticai\\venv1\\lib\\site-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in c:\\agenticai\\venv1\\lib\\site-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.15)\n",
      "Requirement already satisfied: anyio in c:\\agenticai\\venv1\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (4.8.0)\n",
      "Requirement already satisfied: certifi in c:\\agenticai\\venv1\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\agenticai\\venv1\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\agenticai\\venv1\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\agenticai\\venv1\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\agenticai\\venv1\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (3.0.0)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\agenticai\\venv1\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\agenticai\\venv1\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\agenticai\\venv1\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\agenticai\\venv1\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\agenticai\\venv1\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\agenticai\\venv1\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\agenticai\\venv1\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (2.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\agenticai\\venv1\\lib\\site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streamlit app is running! Check your browser.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import threading\n",
    "import webbrowser\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Save your Streamlit app to a temporary file\n",
    "streamlit_code = \"\"\"\n",
    "import streamlit as st\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.messages import HumanMessage\n",
    "from typing import Dict, Any\n",
    "import json\n",
    "\n",
    "# Initialize the graph (using your existing setup)\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(interrupt_before=[\"po_feedback\",\"design_feedback\",\"code_feedback\"], checkpointer=memory)\n",
    "\n",
    "def reset_session():\n",
    "    st.session_state.clear()\n",
    "    st.session_state.thread_id = \"1\"  # Default thread ID\n",
    "\n",
    "def display_state(state: Dict[str, Any]):\n",
    "    \\\"\\\"\\\"Display the current state in an organized way\\\"\\\"\\\"\n",
    "    with st.expander(\"Current State Overview\"):\n",
    "        if \"user_stories\" in state:\n",
    "            st.subheader(\"User Stories\")\n",
    "            st.text(state[\"user_stories\"])\n",
    "        \n",
    "        if \"func_document\" in state:\n",
    "            st.subheader(\"Functional Design\")\n",
    "            st.text(state[\"func_document\"])\n",
    "        \n",
    "        if \"tech_document\" in state:\n",
    "            st.subheader(\"Technical Design\")\n",
    "            st.text(state[\"tech_document\"])\n",
    "        \n",
    "        if \"code_content\" in state:\n",
    "            st.subheader(\"Generated Code\")\n",
    "            st.code(state[\"code_content\"], language='python')\n",
    "        \n",
    "        if \"test_cases_content\" in state:\n",
    "            st.subheader(\"Test Cases\")\n",
    "            st.text(state[\"test_cases_content\"])\n",
    "        \n",
    "        if \"qa_test_comments\" in state:\n",
    "            st.subheader(\"QA Test Results\")\n",
    "            st.text(state[\"qa_test_comments\"])\n",
    "\n",
    "def main():\n",
    "    st.title(\"Software Development Workflow Assistant\")\n",
    "    st.markdown(\\\"\\\"\\\"\n",
    "    This interactive tool guides you through the complete software development lifecycle:\n",
    "    1. Requirements  User Stories\n",
    "    2. Design Documents\n",
    "    3. Code Implementation\n",
    "    4. Security Review\n",
    "    5. Testing\n",
    "    \\\"\\\"\\\")\n",
    "\n",
    "    # Initialize session state\n",
    "    if 'thread_id' not in st.session_state:\n",
    "        reset_session()\n",
    "    \n",
    "    # Sidebar controls\n",
    "    with st.sidebar:\n",
    "        st.header(\"Controls\")\n",
    "        st.text_input(\"Thread ID\", key=\"thread_id\")\n",
    "        if st.button(\"New Session\"):\n",
    "            reset_session()\n",
    "        if st.button(\"Show Full State\"):\n",
    "            state = graph.get_state({\"configurable\": {\"thread_id\": st.session_state.thread_id}})\n",
    "            st.json(state.values)\n",
    "    \n",
    "    # Main interface\n",
    "    tab1, tab2 = st.tabs([\"Workflow\", \"Configuration\"])\n",
    "    \n",
    "    with tab1:\n",
    "        # Step 1: Initial input\n",
    "        if 'initial_input' not in st.session_state:\n",
    "            with st.form(\"initial_input_form\"):\n",
    "                st.subheader(\"Enter Requirements\")\n",
    "                requirements = st.text_area(\"Describe your software requirements:\", \n",
    "                                         \"The requirement is to transfer employee data file from one directory to another using pyspark\")\n",
    "                \n",
    "                if st.form_submit_button(\"Start Workflow\"):\n",
    "                    initial_input = {\n",
    "                        \"messages\": MessagesState(messages=[HumanMessage(content=requirements)])\n",
    "                    }\n",
    "                    thread = {\"configurable\": {\"thread_id\": st.session_state.thread_id}}\n",
    "                    \n",
    "                    # Run initial step\n",
    "                    for event in graph.stream(initial_input, thread, stream_mode=\"values\"):\n",
    "                        st.session_state.current_state = event\n",
    "                    \n",
    "                    st.session_state.initial_input = True\n",
    "                    st.session_state.current_step = \"po_feedback\"\n",
    "                    st.rerun()\n",
    "        \n",
    "        # Subsequent steps\n",
    "        elif 'current_step' in st.session_state:\n",
    "            current_step = st.session_state.current_step\n",
    "            state = graph.get_state({\"configurable\": {\"thread_id\": st.session_state.thread_id}})\n",
    "            \n",
    "            # Display current state\n",
    "            display_state(state.values)\n",
    "            \n",
    "            # Get human feedback for current step\n",
    "            with st.form(f\"{current_step}_feedback_form\"):\n",
    "                st.subheader(f\"Provide Feedback ({current_step.replace('_', ' ')})\")\n",
    "                \n",
    "                # Show appropriate prompt based on current step\n",
    "                if current_step == \"po_feedback\":\n",
    "                    st.markdown(\"Review the generated user stories above and provide feedback:\")\n",
    "                elif current_step == \"design_feedback\":\n",
    "                    st.markdown(\"Review the design documents above and provide feedback:\")\n",
    "                elif current_step == \"code_feedback\":\n",
    "                    st.markdown(\"Review the generated code above and provide feedback:\")\n",
    "                elif current_step == \"testcases_feedback\":\n",
    "                    st.markdown(\"Review the test cases above and provide feedback:\")\n",
    "                \n",
    "                feedback = st.text_area(\"Your feedback:\", \"It looks good\")\n",
    "                \n",
    "                if st.form_submit_button(\"Submit Feedback\"):\n",
    "                    # Update state with new feedback\n",
    "                    current_messages = state.values[\"messages\"]\n",
    "                    new_messages = current_messages + [HumanMessage(content=feedback)]\n",
    "                    updated_messages = MessagesState(messages=new_messages)\n",
    "                    \n",
    "                    graph.update_state(\n",
    "                        {\"configurable\": {\"thread_id\": st.session_state.thread_id}},\n",
    "                        {\"messages\": updated_messages}\n",
    "                    )\n",
    "                    \n",
    "                    # Continue the workflow\n",
    "                    next_events = list(graph.stream(\n",
    "                        None,\n",
    "                        {\"configurable\": {\"thread_id\": st.session_state.thread_id}},\n",
    "                        stream_mode=\"values\"\n",
    "                    ))\n",
    "                    \n",
    "                    if next_events:\n",
    "                        st.session_state.current_state = next_events[-1]\n",
    "                        \n",
    "                        # Determine next step (simplified - in reality would check graph structure)\n",
    "                        if \"user_stories\" in next_events[-1]:\n",
    "                            st.session_state.current_step = \"po_feedback\"\n",
    "                        elif \"func_document\" in next_events[-1]:\n",
    "                            st.session_state.current_step = \"design_feedback\"\n",
    "                        elif \"code_content\" in next_events[-1]:\n",
    "                            st.session_state.current_step = \"code_feedback\"\n",
    "                        elif \"test_cases_content\" in next_events[-1]:\n",
    "                            st.session_state.current_step = \"testcases_feedback\"\n",
    "                        else:\n",
    "                            st.session_state.current_step = None\n",
    "                    \n",
    "                    st.rerun()\n",
    "    \n",
    "    with tab2:\n",
    "        st.subheader(\"Workflow Configuration\")\n",
    "        st.markdown(\\\"\\\"\\\"\n",
    "        ### Workflow Steps:\n",
    "        1. writeUserStories - Creates user stories from requirements\n",
    "        2. po_feedback - Product owner feedback on user stories\n",
    "        3. createFuncDesignDocument - Creates functional design\n",
    "        4. createTechDesignDocument - Creates technical design\n",
    "        5. design_feedback - Feedback on design documents\n",
    "        6. GenerateCode - Implements the code\n",
    "        7. code_feedback - Developer feedback on code\n",
    "        8. security_feedback - Security review\n",
    "        9. writeTestCases - Creates test cases\n",
    "        10. testcases_feedback - Feedback on test cases\n",
    "        11. executingTestCases - QA testing\n",
    "        12. qa_test_feedback - Final approval\n",
    "        \\\"\\\"\\\")\n",
    "        \n",
    "        st.image(graph.get_graph().draw_mermaid_png())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\"\"\"\n",
    "\n",
    "# Write the Streamlit code to a temp file\n",
    "with open(\"temp_streamlit_app.py\", \"w\") as f:\n",
    "    f.write(streamlit_code)\n",
    "\n",
    "# Function to run Streamlit in the background\n",
    "def run_streamlit():\n",
    "    subprocess.run([\"streamlit\", \"run\", \"temp_streamlit_app.py\"])\n",
    "\n",
    "# Launch Streamlit in a separate thread\n",
    "thread = threading.Thread(target=run_streamlit, daemon=True)\n",
    "thread.start()\n",
    "\n",
    "# Wait a moment for Streamlit to start\n",
    "time.sleep(2)\n",
    "\n",
    "# Open the Streamlit UI in a browser\n",
    "webbrowser.open(\"http://localhost:8501\")\n",
    "\n",
    "print(\"Streamlit app is running! Check your browser.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 22:22:26.617 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 22:22:26.620 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 22:22:26.624 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 22:22:26.625 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 22:22:26.626 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 22:22:26.627 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 22:22:26.629 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 22:22:26.629 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 22:22:26.631 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 22:22:26.631 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 22:22:26.632 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 22:22:26.633 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 22:22:26.636 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 22:22:26.641 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 22:22:26.646 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 22:22:26.647 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 22:22:26.648 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 22:22:26.649 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 22:22:26.651 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 22:22:26.651 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 22:22:26.652 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 22:22:26.652 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 22:22:26.653 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 22:22:26.654 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 22:22:26.655 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 22:22:26.655 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 22:22:26.656 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 22:22:26.658 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 22:22:26.659 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 22:22:26.660 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 22:22:26.661 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 22:22:26.662 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 22:22:26.662 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 22:22:26.663 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 22:22:26.663 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 22:22:26.665 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 22:22:26.666 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 22:22:26.666 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 22:22:26.667 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 22:22:26.667 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 22:22:26.668 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 22:22:26.669 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 22:22:26.669 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 22:22:26.670 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 22:22:26.670 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 22:22:26.671 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 22:22:26.671 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 22:22:26.672 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 22:22:26.672 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 22:22:27.119 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 22:22:28.339 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-24 22:22:28.341 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.messages import HumanMessage\n",
    "from typing import Dict, Any\n",
    "import json\n",
    "\n",
    "# Initialize the graph (using your existing setup)\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(interrupt_before=[\"po_feedback\",\"design_feedback\",\"code_feedback\"], checkpointer=memory)\n",
    "\n",
    "def reset_session():\n",
    "    st.session_state.clear()\n",
    "    st.session_state.thread_id = \"1\"  # Default thread ID\n",
    "\n",
    "def display_state(state: Dict[str, Any]):\n",
    "    \"\"\"Display the current state in an organized way\"\"\"\n",
    "    with st.expander(\"Current State Overview\"):\n",
    "        if \"user_stories\" in state:\n",
    "            st.subheader(\"User Stories\")\n",
    "            st.text(state[\"user_stories\"])\n",
    "        \n",
    "        if \"func_document\" in state:\n",
    "            st.subheader(\"Functional Design\")\n",
    "            st.text(state[\"func_document\"])\n",
    "        \n",
    "        if \"tech_document\" in state:\n",
    "            st.subheader(\"Technical Design\")\n",
    "            st.text(state[\"tech_document\"])\n",
    "        \n",
    "        if \"code_content\" in state:\n",
    "            st.subheader(\"Generated Code\")\n",
    "            st.code(state[\"code_content\"], language='python')\n",
    "        \n",
    "        if \"test_cases_content\" in state:\n",
    "            st.subheader(\"Test Cases\")\n",
    "            st.text(state[\"test_cases_content\"])\n",
    "        \n",
    "        if \"qa_test_comments\" in state:\n",
    "            st.subheader(\"QA Test Results\")\n",
    "            st.text(state[\"qa_test_comments\"])\n",
    "\n",
    "def main():\n",
    "    st.title(\"Software Development Workflow Assistant\")\n",
    "    st.markdown(\"\"\"\n",
    "    This interactive tool guides you through the complete software development lifecycle:\n",
    "    1. Requirements  User Stories\n",
    "    2. Design Documents\n",
    "    3. Code Implementation\n",
    "    4. Security Review\n",
    "    5. Testing\n",
    "    \"\"\")\n",
    "\n",
    "    # Initialize session state\n",
    "    if 'thread_id' not in st.session_state:\n",
    "        reset_session()\n",
    "    \n",
    "    # Sidebar controls\n",
    "    with st.sidebar:\n",
    "        st.header(\"Controls\")\n",
    "        st.text_input(\"Thread ID\", key=\"thread_id\")\n",
    "        if st.button(\"New Session\"):\n",
    "            reset_session()\n",
    "        if st.button(\"Show Full State\"):\n",
    "            state = graph.get_state({\"configurable\": {\"thread_id\": st.session_state.thread_id}})\n",
    "            st.json(state.values)\n",
    "    \n",
    "    # Main interface\n",
    "    tab1, tab2 = st.tabs([\"Workflow\", \"Configuration\"])\n",
    "    \n",
    "    with tab1:\n",
    "        # Step 1: Initial input\n",
    "        if 'initial_input' not in st.session_state:\n",
    "            with st.form(\"initial_input_form\"):\n",
    "                st.subheader(\"Enter Requirements\")\n",
    "                requirements = st.text_area(\"Describe your software requirements:\", \n",
    "                                         \"The requirement is to transfer employee data file from one directory to another using pyspark\")\n",
    "                \n",
    "                if st.form_submit_button(\"Start Workflow\"):\n",
    "                    initial_input = {\n",
    "                        \"messages\": MessagesState(messages=[HumanMessage(content=requirements)])\n",
    "                    }\n",
    "                    thread = {\"configurable\": {\"thread_id\": st.session_state.thread_id}}\n",
    "                    \n",
    "                    # Run initial step\n",
    "                    for event in graph.stream(initial_input, thread, stream_mode=\"values\"):\n",
    "                        st.session_state.current_state = event\n",
    "                    \n",
    "                    st.session_state.initial_input = True\n",
    "                    st.session_state.current_step = \"po_feedback\"\n",
    "                    st.rerun()\n",
    "        \n",
    "        # Subsequent steps\n",
    "        elif 'current_step' in st.session_state:\n",
    "            current_step = st.session_state.current_step\n",
    "            state = graph.get_state({\"configurable\": {\"thread_id\": st.session_state.thread_id}})\n",
    "            \n",
    "            # Display current state\n",
    "            display_state(state.values)\n",
    "            \n",
    "            # Get human feedback for current step\n",
    "            with st.form(f\"{current_step}_feedback_form\"):\n",
    "                st.subheader(f\"Provide Feedback ({current_step.replace('_', ' ')})\")\n",
    "                \n",
    "                # Show appropriate prompt based on current step\n",
    "                if current_step == \"po_feedback\":\n",
    "                    st.markdown(\"Review the generated user stories above and provide feedback:\")\n",
    "                elif current_step == \"design_feedback\":\n",
    "                    st.markdown(\"Review the design documents above and provide feedback:\")\n",
    "                elif current_step == \"code_feedback\":\n",
    "                    st.markdown(\"Review the generated code above and provide feedback:\")\n",
    "                elif current_step == \"testcases_feedback\":\n",
    "                    st.markdown(\"Review the test cases above and provide feedback:\")\n",
    "                \n",
    "                feedback = st.text_area(\"Your feedback:\", \"It looks good\")\n",
    "                \n",
    "                if st.form_submit_button(\"Submit Feedback\"):\n",
    "                    # Update state with new feedback\n",
    "                    current_messages = state.values[\"messages\"]\n",
    "                    new_messages = current_messages + [HumanMessage(content=feedback)]\n",
    "                    updated_messages = MessagesState(messages=new_messages)\n",
    "                    \n",
    "                    graph.update_state(\n",
    "                        {\"configurable\": {\"thread_id\": st.session_state.thread_id}},\n",
    "                        {\"messages\": updated_messages}\n",
    "                    )\n",
    "                    \n",
    "                    # Continue the workflow\n",
    "                    next_events = list(graph.stream(\n",
    "                        None,\n",
    "                        {\"configurable\": {\"thread_id\": st.session_state.thread_id}},\n",
    "                        stream_mode=\"values\"\n",
    "                    ))\n",
    "                    \n",
    "                    if next_events:\n",
    "                        st.session_state.current_state = next_events[-1]\n",
    "                        \n",
    "                        # Determine next step (simplified - in reality would check graph structure)\n",
    "                        if \"user_stories\" in next_events[-1]:\n",
    "                            st.session_state.current_step = \"po_feedback\"\n",
    "                        elif \"func_document\" in next_events[-1]:\n",
    "                            st.session_state.current_step = \"design_feedback\"\n",
    "                        elif \"code_content\" in next_events[-1]:\n",
    "                            st.session_state.current_step = \"code_feedback\"\n",
    "                        elif \"test_cases_content\" in next_events[-1]:\n",
    "                            st.session_state.current_step = \"testcases_feedback\"\n",
    "                        else:\n",
    "                            st.session_state.current_step = None\n",
    "                    \n",
    "                    st.rerun()\n",
    "    \n",
    "    with tab2:\n",
    "        st.subheader(\"Workflow Configuration\")\n",
    "        st.markdown(\"\"\"\n",
    "        ### Workflow Steps:\n",
    "        1. writeUserStories - Creates user stories from requirements\n",
    "        2. po_feedback - Product owner feedback on user stories\n",
    "        3. createFuncDesignDocument - Creates functional design\n",
    "        4. createTechDesignDocument - Creates technical design\n",
    "        5. design_feedback - Feedback on design documents\n",
    "        6. GenerateCode - Implements the code\n",
    "        7. code_feedback - Developer feedback on code\n",
    "        8. security_feedback - Security review\n",
    "        9. writeTestCases - Creates test cases\n",
    "        10. testcases_feedback - Feedback on test cases\n",
    "        11. executingTestCases - QA testing\n",
    "        12. qa_test_feedback - Final approval\n",
    "        \"\"\")\n",
    "        \n",
    "        st.image(graph.get_graph().draw_mermaid_png())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3737097518.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[885], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    streamlit run app.py\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "streamlit run app.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
